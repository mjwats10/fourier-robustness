{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fourier Descriptors -- Quickdraw GNN with angle.ipynb","provenance":[{"file_id":"1CFRzDKBTqUiPV9n6qBOAfQ6roOkzbDwz","timestamp":1660249983217},{"file_id":"1-Zu9875EPh8lpZKZBL92PY0SI_EZj2R8","timestamp":1659037754869},{"file_id":"1HmEE3jGUpn2AzNnSdBfhtm74OSHAmt0o","timestamp":1650055586271},{"file_id":"1owTDTec1_uglqFzHWQQKQbfPBG9Ge_Ep","timestamp":1649121474527}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDdT5myR9lsy","executionInfo":{"status":"ok","timestamp":1660845207817,"user_tz":420,"elapsed":15256,"user":{"displayName":"Matthew Watson","userId":"04238265723475746974"}},"outputId":"5a3c031a-4b9b-4963-e71f-72a4d6dd7acc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wDjjZFaGNZXV","executionInfo":{"status":"ok","timestamp":1660845235423,"user_tz":420,"elapsed":27611,"user":{"displayName":"Matthew Watson","userId":"04238265723475746974"}},"outputId":"8417deef-b780-4908-87e2-7bcbc1bafd8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu113\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyefd\n","  Downloading pyefd-1.6.0-py2.py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyefd) (1.21.6)\n","Installing collected packages: pyefd\n","Successfully installed pyefd-1.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting cairocffi\n","  Downloading cairocffi-1.3.0.tar.gz (88 kB)\n","\u001b[K     |████████████████████████████████| 88 kB 6.5 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi) (2.21)\n","Building wheels for collected packages: cairocffi\n","  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cairocffi: filename=cairocffi-1.3.0-py3-none-any.whl size=89668 sha256=b44522b663561359696b598e5208f6dfc5735f0cd33d412f75f16cd68e15fc2a\n","  Stored in directory: /root/.cache/pip/wheels/4e/ca/e1/5c8a9692a27f639a07c949044bec943f26c81cd53d3805319f\n","Successfully built cairocffi\n","Installing collected packages: cairocffi\n","Successfully installed cairocffi-1.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 38.0 MB/s \n","\u001b[?25hCollecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 40.2 MB/s \n","\u001b[?25hCollecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 35.4 MB/s \n","\u001b[?25hCollecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (709 kB)\n","\u001b[K     |████████████████████████████████| 709 kB 16.8 MB/s \n","\u001b[?25hCollecting torch-geometric\n","  Downloading torch_geometric-2.1.0.tar.gz (467 kB)\n","\u001b[K     |████████████████████████████████| 467 kB 34.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.1.0-py3-none-any.whl size=687167 sha256=f6f63da96139f353feb915a89ef7ecb233e5cf08172d1f271d3b648b9f66c4de\n","  Stored in directory: /root/.cache/pip/wheels/95/9b/21/7a21b39bcea8f520c9cf2f0eca06e97cd81d02b0ef42ce4942\n","Successfully built torch-geometric\n","Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n","Successfully installed torch-cluster-1.6.0 torch-geometric-2.1.0 torch-scatter-2.0.9 torch-sparse-0.6.15 torch-spline-conv-1.2.1\n"]}],"source":["import os\n","import torch\n","print(torch.__version__)\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.utils.data import Dataset\n","from torchvision import transforms as T\n","import random\n","import cv2\n","import numpy as np\n","!pip install pyefd\n","import pyefd\n","from google.colab.patches import cv2_imshow\n","!pip install cairocffi\n","import cairocffi as cairo\n","import struct\n","from struct import unpack\n","!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n","from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.nn import global_mean_pool"]},{"cell_type":"code","source":["# Env vars\n","torch.use_deterministic_algorithms(False)\n","\n","# Const vars\n","SAVE_PATH = '/content/drive/My Drive/Fourier/Saved Models/GNN angl.pt'\n","RAND_SEED = 0\n","DEVICE = \"cuda\"\n","\n","IMG_SIDE = 28\n","NUM_CLASSES = 343\n","EPOCHS = 10\n","LEARNING_RATE = 0.001\n","BATCH_SIZE = 500\n","LOSS_FN = nn.CrossEntropyLoss()\n","EDGE_ATTR_DIM = 1"],"metadata":{"id":"q9cgJ84nZ1qR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert raw vector image to list of raster images, one for each stroke\n","def vector_to_raster(vector_image, side=IMG_SIDE, line_diameter=16, padding=96, bg_color=(0,0,0), fg_color=(1,1,1)):\n","  \"\"\"\n","  padding and line_diameter are relative to the original 256x256 image.\n","  \"\"\"\n","  \n","  original_side = 256.\n","  \n","  surface = cairo.ImageSurface(cairo.FORMAT_ARGB32, side, side)\n","  ctx = cairo.Context(surface)\n","  ctx.set_antialias(cairo.ANTIALIAS_BEST)\n","  ctx.set_line_cap(cairo.LINE_CAP_ROUND)\n","  ctx.set_line_join(cairo.LINE_JOIN_ROUND)\n","  ctx.set_line_width(line_diameter)\n","\n","  # scale to match the new size\n","  # add padding at the edges for the line_diameter\n","  # and add additional padding to account for antialiasing\n","  total_padding = padding * 2. + line_diameter\n","  new_scale = float(side) / float(original_side + total_padding)\n","  ctx.scale(new_scale, new_scale)\n","  ctx.translate(total_padding / 2., total_padding / 2.)\n","      \n","  bbox = np.hstack(vector_image).max(axis=1)\n","  offset = ((original_side, original_side) - bbox) / 2.\n","  offset = offset.reshape(-1,1)\n","  centered = [stroke + offset for stroke in vector_image]\n","\n","  stroke_rasters = []\n","  for xv, yv in centered:\n","    # clear background\n","    ctx.set_source_rgb(*bg_color)\n","    ctx.paint()\n","\n","    # draw strokes, this is the most cpu-intensive part\n","    ctx.set_source_rgb(*fg_color)        \n","    ctx.move_to(xv[0], yv[0])\n","    for x, y in zip(xv, yv):\n","        ctx.line_to(x, y)\n","    ctx.stroke()\n","\n","    data = surface.get_data()\n","    stroke_raster = np.copy(np.asarray(data)[::4]).reshape(28, 28)\n","    stroke_rasters.append(stroke_raster)\n","\n","  return stroke_rasters\n","\n","def get_edges(stroke_angles): \n","  adj_1 = []\n","  adj_2 = []\n","  stroke_angle_diffs = []\n","  NORM = 2*np.pi\n","  for i in range(len(stroke_angles)):\n","      for j in range(i+1,len(stroke_angles)):\n","          adj_1.append(i)\n","          adj_2.append(j)\n","          adj_1.append(j)\n","          adj_2.append(i)\n","          \n","          angle_diff_ij = stroke_angles[i] - stroke_angles[j]\n","          angle_diff_ji = stroke_angles[j] - stroke_angles[i]\n","          angle_diff_norm_ij = [angle_diff_ij / NORM]\n","          angle_diff_norm_ji = [angle_diff_ji / NORM]\n","          stroke_angle_diffs.append(angle_diff_norm_ij)\n","          stroke_angle_diffs.append(angle_diff_norm_ji)\n","\n","  edge_indices = torch.LongTensor([adj_1,adj_2])\n","  edge_attr = torch.FloatTensor(stroke_angle_diffs)\n","  return edge_indices, edge_attr\n","\n","# transform functions - take sketch image, return torch tensor of descriptors\n","def fourier_transform(vector_img, is_test):\n","  stroke_rasters = vector_to_raster(vector_img)\n","\n","  # add rotations and translations at test time\n","  if is_test: \n","    stroke_rasters = np.stack(stroke_rasters)\n","    stroke_rasters = torch.from_numpy(stroke_rasters).float()\n","\n","    angle = random.random()*60 - 30\n","    deltaX = random.randint(-3, 3)\n","    deltaY = random.randint(-3, 3)\n","\n","    stroke_rasters = T.functional.affine(stroke_rasters,angle,[deltaX, deltaY],1,0,\n","                                          interpolation=T.InterpolationMode.BILINEAR)\n","    stroke_rasters = np.asarray(stroke_rasters)\n","    stroke_rasters = np.split(stroke_rasters, stroke_rasters.shape[0])\n","    stroke_rasters = [np.squeeze(a) for a in stroke_rasters]\n","\n","  stroke_rasters_binary = []\n","  for raster in stroke_rasters:\n","    raster_binary = cv2.threshold(raster, 100, 255, cv2.THRESH_BINARY)[1]\n","    stroke_rasters_binary.append(raster_binary)\n","\n","  stroke_fourier_descriptors = []\n","  stroke_angles = []\n","  for i, raster in enumerate(stroke_rasters_binary):\n","    contours, hierarchy = cv2.findContours(raster.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n","\n","    largest_size = 0\n","    largest_index = 0\n","    for k, contour in enumerate(contours):\n","        if len(contour) > largest_size:\n","          largest_size = len(contour)\n","          largest_index = k\n","\n","    if largest_size > 1:\n","      contour = contours[largest_index]\n","      coeffs, transform = pyefd.elliptic_fourier_descriptors(np.squeeze(contour), order=FOURIER_ORDER, normalize=True, return_transformation=True)\n","      stroke_angle = transform[1]\n","      stroke_fourier_descriptors.append(coeffs.flatten())\n","      stroke_angles.append(stroke_angle)\n","\n","  edge_indices, edge_attr = get_edges(stroke_angles)\n","  stroke_fourier_descriptors = np.stack(stroke_fourier_descriptors)\n","  stroke_fourier_descriptors = torch.from_numpy(stroke_fourier_descriptors).float()\n","  return stroke_fourier_descriptors, edge_indices, edge_attr\n","\n","# helper method to find class based on imgset index\n","def find_class(idx, num_list):\n","  class_id = 0\n","  sum = num_list[class_id]\n","  while idx >= sum:\n","    class_id += 1\n","    sum += num_list[class_id]\n","  return class_id\n","\n","# deterministic worker re-seeding\n","def seed_worker(worker_id):\n","  worker_seed = torch.initial_seed() % 2**32\n","  np.random.seed(worker_seed)\n","  random.seed(worker_seed)\n","\n","# custom dataset for quickdraw\n","class QuickdrawDataset(Dataset):\n","  def __init__(self, imgs, nums, is_test):\n","    self.imgs = imgs\n","    self.nums = nums\n","    self.len = sum(nums)\n","    self.is_test = is_test\n","\n","  def __len__(self):\n","    return self.len\n","\n","  def __getitem__(self, idx):\n","    img = self.imgs[idx]\n","    x, edge_index, edge_attr = fourier_transform(img, self.is_test)\n","    y = find_class(idx, self.nums)\n","    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n","\n","\n","# pytorch-geometric GCN\n","class GCN(torch.nn.Module):\n","  def __init__(self):\n","    super(GCN, self).__init__()\n","    self.conv1 = GCNConv(FOURIER_ORDER * 4, 128)\n","    self.conv2 = GCNConv(128, 128)\n","    self.conv3 = GCNConv(128, 128)\n","    self.conv4 = GCNConv(128, 128)\n","    self.conv5 = GCNConv(128, 128)\n","    self.conv6 = GCNConv(128, 128)\n","    self.conv7 = GCNConv(128, 128)\n","    self.conv8 = GCNConv(128, 128)\n","    self.fc1 = nn.Linear(128, 512)\n","    self.fc2 = nn.Linear(512, 512)\n","    self.head = nn.Linear(512, NUM_CLASSES)\n","    self.edge_proj = nn.Linear(EDGE_ATTR_DIM, 1)\n","    self.relu = nn.ReLU()\n","    self.sigmoid = nn.Sigmoid()\n","\n","\n","  def forward(self, data):\n","    x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n","\n","    if EDGE_ATTR_DIM > 1:\n","      edge_attr = edge_attr.squeeze(dim=2)\n","    edge_weight = self.edge_proj(edge_attr)\n","    edge_weight = self.sigmoid(edge_weight)\n","    x = self.conv1(x, edge_index, edge_weight)\n","    x = self.relu(x)\n","    x = self.conv2(x, edge_index, edge_weight)\n","    x = self.relu(x)\n","    x = self.conv3(x, edge_index, edge_weight)\n","    x = self.relu(x)\n","    x = self.conv4(x, edge_index, edge_weight)\n","    x = self.relu(x)\n","    x = self.conv5(x, edge_index, edge_weight)\n","    x = self.relu(x)\n","    x = self.conv6(x, edge_index, edge_weight)\n","    x = self.relu(x)\n","    x = self.conv7(x, edge_index, edge_weight)\n","    x = self.relu(x)\n","    x = self.conv8(x, edge_index, edge_weight)\n","    x = self.relu(x)\n","    x = global_mean_pool(x, batch)\n","    x = self.fc1(x)\n","    x = self.relu(x)\n","    x = self.fc2(x)\n","    x = self.relu(x)\n","    return self.head(x)\n","\n","\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train() # put the model in train mode\n","    total_loss = 0\n","    total_correct = 0\n","    # for each batch in the training set compute loss and update model parameters\n","    for batch, data in enumerate(dataloader):\n","      data = data.to(DEVICE)\n","      # Compute prediction and loss\n","      out = model(data)\n","      loss = loss_fn(out, data.y)\n","\n","      # Backpropagation to update model parameters\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # print current training metrics for user\n","      data, out, loss = data.to(\"cpu\"), out.to(\"cpu\"), loss.to(\"cpu\")\n","      loss_val = loss.item()\n","      if batch % 100 == 0:\n","          current = (batch + 1) * BATCH_SIZE\n","          print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","      pred = out.argmax(dim=1, keepdim=True)\n","      correct = pred.eq(data.y.view_as(pred)).sum().item()\n","      total_correct += correct\n","      total_loss += loss_val\n","      # print(f\"train loss: {loss_val:>7f}   train accuracy: {correct / BATCH_SIZE:>7f}   [batch: {batch + 1:>3d}/{(size // BATCH_SIZE) + 1:>3d}]\")      \n","    print(f\"\\nepoch avg train loss: {total_loss / ((size // BATCH_SIZE) + 1):>7f}   epoch avg train accuracy: {total_correct / size:>7f}\")\n","      \n","def eval_loop(dataloader, model):\n","  model.eval()\n","  size = len(dataloader.dataset)\n","  with torch.no_grad():\n","    total_correct = 0\n","    for data in dataloader:\n","      data = data.to(DEVICE)\n","      out = model(data)\n","      data, out = data.to(\"cpu\"), out.to(\"cpu\")\n","      pred = out.argmax(dim=1, keepdim=True)\n","      total_correct += pred.eq(data.y.view_as(pred)).sum().item()\n","\n","    accuracy = total_correct / size\n","    print(f\"test accuracy: {accuracy:>7f}\")"],"metadata":{"id":"OesVurZASnQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define methods for unpacking Quickdraw .bin files\n","def unpack_drawing(file_handle):\n","  file_handle.read(15)\n","  n_strokes, = unpack('H', file_handle.read(2))\n","  image = []\n","  for i in range(n_strokes):\n","      n_points, = unpack('H', file_handle.read(2))\n","      fmt = str(n_points) + 'B'\n","      x = unpack(fmt, file_handle.read(n_points))\n","      y = unpack(fmt, file_handle.read(n_points))\n","      image.append((x, y))\n","\n","  return image\n","\n","\n","def unpack_drawings(filename):\n","  imageset = []\n","  with open(filename, 'rb') as f:\n","      while True:\n","          try:\n","              imageset.append(unpack_drawing(f))\n","          except struct.error:\n","              break\n","  return imageset\n","\n","train_dir = '/content/drive/My Drive/Fourier/Quickdraw Dataset Small/Train/'\n","test_dir = '/content/drive/My Drive/Fourier/Quickdraw Dataset Small/Test/'\n","train_imgs = []\n","test_imgs = []\n","train_nums = []\n","test_nums = []\n","list_of_classes = [\"The Eiffel Tower\", \"The Great Wall of China\", \"The Mona Lisa\",\n","                   \"aircraft carrier\", \"airplane\", \"alarm clock\", \"ambulance\", \n","                   \"angel\", \"ant\", \"anvil\", \"apple\", \"arm\", \"asparagus\", \"axe\", \n","                   \"backpack\", \"banana\", \"bandage\", \"barn\", \"baseball bat\", \n","                   \"baseball\", \"basket\", \"basketball\", \"bathtub\", \"beach\", \"bear\", \n","                   \"beard\", \"bed\", \"bee\", \"belt\", \"bench\", \"bicycle\", \"binoculars\", \n","                   \"bird\", \"birthday cake\", \"blackberry\", \"blueberry\", \"book\", \n","                   \"boomerang\", \"bottlecap\", \"bowtie\", \"bracelet\", \"brain\", \n","                   \"bread\", \"bridge\", \"broccoli\", \"broom\", \"bucket\", \"bulldozer\", \n","                   \"bus\", \"bush\", \"butterfly\", \"cactus\", \"cake\", \"calculator\", \n","                   \"calendar\", \"camel\", \"camera\", \"camouflage\", \"campfire\", \n","                   \"candle\", \"cannon\", \"canoe\", 'car', 'carrot', \"castle\", \"cat\", \"ceiling fan\", \n","                   \"cell phone\", \"cello\", \"chair\", \"chandelier\", \"church\", \n","                   \"circle\", \"clarinet\", \"clock\", \"cloud\", \"coffee cup\", \n","                   \"compass\", \"computer\", \"cookie\", \"cooler\", \"couch\", \"cow\",\n","                   \"crab\", \"crayon\", \"crocodile\", \"crown\", \"cruise ship\", \n","                   \"cup\", \"diamond\", \"dishwasher\", \"diving board\", \"dog\", \n","                   \"dolphin\", \"donut\", \"door\", \"dragon\", \"dresser\", \"drill\", \n","                   \"drums\", \"duck\", \"dumbbell\", \"ear\", \"elbow\", \"elephant\", \n","                   \"envelope\", \"eraser\", \"eye\", \"eyeglasses\", \"face\", \"fan\",\n","                   \"feather\", \"fence\", \"finger\", \"fire hydrant\", \"fireplace\",\n","                   \"firetruck\", \"fish\", \"flamingo\", \"flashlight\", \"flip flops\", \n","                   \"floor lamp\", \"flower\", \"flying saucer\", \"foot\", \"fork\", \n","                   \"frog\", \"frying pan\", \"garden hose\", \"garden\", \"giraffe\", \n","                   \"goatee\", \"golf club\", \"grapes\", \"grass\", \"guitar\", \n","                   \"hamburger\", \"hammer\", \"hand\", \"harp\", \"hat\", \"headphones\", \n","                   \"hedgehog\", \"helicopter\", \"helmet\", \"hexagon\", \"hockey puck\", \n","                   \"hockey stick\", \"horse\", \"hospital\", \"hot air balloon\", \n","                   \"hot dog\", \"hot tub\", \"hourglass\", \"house plant\", \"house\", \n","                   \"hurricane\", \"ice cream\", \"jacket\", \"jail\", \"kangaroo\", \n","                   \"key\", \"keyboard\", \"knee\", \"knife\", \"ladder\", \"lantern\", \n","                   \"laptop\", \"leaf\", \"leg\", \"light bulb\", \"lighter\", \"lighthouse\",\n","                   \"lightning\", \"line\", \"lion\", \"lipstick\", \"lobster\", \"lollipop\",\n","                   \"mailbox\", \"map\", \"marker\", \"matches\", \"megaphone\", \"mermaid\", \n","                   \"microphone\", \"microwave\", \"monkey\", \"moon\", \"mosquito\", \n","                   \"motorbike\", \"mountain\", \"mouse\", \"moustache\", \"mouth\", \"mug\",\n","                   \"mushroom\", \"nail\", \"necklace\", \"nose\", \"ocean\", \"octagon\", \n","                   \"octopus\", \"onion\", \"oven\", \"owl\", \"paint can\", \"paintbrush\", \n","                   \"palm tree\", \"panda\", \"pants\", \"paper clip\", \"parachute\", \n","                   \"parrot\", \"passport\", \"peanut\", \"pear\", \"peas\", \"pencil\", \n","                   \"penguin\", \"piano\", \"pickup truck\", \"picture frame\", \"pig\", \n","                   \"pillow\", \"pineapple\", \"pizza\", \"pliers\", \"police car\", \n","                   \"pond\", \"pool\", \"popsicle\", \"postcard\", \"potato\", \n","                   \"power outlet\", \"purse\", \"rabbit\", \"raccoon\", \"radio\", \n","                   \"rain\", 'rainbow', 'rake', 'remote control', 'rhinoceros', \n","                   'rifle', 'river', 'roller coaster', 'rollerskates', \n","                   'sailboat', 'sandwich', 'saw', 'saxophone', 'school bus', \n","                   'scissors', 'scorpion', 'screwdriver', 'sea turtle', \n","                   'see saw', 'shark', 'sheep', 'shoe', 'shorts', 'shovel', \n","                   'sink', 'skateboard', 'skull', 'skyscraper', 'sleeping bag', \n","                   'smiley face', 'snail', 'snake', 'snorkel', 'snowflake', \n","                   'snowman', 'soccer ball', 'sock', 'speedboat', 'spider', \n","                   'spoon', 'spreadsheet', 'square', 'squiggle', 'squirrel', \n","                   'stairs', 'star', 'steak', 'stereo', 'stethoscope', 'stitches', \n","                   'stop sign', 'stove', 'strawberry', 'streetlight', \n","                   'string bean', 'submarine', 'suitcase', 'sun', 'swan', \n","                   'sweater', 'swing set', 'sword', 'syringe', 't-shirt', \n","                   'table', 'teapot', 'teddy-bear', 'telephone', 'television', \n","                   'tennis racquet', 'tent', 'tiger', 'toaster', 'toe', 'toilet', \n","                   'tooth', 'toothbrush', 'toothpaste', 'tornado', 'tractor', \n","                   'traffic light', 'train', 'tree', 'triangle', 'trombone', \n","                   'truck', 'trumpet', 'umbrella', 'underwear', 'van', 'vase', 'violin', \n","                   'washing machine', 'watermelon', 'waterslide', 'whale', \n","                   'wheel', 'windmill', 'wine bottle', 'wine glass', 'wristwatch', \n","                   'yoga', 'zebra', 'zigzag']"],"metadata":{"id":"3yQmzmk8hsDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load dataset\n","for item in list_of_classes:\n","  train_folder = train_dir + item + '.bin'\n","  test_folder = test_dir + item + '.bin'\n","  train_drawings = unpack_drawings(train_folder)\n","  train_imgs += train_drawings\n","  train_nums.append(len(train_drawings))\n","  test_drawings = unpack_drawings(test_folder)\n","  test_imgs += test_drawings\n","  test_nums.append(len(test_drawings))"],"metadata":{"id":"3qSSfgoWAny4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for FOURIER_ORDER in reversed(range(6, 7)):\n","  # seed RNGs\n","  torch.manual_seed(RAND_SEED)\n","  random.seed(RAND_SEED)\n","\n","  # create datasets\n","  train_fourier_data = QuickdrawDataset(train_imgs, train_nums, is_test=False)\n","  # eval_fourier_data = QuickdrawDataset(test_imgs, test_nums, is_test=False)\n","  test_fourier_data = QuickdrawDataset(test_imgs, test_nums, is_test=True)\n","\n","  # create dataloaders\n","  g = torch.Generator()\n","  g.manual_seed(RAND_SEED)\n","  train_fourier_loader = DataLoader(train_fourier_data, batch_size=BATCH_SIZE, shuffle=True, worker_init_fn=seed_worker, generator=g)\n","  # eval_fourier_loader = DataLoader(eval_fourier_data, batch_size=BATCH_SIZE, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","  test_fourier_loader = DataLoader(test_fourier_data, batch_size=BATCH_SIZE, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","\n","  # init model and optimizer\n","  model = GCN()\n","  model.to(DEVICE)\n","  optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","  print(\"\\n\\n\\nFourier order is: \"+str(FOURIER_ORDER)+\"\\n\\n\\n\")\n","\n","  # train for EPOCHS number of epochs then evaluate on test data with affine transformations\n","  # eval_loop(dataloader=test_fourier_loader,model=model)\n","  for i in range(EPOCHS):\n","      print(\"Epoch \" + str(i + 1) + \"\\n\")\n","      train_loop(dataloader=train_fourier_loader,model=model,loss_fn=LOSS_FN,optimizer=optim)\n","      # eval_loop(dataloader=eval_fourier_loader,model=model)\n","      torch.save({\n","                  'epoch': i + 1,\n","                  'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optim.state_dict()\n","                  }, SAVE_PATH)\n","      print(\"\\n-------------------------------\\n\")\n","  random.seed(RAND_SEED)\n","  eval_loop(dataloader=test_fourier_loader,model=model)"],"metadata":{"id":"J1t1TKOBHG-R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b65712b-1e45-435e-8b2d-49b63fbcabd5","executionInfo":{"status":"ok","timestamp":1660855067083,"user_tz":420,"elapsed":888859,"user":{"displayName":"Matthew Watson","userId":"04238265723475746974"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Fourier order is: 6\n","\n","\n","\n","Epoch 1\n","\n","loss: 5.828428  [  500/339985]\n","loss: 5.464295  [50500/339985]\n","loss: 5.218936  [100500/339985]\n","loss: 4.941697  [150500/339985]\n","loss: 4.763614  [200500/339985]\n","loss: 4.577528  [250500/339985]\n","loss: 4.482871  [300500/339985]\n","\n","epoch avg train loss: 4.991822   epoch avg train accuracy: 0.060503\n","\n","-------------------------------\n","\n","Epoch 2\n","\n","loss: 4.563345  [  500/339985]\n","loss: 4.387664  [50500/339985]\n","loss: 4.323366  [100500/339985]\n","loss: 4.287434  [150500/339985]\n","loss: 4.232512  [200500/339985]\n","loss: 4.134607  [250500/339985]\n","loss: 4.238904  [300500/339985]\n","\n","epoch avg train loss: 4.341004   epoch avg train accuracy: 0.131250\n","\n","-------------------------------\n","\n","Epoch 3\n","\n","loss: 4.272676  [  500/339985]\n","loss: 4.036049  [50500/339985]\n","loss: 4.165728  [100500/339985]\n","loss: 4.161479  [150500/339985]\n","loss: 4.086275  [200500/339985]\n","loss: 4.257757  [250500/339985]\n","loss: 4.023227  [300500/339985]\n","\n","epoch avg train loss: 4.165664   epoch avg train accuracy: 0.155113\n","\n","-------------------------------\n","\n","Epoch 4\n","\n","loss: 4.156181  [  500/339985]\n","loss: 4.079582  [50500/339985]\n","loss: 4.026900  [100500/339985]\n","loss: 4.063997  [150500/339985]\n","loss: 4.137934  [200500/339985]\n","loss: 4.187955  [250500/339985]\n","loss: 3.931429  [300500/339985]\n","\n","epoch avg train loss: 4.077825   epoch avg train accuracy: 0.167387\n","\n","-------------------------------\n","\n","Epoch 5\n","\n","loss: 3.982863  [  500/339985]\n","loss: 4.137398  [50500/339985]\n","loss: 4.027101  [100500/339985]\n","loss: 4.066099  [150500/339985]\n","loss: 3.971434  [200500/339985]\n","loss: 3.982828  [250500/339985]\n","loss: 4.221979  [300500/339985]\n","\n","epoch avg train loss: 4.020473   epoch avg train accuracy: 0.175105\n","\n","-------------------------------\n","\n","Epoch 6\n","\n","loss: 4.054432  [  500/339985]\n","loss: 3.878265  [50500/339985]\n","loss: 4.059065  [100500/339985]\n","loss: 4.076753  [150500/339985]\n","loss: 3.983614  [200500/339985]\n","loss: 4.002280  [250500/339985]\n","loss: 4.041964  [300500/339985]\n","\n","epoch avg train loss: 3.979868   epoch avg train accuracy: 0.181740\n","\n","-------------------------------\n","\n","Epoch 7\n","\n","loss: 4.076001  [  500/339985]\n","loss: 4.086225  [50500/339985]\n","loss: 4.034595  [100500/339985]\n","loss: 4.002717  [150500/339985]\n","loss: 3.936950  [200500/339985]\n","loss: 3.974861  [250500/339985]\n","loss: 3.954305  [300500/339985]\n","\n","epoch avg train loss: 3.951991   epoch avg train accuracy: 0.185643\n","\n","-------------------------------\n","\n","Epoch 8\n","\n","loss: 4.000339  [  500/339985]\n","loss: 3.817809  [50500/339985]\n","loss: 3.959085  [100500/339985]\n","loss: 3.899213  [150500/339985]\n","loss: 3.927605  [200500/339985]\n","loss: 3.961640  [250500/339985]\n","loss: 4.066701  [300500/339985]\n","\n","epoch avg train loss: 3.932091   epoch avg train accuracy: 0.187952\n","\n","-------------------------------\n","\n","Epoch 9\n","\n","loss: 3.987801  [  500/339985]\n","loss: 3.897438  [50500/339985]\n","loss: 3.801104  [100500/339985]\n","loss: 4.008712  [150500/339985]\n","loss: 3.862846  [200500/339985]\n","loss: 3.993095  [250500/339985]\n","loss: 3.858631  [300500/339985]\n","\n","epoch avg train loss: 3.908922   epoch avg train accuracy: 0.191758\n","\n","-------------------------------\n","\n","Epoch 10\n","\n","loss: 3.830687  [  500/339985]\n","loss: 3.892100  [50500/339985]\n","loss: 3.891288  [100500/339985]\n","loss: 3.982405  [150500/339985]\n","loss: 3.818238  [200500/339985]\n","loss: 4.007981  [250500/339985]\n","loss: 3.873838  [300500/339985]\n","\n","epoch avg train loss: 3.894914   epoch avg train accuracy: 0.193703\n","\n","-------------------------------\n","\n","test accuracy: 0.164142\n"]}]}]}