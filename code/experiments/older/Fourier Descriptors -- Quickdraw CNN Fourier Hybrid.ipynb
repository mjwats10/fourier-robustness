{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YbnaTcA2O78WEFXdGIOmPkDRM18AVJ2X","timestamp":1662662466620},{"file_id":"1-Zu9875EPh8lpZKZBL92PY0SI_EZj2R8","timestamp":1660851002402},{"file_id":"1HmEE3jGUpn2AzNnSdBfhtm74OSHAmt0o","timestamp":1650055586271},{"file_id":"1owTDTec1_uglqFzHWQQKQbfPBG9Ge_Ep","timestamp":1649121474527}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IiJuHUdQvdYp","executionInfo":{"status":"ok","timestamp":1662686157853,"user_tz":420,"elapsed":39303,"user":{"displayName":"Matt Watson","userId":"10419224003756061351"}},"outputId":"c571197a-5ed2-4471-e0a0-1698da8ad9e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDjjZFaGNZXV"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms as T\n","import random\n","import cv2\n","import numpy as np\n","!pip install pyefd\n","import pyefd\n","from google.colab.patches import cv2_imshow\n","!pip install cairocffi\n","import cairocffi as cairo\n","import struct\n","from struct import unpack"]},{"cell_type":"code","source":["# Env vars\n","torch.use_deterministic_algorithms(False)\n","\n","# Const vars\n","LOAD_PATH = '/content/drive/My Drive/Fourier/Saved Models/CNN Fourier Hybrid 001.pt'\n","RAND_SEED = 0\n","DEVICE = \"cpu\"\n","\n","FOURIER_ORDER = 1\n","IMG_SIDE = 28\n","IMG_CENTER = np.asarray(((IMG_SIDE - 1) / 2, (IMG_SIDE - 1) / 2))\n","NUM_CLASSES = 343\n","EPOCHS = 10\n","BATCH_SIZE = 512\n","LOSS_FN = nn.CrossEntropyLoss()"],"metadata":{"id":"q9cgJ84nZ1qR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert raw vector image to single raster image\n","def vector_to_raster(vector_image, side=IMG_SIDE, line_diameter=16, padding=96, bg_color=(0,0,0), fg_color=(1,1,1)):\n","  \"\"\"\n","  padding and line_diameter are relative to the original 256x256 image.\n","  \"\"\"\n","  \n","  original_side = 256.\n","  \n","  surface = cairo.ImageSurface(cairo.FORMAT_ARGB32, side, side)\n","  ctx = cairo.Context(surface)\n","  ctx.set_antialias(cairo.ANTIALIAS_BEST)\n","  ctx.set_line_cap(cairo.LINE_CAP_ROUND)\n","  ctx.set_line_join(cairo.LINE_JOIN_ROUND)\n","  ctx.set_line_width(line_diameter)\n","\n","  # scale to match the new size\n","  # add padding at the edges for the line_diameter\n","  # and add additional padding to account for antialiasing\n","  total_padding = padding * 2. + line_diameter\n","  new_scale = float(side) / float(original_side + total_padding)\n","  ctx.scale(new_scale, new_scale)\n","  ctx.translate(total_padding / 2., total_padding / 2.)\n","      \n","  bbox = np.hstack(vector_image).max(axis=1)\n","  offset = ((original_side, original_side) - bbox) / 2.\n","  offset = offset.reshape(-1,1)\n","  centered = [stroke + offset for stroke in vector_image]\n","\n","  # clear background\n","  ctx.set_source_rgb(*bg_color)\n","  ctx.paint()\n","\n","  # draw strokes, this is the most cpu-intensive part\n","  ctx.set_source_rgb(*fg_color)     \n","  for xv, yv in centered:   \n","    ctx.move_to(xv[0], yv[0])\n","    for x, y in zip(xv, yv):\n","        ctx.line_to(x, y)\n","    ctx.stroke()\n","\n","  data = surface.get_data()\n","  raster = np.copy(np.asarray(data)[::4]).reshape(28, 28)\n","  return raster\n","\n","# Define transformation(s) to be applied to dataset-\n","transforms_norm = T.Compose(\n","      [\n","          T.ToTensor(), # scales integer inputs in the range [0, 255] into the range [0.0, 1.0]\n","          T.Normalize(mean=(0.138), std=(0.296)) # Quickdraw mean and stdev (35.213, 75.588), divided by 255\n","          \n","      ]\n","  )\n","\n","transforms_tensor = T.Compose(\n","      [\n","          T.ToTensor(), # scales integer inputs in the range [0, 255] into the range [0.0, 1.0] \n","      ]\n","  )\n","\n","# transform functions - take sketch image, return torch tensor of descriptors\n","def transform(vector_img, is_test):\n","  raster = vector_to_raster(vector_img)\n","\n","  # add rotations and translations at test time\n","  if is_test: \n","    raster = transforms_tensor(raster.astype(np.float32))\n","\n","    angle = random.random()*60 - 30\n","    deltaX = random.randint(-3, 3)\n","    deltaY = random.randint(-3, 3)\n","\n","    raster = T.functional.affine(raster, angle, [deltaX, deltaY], 1, 0,\n","                                 interpolation=T.InterpolationMode.BILINEAR)\n","    raster = np.squeeze(raster.numpy()).astype(np.uint8)\n","  \n","  # fourier transform of outer contour to get rotation angle\n","  raster_binary = cv2.threshold(raster, 100, 255, cv2.THRESH_BINARY)[1]\n","  contours, hierarchy = cv2.findContours(raster_binary, \n","                                         cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n","  \n","  largest_size = 0\n","  largest_index = 0\n","  for k, contour in enumerate(contours):\n","      if len(contour) > largest_size:\n","        largest_size = len(contour)\n","        largest_index = k\n","\n","  contour = np.squeeze(contours[largest_index])\n","  sketch_center = pyefd.calculate_dc_coefficients(contour)\n","  coeffs, transform = pyefd.elliptic_fourier_descriptors(contour, order=FOURIER_ORDER, normalize=True, return_transformation=True)\n","  contour_angle = np.degrees(transform[1])\n","  img_offset = (IMG_CENTER - sketch_center).round()\n","\n","  # de-translate and de-rotate\n","  raster = transforms_norm(raster)\n","  raster = T.functional.affine(raster, 0, (img_offset[0], img_offset[1]), 1, 0,\n","                                 interpolation=T.InterpolationMode.BILINEAR)\n","  raster = T.functional.affine(raster, -1 * contour_angle, [0, 0], 1, 0,\n","                                 interpolation=T.InterpolationMode.BILINEAR)\n","  return raster\n","\n","# helper method to find class based on imgset index\n","def find_class(idx, num_list):\n","  class_id = 0\n","  sum = num_list[class_id]\n","  while idx >= sum:\n","    class_id += 1\n","    sum += num_list[class_id]\n","  return class_id\n","\n","# deterministic worker re-seeding\n","def seed_worker(worker_id):\n","  worker_seed = torch.initial_seed() % 2**32\n","  np.random.seed(worker_seed)\n","  random.seed(worker_seed)\n","\n","# custom dataset for quickdraw\n","class QuickdrawDataset(Dataset):\n","  def __init__(self, imgs, nums, is_test):\n","    self.imgs = imgs\n","    self.nums = nums\n","    self.len = sum(nums)\n","    self.is_test = is_test\n","\n","  def __len__(self):\n","    return self.len\n","\n","  def __getitem__(self, idx):\n","    img = self.imgs[idx]\n","    x = transform(img, self.is_test)\n","    y = find_class(idx, self.nums)\n","    return x, y\n","\n","\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 64, 3)\n","        self.conv2 = nn.Conv2d(64, 64, 3)\n","        self.conv3 = nn.Conv2d(64, 64, 3)\n","        self.conv4 = nn.Conv2d(64, 64, 3)\n","        self.maxpool = nn.MaxPool2d(2) \n","        self.relu = nn.ReLU()\n","        self.fc1 = nn.Linear(64 * 4 * 4, 384)\n","        self.head = nn.Linear(384, NUM_CLASSES)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.conv3(x)\n","        x = self.relu(x)\n","        x = self.conv4(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        return self.head(x)\n","\n","\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train() # put the model in train mode\n","    total_loss = 0\n","    total_correct = 0\n","    # for each batch in the training set compute loss and update model parameters\n","    for batch, (x, y) in enumerate(dataloader):\n","      x, y = x.to(DEVICE), y.to(DEVICE)\n","      # Compute prediction and loss\n","      out = model(x)\n","      loss = loss_fn(out, y)\n","\n","      # Backpropagation to update model parameters\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # print current training metrics for user\n","      y, out, loss = y.to(\"cpu\"), out.to(\"cpu\"), loss.to(\"cpu\")\n","      loss_val = loss.item()\n","      if batch % 100 == 0:\n","          current = (batch + 1) * BATCH_SIZE\n","          print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","      pred = out.argmax(dim=1, keepdim=True)\n","      correct = pred.eq(y.view_as(pred)).sum().item()\n","      total_correct += correct\n","      total_loss += loss_val\n","      # print(f\"train loss: {loss_val:>7f}   train accuracy: {correct / BATCH_SIZE:>7f}   [batch: {batch + 1:>3d}/{(size // BATCH_SIZE) + 1:>3d}]\")      \n","    print(f\"\\nepoch avg train loss: {total_loss / ((size // BATCH_SIZE) + 1):>7f}   epoch avg train accuracy: {total_correct / size:>7f}\")\n","      \n","def eval_loop(dataloader, model):\n","  model.eval()\n","  size = len(dataloader.dataset)\n","  with torch.no_grad():\n","    total_correct = 0\n","    for x, y in dataloader:\n","      x, y = x.to(DEVICE), y.to(DEVICE)\n","      out = model(x)\n","      y, out = y.to(\"cpu\"), out.to(\"cpu\")\n","      pred = out.argmax(dim=1, keepdim=True)\n","      total_correct += pred.eq(y.view_as(pred)).sum().item()\n","\n","    accuracy = total_correct / size\n","    print(f\"test accuracy: {accuracy:>7f}\")\n"],"metadata":{"id":"OesVurZASnQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define methods for unpacking Quickdraw .bin files\n","def unpack_drawing(file_handle):\n","  file_handle.read(15)\n","  n_strokes, = unpack('H', file_handle.read(2))\n","  image = []\n","  for i in range(n_strokes):\n","      n_points, = unpack('H', file_handle.read(2))\n","      fmt = str(n_points) + 'B'\n","      x = unpack(fmt, file_handle.read(n_points))\n","      y = unpack(fmt, file_handle.read(n_points))\n","      image.append((x, y))\n","\n","  return image\n","\n","\n","def unpack_drawings(filename):\n","  imageset = []\n","  with open(filename, 'rb') as f:\n","      while True:\n","          try:\n","              imageset.append(unpack_drawing(f))\n","          except struct.error:\n","              break\n","  return imageset\n","\n","train_dir = '/content/drive/My Drive/Fourier/Quickdraw Dataset Small/Train/'\n","test_dir = '/content/drive/My Drive/Fourier/Quickdraw Dataset Small/Test/'\n","train_imgs = []\n","test_imgs = []\n","train_nums = []\n","test_nums = []\n","list_of_classes = [\"The Eiffel Tower\", \"The Great Wall of China\", \"The Mona Lisa\",\n","                   \"aircraft carrier\", \"airplane\", \"alarm clock\", \"ambulance\", \n","                   \"angel\", \"ant\", \"anvil\", \"apple\", \"arm\", \"asparagus\", \"axe\", \n","                   \"backpack\", \"banana\", \"bandage\", \"barn\", \"baseball bat\", \n","                   \"baseball\", \"basket\", \"basketball\", \"bathtub\", \"beach\", \"bear\", \n","                   \"beard\", \"bed\", \"bee\", \"belt\", \"bench\", \"bicycle\", \"binoculars\", \n","                   \"bird\", \"birthday cake\", \"blackberry\", \"blueberry\", \"book\", \n","                   \"boomerang\", \"bottlecap\", \"bowtie\", \"bracelet\", \"brain\", \n","                   \"bread\", \"bridge\", \"broccoli\", \"broom\", \"bucket\", \"bulldozer\", \n","                   \"bus\", \"bush\", \"butterfly\", \"cactus\", \"cake\", \"calculator\", \n","                   \"calendar\", \"camel\", \"camera\", \"camouflage\", \"campfire\", \n","                   \"candle\", \"cannon\", \"canoe\", 'car', 'carrot', \"castle\", \"cat\", \"ceiling fan\", \n","                   \"cell phone\", \"cello\", \"chair\", \"chandelier\", \"church\", \n","                   \"circle\", \"clarinet\", \"clock\", \"cloud\", \"coffee cup\", \n","                   \"compass\", \"computer\", \"cookie\", \"cooler\", \"couch\", \"cow\",\n","                   \"crab\", \"crayon\", \"crocodile\", \"crown\", \"cruise ship\", \n","                   \"cup\", \"diamond\", \"dishwasher\", \"diving board\", \"dog\", \n","                   \"dolphin\", \"donut\", \"door\", \"dragon\", \"dresser\", \"drill\", \n","                   \"drums\", \"duck\", \"dumbbell\", \"ear\", \"elbow\", \"elephant\", \n","                   \"envelope\", \"eraser\", \"eye\", \"eyeglasses\", \"face\", \"fan\",\n","                   \"feather\", \"fence\", \"finger\", \"fire hydrant\", \"fireplace\",\n","                   \"firetruck\", \"fish\", \"flamingo\", \"flashlight\", \"flip flops\", \n","                   \"floor lamp\", \"flower\", \"flying saucer\", \"foot\", \"fork\", \n","                   \"frog\", \"frying pan\", \"garden hose\", \"garden\", \"giraffe\", \n","                   \"goatee\", \"golf club\", \"grapes\", \"grass\", \"guitar\", \n","                   \"hamburger\", \"hammer\", \"hand\", \"harp\", \"hat\", \"headphones\", \n","                   \"hedgehog\", \"helicopter\", \"helmet\", \"hexagon\", \"hockey puck\", \n","                   \"hockey stick\", \"horse\", \"hospital\", \"hot air balloon\", \n","                   \"hot dog\", \"hot tub\", \"hourglass\", \"house plant\", \"house\", \n","                   \"hurricane\", \"ice cream\", \"jacket\", \"jail\", \"kangaroo\", \n","                   \"key\", \"keyboard\", \"knee\", \"knife\", \"ladder\", \"lantern\", \n","                   \"laptop\", \"leaf\", \"leg\", \"light bulb\", \"lighter\", \"lighthouse\",\n","                   \"lightning\", \"line\", \"lion\", \"lipstick\", \"lobster\", \"lollipop\",\n","                   \"mailbox\", \"map\", \"marker\", \"matches\", \"megaphone\", \"mermaid\", \n","                   \"microphone\", \"microwave\", \"monkey\", \"moon\", \"mosquito\", \n","                   \"motorbike\", \"mountain\", \"mouse\", \"moustache\", \"mouth\", \"mug\",\n","                   \"mushroom\", \"nail\", \"necklace\", \"nose\", \"ocean\", \"octagon\", \n","                   \"octopus\", \"onion\", \"oven\", \"owl\", \"paint can\", \"paintbrush\", \n","                   \"palm tree\", \"panda\", \"pants\", \"paper clip\", \"parachute\", \n","                   \"parrot\", \"passport\", \"peanut\", \"pear\", \"peas\", \"pencil\", \n","                   \"penguin\", \"piano\", \"pickup truck\", \"picture frame\", \"pig\", \n","                   \"pillow\", \"pineapple\", \"pizza\", \"pliers\", \"police car\", \n","                   \"pond\", \"pool\", \"popsicle\", \"postcard\", \"potato\", \n","                   \"power outlet\", \"purse\", \"rabbit\", \"raccoon\", \"radio\", \n","                   \"rain\", 'rainbow', 'rake', 'remote control', 'rhinoceros', \n","                   'rifle', 'river', 'roller coaster', 'rollerskates', \n","                   'sailboat', 'sandwich', 'saw', 'saxophone', 'school bus', \n","                   'scissors', 'scorpion', 'screwdriver', 'sea turtle', \n","                   'see saw', 'shark', 'sheep', 'shoe', 'shorts', 'shovel', \n","                   'sink', 'skateboard', 'skull', 'skyscraper', 'sleeping bag', \n","                   'smiley face', 'snail', 'snake', 'snorkel', 'snowflake', \n","                   'snowman', 'soccer ball', 'sock', 'speedboat', 'spider', \n","                   'spoon', 'spreadsheet', 'square', 'squiggle', 'squirrel', \n","                   'stairs', 'star', 'steak', 'stereo', 'stethoscope', 'stitches', \n","                   'stop sign', 'stove', 'strawberry', 'streetlight', \n","                   'string bean', 'submarine', 'suitcase', 'sun', 'swan', \n","                   'sweater', 'swing set', 'sword', 'syringe', 't-shirt', \n","                   'table', 'teapot', 'teddy-bear', 'telephone', 'television', \n","                   'tennis racquet', 'tent', 'tiger', 'toaster', 'toe', 'toilet', \n","                   'tooth', 'toothbrush', 'toothpaste', 'tornado', 'tractor', \n","                   'traffic light', 'train', 'tree', 'triangle', 'trombone', \n","                   'truck', 'trumpet', 'umbrella', 'underwear', 'van', 'vase', 'violin', \n","                   'washing machine', 'watermelon', 'waterslide', 'whale', \n","                   'wheel', 'windmill', 'wine bottle', 'wine glass', 'wristwatch', \n","                   'yoga', 'zebra', 'zigzag']"],"metadata":{"id":"3yQmzmk8hsDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load dataset\n","for item in list_of_classes:\n","  train_folder = train_dir + item + '.bin'\n","  test_folder = test_dir + item + '.bin'\n","  train_drawings = unpack_drawings(train_folder)\n","  train_imgs += train_drawings\n","  train_nums.append(len(train_drawings))\n","  test_drawings = unpack_drawings(test_folder)\n","  test_imgs += test_drawings\n","  test_nums.append(len(test_drawings))"],"metadata":{"id":"NNCgN-LshS9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# seed RNGs\n","torch.manual_seed(RAND_SEED)\n","random.seed(RAND_SEED)\n","\n","# create datasets\n","train_data = QuickdrawDataset(train_imgs, train_nums, is_test=False)\n","# eval_data = QuickdrawDataset(test_imgs, test_nums, is_test=False)\n","test_data = QuickdrawDataset(test_imgs, test_nums, is_test=True)\n","\n","# create dataloaders\n","g = torch.Generator()\n","g.manual_seed(RAND_SEED)\n","train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, worker_init_fn=seed_worker, generator=g)\n","# eval_loader = DataLoader(eval_data, batch_size=BATCH_SIZE, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","\n","# init model and optimizer\n","model = CNN()\n","checkpoint = torch.load(LOAD_PATH, map_location=torch.device(DEVICE))\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(DEVICE)\n","lr = 0.001\n","optim = torch.optim.Adam(model.parameters(), lr=lr)\n","optim.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","# epoch = 0\n","\n","# train for EPOCHS number of epochs then evaluate on test data with affine transformations\n","# eval_loop(dataloader=test_loader,model=model)\n","SAVE_PATH = '/content/drive/My Drive/Fourier/Saved Models/CNN Fourier Hybrid 001.pt'\n","for i in range(epoch, EPOCHS):\n","    print(\"Epoch \" + str(i + 1) + \"\\n\")\n","    train_loop(dataloader=train_loader,model=model,loss_fn=LOSS_FN,optimizer=optim)\n","    # eval_loop(dataloader=eval_loader,model=model)\n","    torch.save({\n","                'epoch': i + 1,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optim.state_dict()\n","                }, SAVE_PATH)\n","    print(\"\\n-------------------------------\\n\")\n","random.seed(RAND_SEED)\n","eval_loop(dataloader=test_loader,model=model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"a68q0nRu0Bab","executionInfo":{"status":"error","timestamp":1662698533641,"user_tz":420,"elapsed":11982339,"user":{"displayName":"Matt Watson","userId":"10419224003756061351"}},"outputId":"7c9c10ad-6ad5-446c-f2e0-d05c68152ca3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","\n","loss: 2.082483  [  512/342985]\n","loss: 2.195918  [51712/342985]\n","loss: 2.350587  [102912/342985]\n","loss: 2.245284  [154112/342985]\n","loss: 2.147120  [205312/342985]\n","loss: 2.110356  [256512/342985]\n","loss: 2.353235  [307712/342985]\n","\n","epoch avg train loss: 2.212263   epoch avg train accuracy: 0.490427\n","\n","-------------------------------\n","\n","Epoch 2\n","\n","loss: 2.152528  [  512/342985]\n","loss: 2.282310  [51712/342985]\n","loss: 1.985616  [102912/342985]\n","loss: 2.233333  [154112/342985]\n","loss: 2.177717  [205312/342985]\n","loss: 2.069471  [256512/342985]\n","loss: 2.132163  [307712/342985]\n","\n","epoch avg train loss: 2.123165   epoch avg train accuracy: 0.507410\n","\n","-------------------------------\n","\n","Epoch 3\n","\n","loss: 2.133376  [  512/342985]\n","loss: 2.211617  [51712/342985]\n","loss: 1.957432  [102912/342985]\n","loss: 2.169400  [154112/342985]\n","loss: 2.063989  [205312/342985]\n","loss: 2.047572  [256512/342985]\n","loss: 2.060049  [307712/342985]\n","\n","epoch avg train loss: 2.058886   epoch avg train accuracy: 0.519510\n","\n","-------------------------------\n","\n","Epoch 4\n","\n","loss: 1.939825  [  512/342985]\n","loss: 1.869473  [51712/342985]\n","loss: 1.941978  [102912/342985]\n","loss: 1.969089  [154112/342985]\n","loss: 2.082650  [205312/342985]\n","loss: 2.003793  [256512/342985]\n","loss: 1.947989  [307712/342985]\n","\n","epoch avg train loss: 2.007279   epoch avg train accuracy: 0.529303\n","\n","-------------------------------\n","\n","Epoch 5\n","\n","loss: 1.881960  [  512/342985]\n","loss: 1.950137  [51712/342985]\n","loss: 2.044880  [102912/342985]\n","loss: 1.901303  [154112/342985]\n","loss: 1.899938  [205312/342985]\n","loss: 2.082746  [256512/342985]\n","loss: 1.943315  [307712/342985]\n","\n","epoch avg train loss: 1.960710   epoch avg train accuracy: 0.537018\n","\n","-------------------------------\n","\n","Epoch 6\n","\n","loss: 1.784018  [  512/342985]\n","loss: 1.763832  [51712/342985]\n","loss: 1.817001  [102912/342985]\n","loss: 1.946300  [154112/342985]\n","loss: 1.947561  [205312/342985]\n","loss: 1.878411  [256512/342985]\n","loss: 2.084279  [307712/342985]\n","\n","epoch avg train loss: 1.920467   epoch avg train accuracy: 0.545581\n","\n","-------------------------------\n","\n","Epoch 7\n","\n","loss: 1.786886  [  512/342985]\n","loss: 1.859004  [51712/342985]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-8379b4479b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOSS_FN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# eval_loop(dataloader=eval_loader,model=model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     torch.save({\n","\u001b[0;32m<ipython-input-4-9f214bca8c1a>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;31m# Backpropagation to update model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model = CNN()\n","checkpoint = torch.load(LOAD_PATH, map_location=torch.device(DEVICE))\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(DEVICE)\n","\n","random.seed(RAND_SEED)\n","eval_loop(dataloader=test_loader,model=model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"So9CxsKNj7WX","executionInfo":{"status":"ok","timestamp":1662698806785,"user_tz":420,"elapsed":113381,"user":{"displayName":"Matt Watson","userId":"10419224003756061351"}},"outputId":"56f9ca29-4cb4-4c7a-8355-2011de50013c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["test accuracy: 0.410905\n"]}]},{"cell_type":"code","source":["# seed RNGs\n","torch.manual_seed(RAND_SEED)\n","random.seed(RAND_SEED)\n","\n","# create datasets\n","train_data = QuickdrawDataset(train_imgs, train_nums, is_test=False)\n","# eval_data = QuickdrawDataset(test_imgs, test_nums, is_test=False)\n","test_data = QuickdrawDataset(test_imgs, test_nums, is_test=True)\n","\n","# create dataloaders\n","g = torch.Generator()\n","g.manual_seed(RAND_SEED)\n","train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, worker_init_fn=seed_worker, generator=g)\n","# eval_loader = DataLoader(eval_data, batch_size=BATCH_SIZE, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","\n","# init model and optimizer\n","model = CNN()\n","# checkpoint = torch.load(SAVE_PATH, map_location=torch.device(DEVICE))\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(DEVICE)\n","lr = 0.001\n","optim = torch.optim.Adam(model.parameters(), lr=lr)\n","# optim.load_state_dict(checkpoint['optimizer_state_dict'])\n","# epoch = checkpoint['epoch']\n","epoch = 0\n","\n","# train for EPOCHS number of epochs then evaluate on test data with affine transformations\n","# eval_loop(dataloader=test_loader,model=model)\n","SAVE_PATH = '/content/drive/My Drive/Fourier/Saved Models/CNN Fourier Hybrid 001.pt'\n","for i in range(epoch, EPOCHS):\n","    print(\"Epoch \" + str(i + 1) + \"\\n\")\n","    train_loop(dataloader=train_loader,model=model,loss_fn=LOSS_FN,optimizer=optim)\n","    # eval_loop(dataloader=eval_loader,model=model)\n","    torch.save({\n","                'epoch': i + 1,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optim.state_dict()\n","                }, SAVE_PATH)\n","    print(\"\\n-------------------------------\\n\")\n","random.seed(RAND_SEED)\n","eval_loop(dataloader=test_loader,model=model)"],"metadata":{"id":"rHNdBKEjawEy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b00c5e18-cba1-43b0-f523-f9c414c50194"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","\n","loss: 5.840285  [  512/342985]\n","loss: 4.424491  [51712/342985]\n","loss: 4.057218  [102912/342985]\n","loss: 3.599695  [154112/342985]\n","loss: 3.353966  [205312/342985]\n","loss: 3.221454  [256512/342985]\n","loss: 3.217906  [307712/342985]\n","\n","epoch avg train loss: 3.777267   epoch avg train accuracy: 0.230071\n","\n","-------------------------------\n","\n","Epoch 2\n","\n","loss: 2.943971  [  512/342985]\n","loss: 3.047504  [51712/342985]\n","loss: 2.762405  [102912/342985]\n","loss: 2.899561  [154112/342985]\n","loss: 2.782022  [205312/342985]\n","loss: 2.612560  [256512/342985]\n","loss: 2.737793  [307712/342985]\n","\n","epoch avg train loss: 2.786425   epoch avg train accuracy: 0.386743\n","\n","-------------------------------\n","\n","Epoch 3\n","\n","loss: 2.628693  [  512/342985]\n","loss: 2.749506  [51712/342985]\n","loss: 2.447186  [102912/342985]\n","loss: 2.518146  [154112/342985]\n","loss: 2.547091  [205312/342985]\n","loss: 2.447661  [256512/342985]\n","loss: 2.485755  [307712/342985]\n","\n","epoch avg train loss: 2.490942   epoch avg train accuracy: 0.438191\n","\n","-------------------------------\n","\n","Epoch 4\n","\n","loss: 2.298654  [  512/342985]\n","loss: 2.211307  [51712/342985]\n","loss: 2.341632  [102912/342985]\n","loss: 2.274363  [154112/342985]\n","loss: 2.400641  [205312/342985]\n","loss: 2.252213  [256512/342985]\n","loss: 2.200760  [307712/342985]\n","\n","epoch avg train loss: 2.321650   epoch avg train accuracy: 0.469738\n","\n","-------------------------------\n","\n","Epoch 5\n","\n","loss: 2.177520  [  512/342985]\n"]}]}]}