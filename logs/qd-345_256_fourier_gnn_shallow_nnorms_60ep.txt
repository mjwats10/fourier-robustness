Epoch 1

loss: 5.845556  [  128/344985]
loss: 5.754654  [ 6528/344985]
loss: 5.690073  [12928/344985]
loss: 5.676709  [19328/344985]
loss: 5.547987  [25728/344985]
loss: 5.445016  [32128/344985]
loss: 5.413743  [38528/344985]
loss: 5.196812  [44928/344985]
loss: 5.300289  [51328/344985]
loss: 5.329350  [57728/344985]
loss: 5.147890  [64128/344985]
loss: 5.187465  [70528/344985]
loss: 5.129307  [76928/344985]
loss: 5.314101  [83328/344985]
loss: 4.991405  [89728/344985]
loss: 4.892277  [96128/344985]
loss: 4.908268  [102528/344985]
loss: 4.690177  [108928/344985]
loss: 4.774498  [115328/344985]
loss: 4.793363  [121728/344985]
loss: 4.696542  [128128/344985]
loss: 4.761522  [134528/344985]
loss: 4.543581  [140928/344985]
loss: 4.520959  [147328/344985]
loss: 4.422519  [153728/344985]
loss: 4.418366  [160128/344985]
loss: 4.371353  [166528/344985]
loss: 4.306620  [172928/344985]
loss: 4.555370  [179328/344985]
loss: 4.191075  [185728/344985]
loss: 4.350101  [192128/344985]
loss: 4.359773  [198528/344985]
loss: 4.396114  [204928/344985]
loss: 4.471260  [211328/344985]
loss: 4.113095  [217728/344985]
loss: 4.133726  [224128/344985]
loss: 4.242349  [230528/344985]
loss: 3.787819  [236928/344985]
loss: 4.592918  [243328/344985]
loss: 3.902259  [249728/344985]
loss: 3.836868  [256128/344985]
loss: 4.059393  [262528/344985]
loss: 4.202370  [268928/344985]
loss: 4.395044  [275328/344985]
loss: 4.086070  [281728/344985]
loss: 3.776951  [288128/344985]
loss: 4.058944  [294528/344985]
loss: 3.857806  [300928/344985]
loss: 4.294614  [307328/344985]
loss: 3.906256  [313728/344985]
loss: 3.913473  [320128/344985]
loss: 3.968676  [326528/344985]
loss: 3.723313  [332928/344985]
loss: 3.637061  [339328/344985]

epoch avg train loss: 4.5373130   epoch avg train accuracy: 0.1017667

-------------------------------

Epoch 2

loss: 3.598960  [  128/344985]
loss: 4.068893  [ 6528/344985]
loss: 3.660435  [12928/344985]
loss: 3.743942  [19328/344985]
loss: 3.483322  [25728/344985]
loss: 3.649331  [32128/344985]
loss: 3.974693  [38528/344985]
loss: 3.976955  [44928/344985]
loss: 3.542176  [51328/344985]
loss: 3.816511  [57728/344985]
loss: 3.808246  [64128/344985]
loss: 3.836173  [70528/344985]
loss: 3.653743  [76928/344985]
loss: 3.456666  [83328/344985]
loss: 3.948939  [89728/344985]
loss: 3.431305  [96128/344985]
loss: 3.878688  [102528/344985]
loss: 3.789316  [108928/344985]
loss: 3.644136  [115328/344985]
loss: 3.863974  [121728/344985]
loss: 3.674123  [128128/344985]
loss: 3.932881  [134528/344985]
loss: 3.863817  [140928/344985]
loss: 3.783284  [147328/344985]
loss: 3.689456  [153728/344985]
loss: 3.831376  [160128/344985]
loss: 3.673807  [166528/344985]
loss: 3.498236  [172928/344985]
loss: 3.558483  [179328/344985]
loss: 3.382121  [185728/344985]
loss: 3.941725  [192128/344985]
loss: 3.327251  [198528/344985]
loss: 3.194924  [204928/344985]
loss: 3.391076  [211328/344985]
loss: 3.223313  [217728/344985]
loss: 3.571224  [224128/344985]
loss: 3.631046  [230528/344985]
loss: 3.405675  [236928/344985]
loss: 3.874246  [243328/344985]
loss: 3.742115  [249728/344985]
loss: 2.930432  [256128/344985]
loss: 3.551738  [262528/344985]
loss: 3.714072  [268928/344985]
loss: 3.542057  [275328/344985]
loss: 3.318926  [281728/344985]
loss: 3.688382  [288128/344985]
loss: 3.658579  [294528/344985]
loss: 3.514300  [300928/344985]
loss: 3.339417  [307328/344985]
loss: 3.551386  [313728/344985]
loss: 3.490837  [320128/344985]
loss: 3.499209  [326528/344985]
loss: 3.120765  [332928/344985]
loss: 3.244362  [339328/344985]

epoch avg train loss: 3.6046695   epoch avg train accuracy: 0.2205226

-------------------------------

Epoch 3

loss: 3.511828  [  128/344985]
loss: 3.387717  [ 6528/344985]
loss: 3.463258  [12928/344985]
loss: 3.417134  [19328/344985]
loss: 3.105497  [25728/344985]
loss: 3.349166  [32128/344985]
loss: 2.951632  [38528/344985]
loss: 3.239964  [44928/344985]
loss: 3.576924  [51328/344985]
loss: 3.449391  [57728/344985]
loss: 3.527154  [64128/344985]
loss: 3.540488  [70528/344985]
loss: 3.328298  [76928/344985]
loss: 2.959888  [83328/344985]
loss: 3.347298  [89728/344985]
loss: 3.374871  [96128/344985]
loss: 3.369499  [102528/344985]
loss: 3.410958  [108928/344985]
loss: 3.188432  [115328/344985]
loss: 3.384784  [121728/344985]
loss: 3.044902  [128128/344985]
loss: 3.209695  [134528/344985]
loss: 3.299549  [140928/344985]
loss: 3.462770  [147328/344985]
loss: 3.142339  [153728/344985]
loss: 3.074898  [160128/344985]
loss: 3.468592  [166528/344985]
loss: 3.012058  [172928/344985]
loss: 2.951560  [179328/344985]
loss: 3.387374  [185728/344985]
loss: 2.997639  [192128/344985]
loss: 3.295795  [198528/344985]
loss: 3.502111  [204928/344985]
loss: 3.673996  [211328/344985]
loss: 3.481550  [217728/344985]
loss: 3.409553  [224128/344985]
loss: 3.276069  [230528/344985]
loss: 3.352393  [236928/344985]
loss: 3.368472  [243328/344985]
loss: 3.381988  [249728/344985]
loss: 3.564472  [256128/344985]
loss: 3.232486  [262528/344985]
loss: 3.194391  [268928/344985]
loss: 3.218818  [275328/344985]
loss: 3.060380  [281728/344985]
loss: 3.126979  [288128/344985]
loss: 2.961661  [294528/344985]
loss: 3.286908  [300928/344985]
loss: 3.142403  [307328/344985]
loss: 3.316085  [313728/344985]
loss: 3.215535  [320128/344985]
loss: 3.250191  [326528/344985]
loss: 3.132775  [332928/344985]
loss: 3.267140  [339328/344985]

epoch avg train loss: 3.2789191   epoch avg train accuracy: 0.2721597

-------------------------------

Epoch 4

loss: 2.991952  [  128/344985]
loss: 3.192451  [ 6528/344985]
loss: 3.183561  [12928/344985]
loss: 3.082287  [19328/344985]
loss: 2.973846  [25728/344985]
loss: 2.853329  [32128/344985]
loss: 3.118692  [38528/344985]
loss: 3.175650  [44928/344985]
loss: 3.105364  [51328/344985]
loss: 2.851951  [57728/344985]
loss: 3.088053  [64128/344985]
loss: 2.986313  [70528/344985]
loss: 2.883612  [76928/344985]
loss: 2.955759  [83328/344985]
loss: 3.140815  [89728/344985]
loss: 3.294018  [96128/344985]
loss: 3.273860  [102528/344985]
loss: 2.744133  [108928/344985]
loss: 2.777313  [115328/344985]
loss: 2.754023  [121728/344985]
loss: 3.256109  [128128/344985]
loss: 3.195317  [134528/344985]
loss: 3.225577  [140928/344985]
loss: 3.065522  [147328/344985]
loss: 2.905181  [153728/344985]
loss: 2.923118  [160128/344985]
loss: 2.919484  [166528/344985]
loss: 2.966798  [172928/344985]
loss: 2.955118  [179328/344985]
loss: 3.049068  [185728/344985]
loss: 2.891301  [192128/344985]
loss: 3.012555  [198528/344985]
loss: 3.365860  [204928/344985]
loss: 3.034755  [211328/344985]
loss: 2.956391  [217728/344985]
loss: 2.760131  [224128/344985]
loss: 2.972244  [230528/344985]
loss: 3.108719  [236928/344985]
loss: 2.866662  [243328/344985]
loss: 2.955106  [249728/344985]
loss: 3.343342  [256128/344985]
loss: 3.511049  [262528/344985]
loss: 2.901234  [268928/344985]
loss: 2.850943  [275328/344985]
loss: 2.994500  [281728/344985]
loss: 3.269806  [288128/344985]
loss: 3.220684  [294528/344985]
loss: 2.903960  [300928/344985]
loss: 3.123589  [307328/344985]
loss: 3.031789  [313728/344985]
loss: 3.190760  [320128/344985]
loss: 3.221192  [326528/344985]
loss: 3.103206  [332928/344985]
loss: 3.374151  [339328/344985]

epoch avg train loss: 3.0692786   epoch avg train accuracy: 0.3069699

-------------------------------

Epoch 5

loss: 3.119771  [  128/344985]
loss: 2.906428  [ 6528/344985]
loss: 2.720351  [12928/344985]
loss: 3.089644  [19328/344985]
loss: 2.920057  [25728/344985]
loss: 3.067719  [32128/344985]
loss: 2.815966  [38528/344985]
loss: 3.008920  [44928/344985]
loss: 2.874515  [51328/344985]
loss: 2.564702  [57728/344985]
loss: 2.825996  [64128/344985]
loss: 3.037463  [70528/344985]
loss: 2.723735  [76928/344985]
loss: 2.790508  [83328/344985]
loss: 2.917167  [89728/344985]
loss: 3.069089  [96128/344985]
loss: 2.929999  [102528/344985]
loss: 2.650027  [108928/344985]
loss: 3.016706  [115328/344985]
loss: 2.994645  [121728/344985]
loss: 2.722964  [128128/344985]
loss: 2.946458  [134528/344985]
loss: 3.064466  [140928/344985]
loss: 2.747673  [147328/344985]
loss: 2.997851  [153728/344985]
loss: 2.709603  [160128/344985]
loss: 3.079051  [166528/344985]
loss: 2.486879  [172928/344985]
loss: 2.946132  [179328/344985]
loss: 2.708765  [185728/344985]
loss: 3.133417  [192128/344985]
loss: 2.926062  [198528/344985]
loss: 2.979185  [204928/344985]
loss: 2.997905  [211328/344985]
loss: 2.890041  [217728/344985]
loss: 3.096346  [224128/344985]
loss: 2.851539  [230528/344985]
loss: 2.711073  [236928/344985]
loss: 3.313852  [243328/344985]
loss: 3.090451  [249728/344985]
loss: 2.989574  [256128/344985]
loss: 2.784136  [262528/344985]
loss: 3.032223  [268928/344985]
loss: 2.699903  [275328/344985]
loss: 3.061913  [281728/344985]
loss: 2.617668  [288128/344985]
loss: 3.004318  [294528/344985]
loss: 2.546005  [300928/344985]
loss: 2.889402  [307328/344985]
loss: 2.675334  [313728/344985]
loss: 2.828471  [320128/344985]
loss: 2.899962  [326528/344985]
loss: 2.868427  [332928/344985]
loss: 2.657331  [339328/344985]

epoch avg train loss: 2.9078716   epoch avg train accuracy: 0.3350465

-------------------------------

Epoch 6

loss: 2.654939  [  128/344985]
loss: 2.708875  [ 6528/344985]
loss: 2.872019  [12928/344985]
loss: 2.784168  [19328/344985]
loss: 2.753439  [25728/344985]
loss: 2.781213  [32128/344985]
loss: 2.760515  [38528/344985]
loss: 2.472680  [44928/344985]
loss: 2.723737  [51328/344985]
loss: 2.530142  [57728/344985]
loss: 2.706388  [64128/344985]
loss: 2.646449  [70528/344985]
loss: 2.703299  [76928/344985]
loss: 2.791324  [83328/344985]
loss: 2.567389  [89728/344985]
loss: 2.954560  [96128/344985]
loss: 3.039017  [102528/344985]
loss: 2.803784  [108928/344985]
loss: 2.921438  [115328/344985]
loss: 2.615953  [121728/344985]
loss: 3.125505  [128128/344985]
loss: 2.741962  [134528/344985]
loss: 2.895252  [140928/344985]
loss: 3.038053  [147328/344985]
loss: 3.108050  [153728/344985]
loss: 3.192906  [160128/344985]
loss: 2.430909  [166528/344985]
loss: 2.611193  [172928/344985]
loss: 2.586381  [179328/344985]
loss: 2.831149  [185728/344985]
loss: 2.755685  [192128/344985]
loss: 2.677120  [198528/344985]
loss: 3.008964  [204928/344985]
loss: 2.377892  [211328/344985]
loss: 3.147011  [217728/344985]
loss: 2.871698  [224128/344985]
loss: 2.808497  [230528/344985]
loss: 3.091034  [236928/344985]
loss: 3.002222  [243328/344985]
loss: 3.072391  [249728/344985]
loss: 2.630047  [256128/344985]
loss: 3.074602  [262528/344985]
loss: 2.683751  [268928/344985]
loss: 3.097881  [275328/344985]
loss: 2.571273  [281728/344985]
loss: 2.999980  [288128/344985]
loss: 2.615162  [294528/344985]
loss: 2.756548  [300928/344985]
loss: 3.147166  [307328/344985]
loss: 2.672789  [313728/344985]
loss: 2.953264  [320128/344985]
loss: 2.819067  [326528/344985]
loss: 2.908198  [332928/344985]
loss: 2.560519  [339328/344985]

epoch avg train loss: 2.7782092   epoch avg train accuracy: 0.3575779

-------------------------------

Epoch 7

loss: 2.675009  [  128/344985]
loss: 2.266534  [ 6528/344985]
loss: 2.769111  [12928/344985]
loss: 2.222163  [19328/344985]
loss: 2.498569  [25728/344985]
loss: 2.778208  [32128/344985]
loss: 2.659284  [38528/344985]
loss: 2.528975  [44928/344985]
loss: 2.611180  [51328/344985]
loss: 2.610609  [57728/344985]
loss: 2.498509  [64128/344985]
loss: 2.719129  [70528/344985]
loss: 2.726540  [76928/344985]
loss: 2.823113  [83328/344985]
loss: 2.652440  [89728/344985]
loss: 2.298483  [96128/344985]
loss: 2.569770  [102528/344985]
loss: 2.698998  [108928/344985]
loss: 2.792376  [115328/344985]
loss: 2.860159  [121728/344985]
loss: 2.369337  [128128/344985]
loss: 2.859806  [134528/344985]
loss: 2.715274  [140928/344985]
loss: 2.825555  [147328/344985]
loss: 3.021571  [153728/344985]
loss: 2.495806  [160128/344985]
loss: 2.701662  [166528/344985]
loss: 2.589967  [172928/344985]
loss: 2.512433  [179328/344985]
loss: 2.668669  [185728/344985]
loss: 2.696252  [192128/344985]
loss: 2.645976  [198528/344985]
loss: 2.734538  [204928/344985]
loss: 2.826169  [211328/344985]
loss: 2.413882  [217728/344985]
loss: 2.724880  [224128/344985]
loss: 3.034273  [230528/344985]
loss: 2.818847  [236928/344985]
loss: 2.428615  [243328/344985]
loss: 2.836389  [249728/344985]
loss: 2.739101  [256128/344985]
loss: 2.693437  [262528/344985]
loss: 2.678684  [268928/344985]
loss: 2.997241  [275328/344985]
loss: 2.716290  [281728/344985]
loss: 3.020313  [288128/344985]
loss: 2.795287  [294528/344985]
loss: 3.018699  [300928/344985]
loss: 2.780842  [307328/344985]
loss: 2.977986  [313728/344985]
loss: 2.767766  [320128/344985]
loss: 2.875036  [326528/344985]
loss: 2.529531  [332928/344985]
loss: 2.778618  [339328/344985]

epoch avg train loss: 2.6626947   epoch avg train accuracy: 0.3778715

-------------------------------

Epoch 8

loss: 2.905761  [  128/344985]
loss: 2.329409  [ 6528/344985]
loss: 2.470348  [12928/344985]
loss: 2.351400  [19328/344985]
loss: 2.536618  [25728/344985]
loss: 2.402424  [32128/344985]
loss: 2.551955  [38528/344985]
loss: 2.210417  [44928/344985]
loss: 2.487706  [51328/344985]
loss: 2.406749  [57728/344985]
loss: 2.801474  [64128/344985]
loss: 2.334534  [70528/344985]
loss: 3.007030  [76928/344985]
loss: 2.688343  [83328/344985]
loss: 2.665005  [89728/344985]
loss: 2.644428  [96128/344985]
loss: 2.487334  [102528/344985]
loss: 2.731900  [108928/344985]
loss: 2.422452  [115328/344985]
loss: 2.561123  [121728/344985]
loss: 2.608911  [128128/344985]
loss: 2.751896  [134528/344985]
loss: 2.622821  [140928/344985]
loss: 2.350451  [147328/344985]
loss: 2.496871  [153728/344985]
loss: 2.490163  [160128/344985]
loss: 2.789675  [166528/344985]
loss: 2.477823  [172928/344985]
loss: 2.558570  [179328/344985]
loss: 2.299342  [185728/344985]
loss: 2.441410  [192128/344985]
loss: 2.552351  [198528/344985]
loss: 2.876572  [204928/344985]
loss: 2.732810  [211328/344985]
loss: 2.665153  [217728/344985]
loss: 2.470304  [224128/344985]
loss: 2.530875  [230528/344985]
loss: 2.476272  [236928/344985]
loss: 2.593470  [243328/344985]
loss: 2.378891  [249728/344985]
loss: 2.664261  [256128/344985]
loss: 2.431552  [262528/344985]
loss: 2.618822  [268928/344985]
loss: 2.466594  [275328/344985]
loss: 2.322810  [281728/344985]
loss: 2.715036  [288128/344985]
loss: 2.521410  [294528/344985]
loss: 2.430173  [300928/344985]
loss: 2.544743  [307328/344985]
loss: 2.644233  [313728/344985]
loss: 2.516098  [320128/344985]
loss: 2.181988  [326528/344985]
loss: 2.625004  [332928/344985]
loss: 2.560606  [339328/344985]

epoch avg train loss: 2.5626445   epoch avg train accuracy: 0.3948867

-------------------------------

Epoch 9

loss: 2.503412  [  128/344985]
loss: 2.636874  [ 6528/344985]
loss: 2.368503  [12928/344985]
loss: 2.552433  [19328/344985]
loss: 2.595672  [25728/344985]
loss: 2.168798  [32128/344985]
loss: 2.597903  [38528/344985]
loss: 2.238613  [44928/344985]
loss: 2.803686  [51328/344985]
loss: 2.053030  [57728/344985]
loss: 2.173333  [64128/344985]
loss: 2.411990  [70528/344985]
loss: 2.510517  [76928/344985]
loss: 2.294281  [83328/344985]
loss: 2.800951  [89728/344985]
loss: 2.581851  [96128/344985]
loss: 2.354863  [102528/344985]
loss: 2.496234  [108928/344985]
loss: 2.541515  [115328/344985]
loss: 2.688545  [121728/344985]
loss: 2.327971  [128128/344985]
loss: 2.441396  [134528/344985]
loss: 2.347663  [140928/344985]
loss: 2.066138  [147328/344985]
loss: 2.560901  [153728/344985]
loss: 2.430543  [160128/344985]
loss: 2.326984  [166528/344985]
loss: 2.630114  [172928/344985]
loss: 2.689126  [179328/344985]
loss: 2.251645  [185728/344985]
loss: 2.383011  [192128/344985]
loss: 2.623070  [198528/344985]
loss: 2.282706  [204928/344985]
loss: 2.728751  [211328/344985]
loss: 2.638717  [217728/344985]
loss: 2.156474  [224128/344985]
loss: 2.559392  [230528/344985]
loss: 2.441610  [236928/344985]
loss: 2.313953  [243328/344985]
loss: 2.203945  [249728/344985]
loss: 2.286058  [256128/344985]
loss: 2.579327  [262528/344985]
loss: 2.561689  [268928/344985]
loss: 2.813328  [275328/344985]
loss: 2.560814  [281728/344985]
loss: 2.613650  [288128/344985]
loss: 2.523253  [294528/344985]
loss: 2.315523  [300928/344985]
loss: 2.517002  [307328/344985]
loss: 2.355930  [313728/344985]
loss: 2.695541  [320128/344985]
loss: 2.570740  [326528/344985]
loss: 2.830967  [332928/344985]
loss: 2.390549  [339328/344985]

epoch avg train loss: 2.4763951   epoch avg train accuracy: 0.4109483

-------------------------------

Epoch 10

loss: 2.386788  [  128/344985]
loss: 2.300642  [ 6528/344985]
loss: 2.426636  [12928/344985]
loss: 2.334407  [19328/344985]
loss: 2.221644  [25728/344985]
loss: 1.984579  [32128/344985]
loss: 2.453975  [38528/344985]
loss: 2.329895  [44928/344985]
loss: 2.247093  [51328/344985]
loss: 2.165474  [57728/344985]
loss: 2.771349  [64128/344985]
loss: 2.608620  [70528/344985]
loss: 2.613648  [76928/344985]
loss: 2.749295  [83328/344985]
loss: 2.694941  [89728/344985]
loss: 2.493226  [96128/344985]
loss: 2.405735  [102528/344985]
loss: 2.550560  [108928/344985]
loss: 2.269398  [115328/344985]
loss: 2.607909  [121728/344985]
loss: 2.605180  [128128/344985]
loss: 2.462426  [134528/344985]
loss: 2.444938  [140928/344985]
loss: 2.043173  [147328/344985]
loss: 2.331401  [153728/344985]
loss: 2.473860  [160128/344985]
loss: 2.434768  [166528/344985]
loss: 2.662591  [172928/344985]
loss: 2.372758  [179328/344985]
loss: 2.470495  [185728/344985]
loss: 2.805990  [192128/344985]
loss: 2.818832  [198528/344985]
loss: 2.291615  [204928/344985]
loss: 2.425436  [211328/344985]
loss: 2.390946  [217728/344985]
loss: 2.716663  [224128/344985]
loss: 2.328497  [230528/344985]
loss: 2.683867  [236928/344985]
loss: 2.258862  [243328/344985]
loss: 2.427893  [249728/344985]
loss: 2.373531  [256128/344985]
loss: 2.419019  [262528/344985]
loss: 2.357586  [268928/344985]
loss: 2.292768  [275328/344985]
loss: 2.250970  [281728/344985]
loss: 2.629768  [288128/344985]
loss: 2.381058  [294528/344985]
loss: 2.411092  [300928/344985]
loss: 2.611076  [307328/344985]
loss: 2.504966  [313728/344985]
loss: 2.713761  [320128/344985]
loss: 2.425970  [326528/344985]
loss: 2.233045  [332928/344985]
loss: 2.777771  [339328/344985]

epoch avg train loss: 2.3927825   epoch avg train accuracy: 0.4263925

-------------------------------

Epoch 11

loss: 2.291778  [  128/344985]
loss: 2.117403  [ 6528/344985]
loss: 2.085630  [12928/344985]
loss: 1.808506  [19328/344985]
loss: 2.020714  [25728/344985]
loss: 2.229362  [32128/344985]
loss: 2.281861  [38528/344985]
loss: 1.990862  [44928/344985]
loss: 2.219772  [51328/344985]
loss: 2.450087  [57728/344985]
loss: 2.081310  [64128/344985]
loss: 2.152099  [70528/344985]
loss: 2.385427  [76928/344985]
loss: 2.510937  [83328/344985]
loss: 2.476385  [89728/344985]
loss: 2.329716  [96128/344985]
loss: 2.597973  [102528/344985]
loss: 1.963969  [108928/344985]
loss: 2.259941  [115328/344985]
loss: 2.221140  [121728/344985]
loss: 1.777568  [128128/344985]
loss: 2.074053  [134528/344985]
loss: 2.503435  [140928/344985]
loss: 2.523397  [147328/344985]
loss: 2.104813  [153728/344985]
loss: 2.402919  [160128/344985]
loss: 2.749969  [166528/344985]
loss: 2.250283  [172928/344985]
loss: 2.120471  [179328/344985]
loss: 2.209667  [185728/344985]
loss: 2.605069  [192128/344985]
loss: 2.061648  [198528/344985]
loss: 2.568956  [204928/344985]
loss: 2.035661  [211328/344985]
loss: 2.264363  [217728/344985]
loss: 1.989554  [224128/344985]
loss: 2.451344  [230528/344985]
loss: 2.838014  [236928/344985]
loss: 2.230710  [243328/344985]
loss: 2.522039  [249728/344985]
loss: 2.511897  [256128/344985]
loss: 2.426057  [262528/344985]
loss: 2.314609  [268928/344985]
loss: 2.379465  [275328/344985]
loss: 2.322987  [281728/344985]
loss: 2.742812  [288128/344985]
loss: 2.538259  [294528/344985]
loss: 2.037781  [300928/344985]
loss: 2.743809  [307328/344985]
loss: 2.186705  [313728/344985]
loss: 2.593964  [320128/344985]
loss: 2.535983  [326528/344985]
loss: 2.080729  [332928/344985]
loss: 2.548400  [339328/344985]

epoch avg train loss: 2.3194067   epoch avg train accuracy: 0.4385002

-------------------------------

Epoch 12

loss: 2.374656  [  128/344985]
loss: 1.955578  [ 6528/344985]
loss: 2.028905  [12928/344985]
loss: 2.238669  [19328/344985]
loss: 2.127293  [25728/344985]
loss: 2.020330  [32128/344985]
loss: 1.944821  [38528/344985]
loss: 2.105837  [44928/344985]
loss: 2.491483  [51328/344985]
loss: 2.305983  [57728/344985]
loss: 2.153338  [64128/344985]
loss: 2.465982  [70528/344985]
loss: 2.256562  [76928/344985]
loss: 2.270250  [83328/344985]
loss: 2.049624  [89728/344985]
loss: 2.516842  [96128/344985]
loss: 2.172615  [102528/344985]
loss: 2.414923  [108928/344985]
loss: 2.114543  [115328/344985]
loss: 2.295592  [121728/344985]
loss: 2.173279  [128128/344985]
loss: 2.227199  [134528/344985]
loss: 2.067747  [140928/344985]
loss: 2.388107  [147328/344985]
loss: 2.255737  [153728/344985]
loss: 2.420335  [160128/344985]
loss: 2.221810  [166528/344985]
loss: 2.334533  [172928/344985]
loss: 2.307709  [179328/344985]
loss: 2.447384  [185728/344985]
loss: 2.311221  [192128/344985]
loss: 2.448261  [198528/344985]
loss: 2.157821  [204928/344985]
loss: 2.392965  [211328/344985]
loss: 2.223606  [217728/344985]
loss: 2.464157  [224128/344985]
loss: 2.390343  [230528/344985]
loss: 1.919378  [236928/344985]
loss: 2.071618  [243328/344985]
loss: 2.125451  [249728/344985]
loss: 2.049946  [256128/344985]
loss: 1.940307  [262528/344985]
loss: 2.174505  [268928/344985]
loss: 2.451420  [275328/344985]
loss: 2.451981  [281728/344985]
loss: 2.370697  [288128/344985]
loss: 2.252209  [294528/344985]
loss: 2.786194  [300928/344985]
loss: 2.288162  [307328/344985]
loss: 2.437872  [313728/344985]
loss: 2.707951  [320128/344985]
loss: 2.361418  [326528/344985]
loss: 2.205137  [332928/344985]
loss: 2.275692  [339328/344985]

epoch avg train loss: 2.2496888   epoch avg train accuracy: 0.4518747

-------------------------------

Epoch 13

loss: 2.483000  [  128/344985]
loss: 1.916271  [ 6528/344985]
loss: 2.285083  [12928/344985]
loss: 2.167867  [19328/344985]
loss: 1.916478  [25728/344985]
loss: 2.252198  [32128/344985]
loss: 2.456115  [38528/344985]
loss: 2.099967  [44928/344985]
loss: 2.140049  [51328/344985]
loss: 2.430656  [57728/344985]
loss: 2.111141  [64128/344985]
loss: 2.034260  [70528/344985]
loss: 2.356325  [76928/344985]
loss: 2.253216  [83328/344985]
loss: 2.527325  [89728/344985]
loss: 2.174276  [96128/344985]
loss: 2.091322  [102528/344985]
loss: 2.059243  [108928/344985]
loss: 2.257569  [115328/344985]
loss: 2.099011  [121728/344985]
loss: 2.217035  [128128/344985]
loss: 2.095018  [134528/344985]
loss: 2.057202  [140928/344985]
loss: 2.162611  [147328/344985]
loss: 1.978194  [153728/344985]
loss: 2.085320  [160128/344985]
loss: 2.503096  [166528/344985]
loss: 2.441530  [172928/344985]
loss: 2.149030  [179328/344985]
loss: 2.536446  [185728/344985]
loss: 2.034802  [192128/344985]
loss: 1.975859  [198528/344985]
loss: 2.149037  [204928/344985]
loss: 2.436057  [211328/344985]
loss: 2.064664  [217728/344985]
loss: 2.236224  [224128/344985]
loss: 2.028337  [230528/344985]
loss: 2.270373  [236928/344985]
loss: 2.675709  [243328/344985]
loss: 2.416551  [249728/344985]
loss: 2.012964  [256128/344985]
loss: 2.266723  [262528/344985]
loss: 2.655307  [268928/344985]
loss: 2.221726  [275328/344985]
loss: 2.474415  [281728/344985]
loss: 2.426455  [288128/344985]
loss: 2.057264  [294528/344985]
loss: 2.251308  [300928/344985]
loss: 2.428092  [307328/344985]
loss: 2.512085  [313728/344985]
loss: 2.487958  [320128/344985]
loss: 2.377710  [326528/344985]
loss: 2.280638  [332928/344985]
loss: 2.125741  [339328/344985]

epoch avg train loss: 2.1840723   epoch avg train accuracy: 0.4637999

-------------------------------

Epoch 14

loss: 1.948130  [  128/344985]
loss: 2.025492  [ 6528/344985]
loss: 2.206637  [12928/344985]
loss: 1.838078  [19328/344985]
loss: 2.006201  [25728/344985]
loss: 2.001390  [32128/344985]
loss: 2.295631  [38528/344985]
loss: 1.915067  [44928/344985]
loss: 1.688347  [51328/344985]
loss: 2.287580  [57728/344985]
loss: 2.102477  [64128/344985]
loss: 1.988827  [70528/344985]
loss: 2.072040  [76928/344985]
loss: 2.016495  [83328/344985]
loss: 2.272587  [89728/344985]
loss: 2.161150  [96128/344985]
loss: 1.929344  [102528/344985]
loss: 2.191974  [108928/344985]
loss: 2.100238  [115328/344985]
loss: 1.794270  [121728/344985]
loss: 1.952143  [128128/344985]
loss: 1.992249  [134528/344985]
loss: 2.354914  [140928/344985]
loss: 2.213719  [147328/344985]
loss: 2.022453  [153728/344985]
loss: 2.161047  [160128/344985]
loss: 2.267981  [166528/344985]
loss: 2.132361  [172928/344985]
loss: 2.219934  [179328/344985]
loss: 2.283089  [185728/344985]
loss: 2.280362  [192128/344985]
loss: 2.521043  [198528/344985]
loss: 2.279914  [204928/344985]
loss: 2.539750  [211328/344985]
loss: 2.158996  [217728/344985]
loss: 2.471448  [224128/344985]
loss: 1.986592  [230528/344985]
loss: 1.966181  [236928/344985]
loss: 2.504446  [243328/344985]
loss: 1.932121  [249728/344985]
loss: 2.340819  [256128/344985]
loss: 2.075518  [262528/344985]
loss: 2.163947  [268928/344985]
loss: 2.026107  [275328/344985]
loss: 2.319545  [281728/344985]
loss: 2.556386  [288128/344985]
loss: 2.062549  [294528/344985]
loss: 1.964172  [300928/344985]
loss: 2.190681  [307328/344985]
loss: 2.144160  [313728/344985]
loss: 2.197380  [320128/344985]
loss: 2.214462  [326528/344985]
loss: 2.380567  [332928/344985]
loss: 2.478254  [339328/344985]

epoch avg train loss: 2.1234740   epoch avg train accuracy: 0.4735713

-------------------------------

Epoch 15

loss: 1.832933  [  128/344985]
loss: 1.717159  [ 6528/344985]
loss: 2.011455  [12928/344985]
loss: 1.936478  [19328/344985]
loss: 1.661132  [25728/344985]
loss: 1.811932  [32128/344985]
loss: 1.782297  [38528/344985]
loss: 1.754591  [44928/344985]
loss: 1.788897  [51328/344985]
loss: 1.981241  [57728/344985]
loss: 2.044597  [64128/344985]
loss: 2.322684  [70528/344985]
loss: 1.619629  [76928/344985]
loss: 1.956820  [83328/344985]
loss: 1.813677  [89728/344985]
loss: 1.980281  [96128/344985]
loss: 1.807998  [102528/344985]
loss: 2.258751  [108928/344985]
loss: 1.976370  [115328/344985]
loss: 2.121642  [121728/344985]
loss: 2.146333  [128128/344985]
loss: 1.776698  [134528/344985]
loss: 1.903691  [140928/344985]
loss: 2.215391  [147328/344985]
loss: 2.014641  [153728/344985]
loss: 2.248622  [160128/344985]
loss: 2.062505  [166528/344985]
loss: 2.257094  [172928/344985]
loss: 2.118495  [179328/344985]
loss: 1.993532  [185728/344985]
loss: 2.069719  [192128/344985]
loss: 2.068659  [198528/344985]
loss: 2.121087  [204928/344985]
loss: 2.294243  [211328/344985]
loss: 2.198185  [217728/344985]
loss: 2.049592  [224128/344985]
loss: 2.031967  [230528/344985]
loss: 2.135372  [236928/344985]
loss: 2.127263  [243328/344985]
loss: 2.324982  [249728/344985]
loss: 1.884868  [256128/344985]
loss: 2.162480  [262528/344985]
loss: 1.914712  [268928/344985]
loss: 1.999687  [275328/344985]
loss: 2.283331  [281728/344985]
loss: 2.073530  [288128/344985]
loss: 2.067956  [294528/344985]
loss: 1.997884  [300928/344985]
loss: 2.161451  [307328/344985]
loss: 2.121640  [313728/344985]
loss: 1.945528  [320128/344985]
loss: 2.082171  [326528/344985]
loss: 2.063658  [332928/344985]
loss: 2.139873  [339328/344985]

epoch avg train loss: 2.0642022   epoch avg train accuracy: 0.4848327

-------------------------------

Epoch 16

loss: 1.972102  [  128/344985]
loss: 1.860006  [ 6528/344985]
loss: 1.924621  [12928/344985]
loss: 1.600735  [19328/344985]
loss: 1.838829  [25728/344985]
loss: 1.780707  [32128/344985]
loss: 1.831118  [38528/344985]
loss: 1.951031  [44928/344985]
loss: 1.721175  [51328/344985]
loss: 2.174170  [57728/344985]
loss: 1.763430  [64128/344985]
loss: 1.900455  [70528/344985]
loss: 2.027173  [76928/344985]
loss: 2.123852  [83328/344985]
loss: 1.782524  [89728/344985]
loss: 1.985961  [96128/344985]
loss: 1.912922  [102528/344985]
loss: 1.735537  [108928/344985]
loss: 1.697242  [115328/344985]
loss: 1.834889  [121728/344985]
loss: 1.635008  [128128/344985]
loss: 2.056035  [134528/344985]
loss: 1.907658  [140928/344985]
loss: 2.120210  [147328/344985]
loss: 1.993277  [153728/344985]
loss: 2.000095  [160128/344985]
loss: 1.843523  [166528/344985]
loss: 2.123390  [172928/344985]
loss: 1.835429  [179328/344985]
loss: 1.925675  [185728/344985]
loss: 1.710134  [192128/344985]
loss: 2.352751  [198528/344985]
loss: 1.843986  [204928/344985]
loss: 2.163282  [211328/344985]
loss: 1.748205  [217728/344985]
loss: 2.174151  [224128/344985]
loss: 1.817816  [230528/344985]
loss: 2.170219  [236928/344985]
loss: 2.071465  [243328/344985]
loss: 2.022637  [249728/344985]
loss: 1.847178  [256128/344985]
loss: 2.487849  [262528/344985]
loss: 1.955052  [268928/344985]
loss: 2.037551  [275328/344985]
loss: 1.933718  [281728/344985]
loss: 2.360416  [288128/344985]
loss: 2.124487  [294528/344985]
loss: 2.302504  [300928/344985]
loss: 1.987440  [307328/344985]
loss: 1.880471  [313728/344985]
loss: 2.064727  [320128/344985]
loss: 2.097225  [326528/344985]
loss: 2.244703  [332928/344985]
loss: 2.000247  [339328/344985]

epoch avg train loss: 2.0117120   epoch avg train accuracy: 0.4953839

-------------------------------

Epoch 17

loss: 1.973202  [  128/344985]
loss: 1.755265  [ 6528/344985]
loss: 2.124612  [12928/344985]
loss: 2.030265  [19328/344985]
loss: 1.826844  [25728/344985]
loss: 1.887187  [32128/344985]
loss: 1.688198  [38528/344985]
loss: 2.037125  [44928/344985]
loss: 1.953469  [51328/344985]
loss: 1.602162  [57728/344985]
loss: 1.992406  [64128/344985]
loss: 1.983068  [70528/344985]
loss: 1.854442  [76928/344985]
loss: 1.713252  [83328/344985]
loss: 1.969405  [89728/344985]
loss: 1.630796  [96128/344985]
loss: 1.629437  [102528/344985]
loss: 1.907295  [108928/344985]
loss: 1.932808  [115328/344985]
loss: 1.923726  [121728/344985]
loss: 1.856027  [128128/344985]
loss: 1.963480  [134528/344985]
loss: 1.725701  [140928/344985]
loss: 1.733845  [147328/344985]
loss: 1.933894  [153728/344985]
loss: 1.823612  [160128/344985]
loss: 2.085208  [166528/344985]
loss: 1.822247  [172928/344985]
loss: 1.880562  [179328/344985]
loss: 1.984693  [185728/344985]
loss: 1.552823  [192128/344985]
loss: 1.832874  [198528/344985]
loss: 2.145739  [204928/344985]
loss: 1.931935  [211328/344985]
loss: 2.122680  [217728/344985]
loss: 1.953303  [224128/344985]
loss: 1.994007  [230528/344985]
loss: 1.939208  [236928/344985]
loss: 1.656282  [243328/344985]
loss: 2.145333  [249728/344985]
loss: 1.889437  [256128/344985]
loss: 2.162895  [262528/344985]
loss: 1.804468  [268928/344985]
loss: 2.032362  [275328/344985]
loss: 1.993561  [281728/344985]
loss: 2.186360  [288128/344985]
loss: 2.162951  [294528/344985]
loss: 1.651375  [300928/344985]
loss: 1.895939  [307328/344985]
loss: 1.803150  [313728/344985]
loss: 2.138087  [320128/344985]
loss: 2.317393  [326528/344985]
loss: 1.936145  [332928/344985]
loss: 1.789279  [339328/344985]

epoch avg train loss: 1.9618970   epoch avg train accuracy: 0.5053292

-------------------------------

Epoch 18

loss: 1.922058  [  128/344985]
loss: 2.142539  [ 6528/344985]
loss: 1.580049  [12928/344985]
loss: 1.674058  [19328/344985]
loss: 1.847605  [25728/344985]
loss: 1.907464  [32128/344985]
loss: 1.725483  [38528/344985]
loss: 1.626189  [44928/344985]
loss: 1.583845  [51328/344985]
loss: 1.833331  [57728/344985]
loss: 1.914654  [64128/344985]
loss: 1.969066  [70528/344985]
loss: 1.887216  [76928/344985]
loss: 1.868465  [83328/344985]
loss: 2.251461  [89728/344985]
loss: 1.584591  [96128/344985]
loss: 2.049439  [102528/344985]
loss: 1.909985  [108928/344985]
loss: 1.721920  [115328/344985]
loss: 2.046899  [121728/344985]
loss: 1.945154  [128128/344985]
loss: 1.885283  [134528/344985]
loss: 1.877676  [140928/344985]
loss: 1.948258  [147328/344985]
loss: 1.836282  [153728/344985]
loss: 2.049479  [160128/344985]
loss: 2.007487  [166528/344985]
loss: 2.273810  [172928/344985]
loss: 1.693582  [179328/344985]
loss: 1.805598  [185728/344985]
loss: 2.108762  [192128/344985]
loss: 2.010639  [198528/344985]
loss: 1.975059  [204928/344985]
loss: 2.019248  [211328/344985]
loss: 1.749513  [217728/344985]
loss: 1.986734  [224128/344985]
loss: 1.939079  [230528/344985]
loss: 1.908996  [236928/344985]
loss: 1.987541  [243328/344985]
loss: 1.858999  [249728/344985]
loss: 2.071864  [256128/344985]
loss: 2.282979  [262528/344985]
loss: 2.029454  [268928/344985]
loss: 1.707078  [275328/344985]
loss: 1.975933  [281728/344985]
loss: 2.194284  [288128/344985]
loss: 2.200691  [294528/344985]
loss: 1.951381  [300928/344985]
loss: 2.273385  [307328/344985]
loss: 2.022960  [313728/344985]
loss: 2.180687  [320128/344985]
loss: 2.043792  [326528/344985]
loss: 2.034319  [332928/344985]
loss: 2.233946  [339328/344985]

epoch avg train loss: 1.9142507   epoch avg train accuracy: 0.5137064

-------------------------------

Epoch 19

loss: 1.854897  [  128/344985]
loss: 1.632738  [ 6528/344985]
loss: 1.853616  [12928/344985]
loss: 1.509486  [19328/344985]
loss: 1.502662  [25728/344985]
loss: 1.700705  [32128/344985]
loss: 1.689600  [38528/344985]
loss: 1.668545  [44928/344985]
loss: 1.768730  [51328/344985]
loss: 1.533696  [57728/344985]
loss: 1.721285  [64128/344985]
loss: 1.950905  [70528/344985]
loss: 2.020221  [76928/344985]
loss: 1.799037  [83328/344985]
loss: 1.714881  [89728/344985]
loss: 1.782052  [96128/344985]
loss: 1.855889  [102528/344985]
loss: 1.699006  [108928/344985]
loss: 1.775090  [115328/344985]
loss: 1.857382  [121728/344985]
loss: 1.838933  [128128/344985]
loss: 1.721475  [134528/344985]
loss: 1.456397  [140928/344985]
loss: 1.765463  [147328/344985]
loss: 1.858640  [153728/344985]
loss: 1.594920  [160128/344985]
loss: 1.955542  [166528/344985]
loss: 1.876837  [172928/344985]
loss: 2.132308  [179328/344985]
loss: 2.042734  [185728/344985]
loss: 2.133034  [192128/344985]
loss: 2.004801  [198528/344985]
loss: 1.923352  [204928/344985]
loss: 2.131437  [211328/344985]
loss: 1.816029  [217728/344985]
loss: 1.888552  [224128/344985]
loss: 1.906631  [230528/344985]
loss: 1.709936  [236928/344985]
loss: 1.857954  [243328/344985]
loss: 1.615857  [249728/344985]
loss: 1.714911  [256128/344985]
loss: 1.899983  [262528/344985]
loss: 1.845997  [268928/344985]
loss: 1.592839  [275328/344985]
loss: 1.925574  [281728/344985]
loss: 1.876004  [288128/344985]
loss: 1.782176  [294528/344985]
loss: 2.275622  [300928/344985]
loss: 1.918951  [307328/344985]
loss: 1.922457  [313728/344985]
loss: 2.071053  [320128/344985]
loss: 1.873601  [326528/344985]
loss: 1.925419  [332928/344985]
loss: 2.361109  [339328/344985]

epoch avg train loss: 1.8651936   epoch avg train accuracy: 0.5234633

-------------------------------

Epoch 20

loss: 1.622855  [  128/344985]
loss: 1.664560  [ 6528/344985]
loss: 1.861061  [12928/344985]
loss: 1.476418  [19328/344985]
loss: 1.638473  [25728/344985]
loss: 1.601386  [32128/344985]
loss: 1.644341  [38528/344985]
loss: 1.665854  [44928/344985]
loss: 1.599880  [51328/344985]
loss: 1.540833  [57728/344985]
loss: 1.734460  [64128/344985]
loss: 1.465560  [70528/344985]
loss: 1.571148  [76928/344985]
loss: 1.798572  [83328/344985]
loss: 1.834899  [89728/344985]
loss: 1.741631  [96128/344985]
loss: 1.651100  [102528/344985]
loss: 2.274638  [108928/344985]
loss: 1.637808  [115328/344985]
loss: 2.142062  [121728/344985]
loss: 2.039508  [128128/344985]
loss: 1.676730  [134528/344985]
loss: 2.255273  [140928/344985]
loss: 1.822567  [147328/344985]
loss: 1.873202  [153728/344985]
loss: 1.891700  [160128/344985]
loss: 1.508559  [166528/344985]
loss: 1.749729  [172928/344985]
loss: 1.820758  [179328/344985]
loss: 1.902647  [185728/344985]
loss: 2.106516  [192128/344985]
loss: 2.190474  [198528/344985]
loss: 1.715308  [204928/344985]
loss: 2.060136  [211328/344985]
loss: 1.531912  [217728/344985]
loss: 2.039738  [224128/344985]
loss: 1.713525  [230528/344985]
loss: 1.931856  [236928/344985]
loss: 1.847685  [243328/344985]
loss: 1.801752  [249728/344985]
loss: 1.867030  [256128/344985]
loss: 1.909772  [262528/344985]
loss: 1.501913  [268928/344985]
loss: 2.038529  [275328/344985]
loss: 1.691598  [281728/344985]
loss: 1.985592  [288128/344985]
loss: 2.106364  [294528/344985]
loss: 1.999040  [300928/344985]
loss: 2.023237  [307328/344985]
loss: 1.895334  [313728/344985]
loss: 1.878302  [320128/344985]
loss: 1.736555  [326528/344985]
loss: 1.773306  [332928/344985]
loss: 1.852795  [339328/344985]

epoch avg train loss: 1.8265936   epoch avg train accuracy: 0.5309100

-------------------------------

Epoch 21

loss: 1.621849  [  128/344985]
loss: 1.375125  [ 6528/344985]
loss: 1.606380  [12928/344985]
loss: 1.779495  [19328/344985]
loss: 1.553254  [25728/344985]
loss: 1.639542  [32128/344985]
loss: 1.557301  [38528/344985]
loss: 1.712612  [44928/344985]
loss: 1.622263  [51328/344985]
loss: 1.649273  [57728/344985]
loss: 1.515814  [64128/344985]
loss: 1.620474  [70528/344985]
loss: 1.874917  [76928/344985]
loss: 1.801946  [83328/344985]
loss: 1.580424  [89728/344985]
loss: 1.672580  [96128/344985]
loss: 1.624067  [102528/344985]
loss: 1.623712  [108928/344985]
loss: 1.783206  [115328/344985]
loss: 1.466884  [121728/344985]
loss: 1.429998  [128128/344985]
loss: 1.807615  [134528/344985]
loss: 1.551079  [140928/344985]
loss: 1.743053  [147328/344985]
loss: 1.790585  [153728/344985]
loss: 2.058532  [160128/344985]
loss: 1.664912  [166528/344985]
loss: 1.573873  [172928/344985]
loss: 1.918022  [179328/344985]
loss: 1.778252  [185728/344985]
loss: 1.820820  [192128/344985]
loss: 2.022180  [198528/344985]
loss: 2.015997  [204928/344985]
loss: 1.738951  [211328/344985]
loss: 1.759201  [217728/344985]
loss: 1.866470  [224128/344985]
loss: 1.884560  [230528/344985]
loss: 1.738592  [236928/344985]
loss: 1.756495  [243328/344985]
loss: 1.969838  [249728/344985]
loss: 1.858937  [256128/344985]
loss: 2.056824  [262528/344985]
loss: 1.546146  [268928/344985]
loss: 1.562729  [275328/344985]
loss: 1.603332  [281728/344985]
loss: 1.818104  [288128/344985]
loss: 2.345614  [294528/344985]
loss: 1.891241  [300928/344985]
loss: 1.860828  [307328/344985]
loss: 1.639506  [313728/344985]
loss: 1.462530  [320128/344985]
loss: 2.090799  [326528/344985]
loss: 2.025933  [332928/344985]
loss: 1.556770  [339328/344985]

epoch avg train loss: 1.7822305   epoch avg train accuracy: 0.5400525

-------------------------------

Epoch 22

loss: 1.833117  [  128/344985]
loss: 1.548635  [ 6528/344985]
loss: 1.317725  [12928/344985]
loss: 1.346923  [19328/344985]
loss: 1.583430  [25728/344985]
loss: 1.721941  [32128/344985]
loss: 1.774882  [38528/344985]
loss: 1.729670  [44928/344985]
loss: 1.647843  [51328/344985]
loss: 1.759798  [57728/344985]
loss: 1.775199  [64128/344985]
loss: 1.651674  [70528/344985]
loss: 1.762328  [76928/344985]
loss: 1.368651  [83328/344985]
loss: 1.643422  [89728/344985]
loss: 1.881038  [96128/344985]
loss: 1.707792  [102528/344985]
loss: 1.894865  [108928/344985]
loss: 2.172945  [115328/344985]
loss: 1.857235  [121728/344985]
loss: 1.630516  [128128/344985]
loss: 1.735730  [134528/344985]
loss: 1.867917  [140928/344985]
loss: 1.603893  [147328/344985]
loss: 1.562572  [153728/344985]
loss: 1.545120  [160128/344985]
loss: 1.693655  [166528/344985]
loss: 1.617165  [172928/344985]
loss: 1.618721  [179328/344985]
loss: 1.898190  [185728/344985]
loss: 1.888849  [192128/344985]
loss: 1.644166  [198528/344985]
loss: 2.008513  [204928/344985]
loss: 1.745065  [211328/344985]
loss: 1.735189  [217728/344985]
loss: 1.791209  [224128/344985]
loss: 1.737269  [230528/344985]
loss: 1.611442  [236928/344985]
loss: 1.857544  [243328/344985]
loss: 1.796146  [249728/344985]
loss: 1.697368  [256128/344985]
loss: 1.752429  [262528/344985]
loss: 1.536013  [268928/344985]
loss: 1.853787  [275328/344985]
loss: 1.824862  [281728/344985]
loss: 1.853276  [288128/344985]
loss: 1.671615  [294528/344985]
loss: 2.235140  [300928/344985]
loss: 1.718873  [307328/344985]
loss: 1.761323  [313728/344985]
loss: 1.621005  [320128/344985]
loss: 1.904471  [326528/344985]
loss: 1.871622  [332928/344985]
loss: 1.777775  [339328/344985]

epoch avg train loss: 1.7422057   epoch avg train accuracy: 0.5472963

-------------------------------

Epoch 23

loss: 1.625869  [  128/344985]
loss: 1.651929  [ 6528/344985]
loss: 1.460304  [12928/344985]
loss: 1.547943  [19328/344985]
loss: 1.663892  [25728/344985]
loss: 1.376471  [32128/344985]
loss: 1.419163  [38528/344985]
loss: 1.765131  [44928/344985]
loss: 1.776569  [51328/344985]
loss: 1.897673  [57728/344985]
loss: 1.688405  [64128/344985]
loss: 1.503400  [70528/344985]
loss: 1.713386  [76928/344985]
loss: 1.655188  [83328/344985]
loss: 1.705171  [89728/344985]
loss: 1.531675  [96128/344985]
loss: 1.464108  [102528/344985]
loss: 1.576343  [108928/344985]
loss: 1.571080  [115328/344985]
loss: 1.762825  [121728/344985]
loss: 1.374320  [128128/344985]
loss: 1.494451  [134528/344985]
loss: 1.792290  [140928/344985]
loss: 1.890917  [147328/344985]
loss: 1.853686  [153728/344985]
loss: 1.727689  [160128/344985]
loss: 1.636882  [166528/344985]
loss: 1.656748  [172928/344985]
loss: 1.934893  [179328/344985]
loss: 1.563317  [185728/344985]
loss: 1.986773  [192128/344985]
loss: 1.756395  [198528/344985]
loss: 1.596020  [204928/344985]
loss: 1.621340  [211328/344985]
loss: 1.729493  [217728/344985]
loss: 1.757350  [224128/344985]
loss: 1.590163  [230528/344985]
loss: 2.036481  [236928/344985]
loss: 2.267749  [243328/344985]
loss: 1.740822  [249728/344985]
loss: 1.660101  [256128/344985]
loss: 2.009910  [262528/344985]
loss: 1.632760  [268928/344985]
loss: 1.635442  [275328/344985]
loss: 1.620098  [281728/344985]
loss: 2.169930  [288128/344985]
loss: 1.704662  [294528/344985]
loss: 1.643803  [300928/344985]
loss: 2.066013  [307328/344985]
loss: 1.739531  [313728/344985]
loss: 2.041868  [320128/344985]
loss: 1.969007  [326528/344985]
loss: 1.951007  [332928/344985]
loss: 1.782537  [339328/344985]

epoch avg train loss: 1.7038900   epoch avg train accuracy: 0.5566329

-------------------------------

Epoch 24

loss: 1.491086  [  128/344985]
loss: 1.969383  [ 6528/344985]
loss: 1.576315  [12928/344985]
loss: 1.295965  [19328/344985]
loss: 1.402094  [25728/344985]
loss: 1.436838  [32128/344985]
loss: 1.504741  [38528/344985]
loss: 1.568919  [44928/344985]
loss: 1.264424  [51328/344985]
loss: 1.529971  [57728/344985]
loss: 1.389827  [64128/344985]
loss: 1.695252  [70528/344985]
loss: 1.525049  [76928/344985]
loss: 1.714062  [83328/344985]
loss: 1.566995  [89728/344985]
loss: 1.787719  [96128/344985]
loss: 1.530089  [102528/344985]
loss: 1.561144  [108928/344985]
loss: 1.571605  [115328/344985]
loss: 1.602577  [121728/344985]
loss: 1.635361  [128128/344985]
loss: 1.550237  [134528/344985]
loss: 1.883557  [140928/344985]
loss: 1.585300  [147328/344985]
loss: 1.977533  [153728/344985]
loss: 1.775335  [160128/344985]
loss: 1.699934  [166528/344985]
loss: 1.500367  [172928/344985]
loss: 1.747149  [179328/344985]
loss: 1.700774  [185728/344985]
loss: 1.714855  [192128/344985]
loss: 1.618101  [198528/344985]
loss: 1.709469  [204928/344985]
loss: 1.885978  [211328/344985]
loss: 1.502494  [217728/344985]
loss: 1.749640  [224128/344985]
loss: 1.909944  [230528/344985]
loss: 1.888624  [236928/344985]
loss: 1.740909  [243328/344985]
loss: 1.433668  [249728/344985]
loss: 1.900882  [256128/344985]
loss: 1.967444  [262528/344985]
loss: 1.696031  [268928/344985]
loss: 2.110482  [275328/344985]
loss: 1.818335  [281728/344985]
loss: 1.830108  [288128/344985]
loss: 1.549640  [294528/344985]
loss: 1.669945  [300928/344985]
loss: 2.039149  [307328/344985]
loss: 1.926705  [313728/344985]
loss: 1.915364  [320128/344985]
loss: 1.698657  [326528/344985]
loss: 1.667591  [332928/344985]
loss: 1.521263  [339328/344985]

epoch avg train loss: 1.6719639   epoch avg train accuracy: 0.5622824

-------------------------------

Epoch 25

loss: 1.527548  [  128/344985]
loss: 1.801502  [ 6528/344985]
loss: 1.599965  [12928/344985]
loss: 1.437980  [19328/344985]
loss: 1.508253  [25728/344985]
loss: 1.664123  [32128/344985]
loss: 1.413622  [38528/344985]
loss: 1.456003  [44928/344985]
loss: 1.473505  [51328/344985]
loss: 1.639189  [57728/344985]
loss: 1.649364  [64128/344985]
loss: 1.485654  [70528/344985]
loss: 1.558080  [76928/344985]
loss: 1.598484  [83328/344985]
loss: 1.507280  [89728/344985]
loss: 1.635892  [96128/344985]
loss: 1.228636  [102528/344985]
loss: 1.684082  [108928/344985]
loss: 1.584946  [115328/344985]
loss: 1.726284  [121728/344985]
loss: 1.689879  [128128/344985]
loss: 1.450212  [134528/344985]
loss: 1.523324  [140928/344985]
loss: 1.487607  [147328/344985]
loss: 1.349010  [153728/344985]
loss: 1.783434  [160128/344985]
loss: 1.766020  [166528/344985]
loss: 1.509665  [172928/344985]
loss: 1.392069  [179328/344985]
loss: 1.503508  [185728/344985]
loss: 1.610610  [192128/344985]
loss: 1.613043  [198528/344985]
loss: 1.771866  [204928/344985]
loss: 1.859529  [211328/344985]
loss: 1.493128  [217728/344985]
loss: 1.457546  [224128/344985]
loss: 1.952761  [230528/344985]
loss: 1.736446  [236928/344985]
loss: 1.581486  [243328/344985]
loss: 1.687979  [249728/344985]
loss: 1.712373  [256128/344985]
loss: 1.870662  [262528/344985]
loss: 1.566838  [268928/344985]
loss: 1.664309  [275328/344985]
loss: 2.173392  [281728/344985]
loss: 1.574601  [288128/344985]
loss: 1.975210  [294528/344985]
loss: 1.720721  [300928/344985]
loss: 1.892425  [307328/344985]
loss: 1.855656  [313728/344985]
loss: 1.645361  [320128/344985]
loss: 1.752681  [326528/344985]
loss: 1.912862  [332928/344985]
loss: 1.774992  [339328/344985]

epoch avg train loss: 1.6336509   epoch avg train accuracy: 0.5711466

-------------------------------

Epoch 26

loss: 1.831889  [  128/344985]
loss: 1.773929  [ 6528/344985]
loss: 1.387474  [12928/344985]
loss: 1.256110  [19328/344985]
loss: 1.240649  [25728/344985]
loss: 1.475361  [32128/344985]
loss: 1.505634  [38528/344985]
loss: 1.263440  [44928/344985]
loss: 1.370474  [51328/344985]
loss: 1.632793  [57728/344985]
loss: 1.363097  [64128/344985]
loss: 1.284435  [70528/344985]
loss: 1.452467  [76928/344985]
loss: 1.310138  [83328/344985]
loss: 1.456754  [89728/344985]
loss: 1.427191  [96128/344985]
loss: 1.390183  [102528/344985]
loss: 1.637615  [108928/344985]
loss: 1.478031  [115328/344985]
loss: 1.698816  [121728/344985]
loss: 1.611335  [128128/344985]
loss: 1.665881  [134528/344985]
loss: 1.634777  [140928/344985]
loss: 1.537647  [147328/344985]
loss: 1.514943  [153728/344985]
loss: 1.709611  [160128/344985]
loss: 1.424289  [166528/344985]
loss: 1.645529  [172928/344985]
loss: 1.659359  [179328/344985]
loss: 1.528403  [185728/344985]
loss: 1.589590  [192128/344985]
loss: 1.490152  [198528/344985]
loss: 1.505189  [204928/344985]
loss: 1.627591  [211328/344985]
loss: 2.011224  [217728/344985]
loss: 1.785156  [224128/344985]
loss: 1.728749  [230528/344985]
loss: 1.606950  [236928/344985]
loss: 1.690551  [243328/344985]
loss: 1.429078  [249728/344985]
loss: 1.706877  [256128/344985]
loss: 2.041763  [262528/344985]
loss: 1.680540  [268928/344985]
loss: 1.590301  [275328/344985]
loss: 1.816078  [281728/344985]
loss: 1.422828  [288128/344985]
loss: 1.666675  [294528/344985]
loss: 1.429924  [300928/344985]
loss: 1.569210  [307328/344985]
loss: 1.476224  [313728/344985]
loss: 1.897230  [320128/344985]
loss: 1.704582  [326528/344985]
loss: 1.809926  [332928/344985]
loss: 1.337625  [339328/344985]

epoch avg train loss: 1.6021334   epoch avg train accuracy: 0.5764975

-------------------------------

Epoch 27

loss: 1.284690  [  128/344985]
loss: 1.329765  [ 6528/344985]
loss: 1.256312  [12928/344985]
loss: 1.533072  [19328/344985]
loss: 1.579534  [25728/344985]
loss: 1.855542  [32128/344985]
loss: 1.320461  [38528/344985]
loss: 1.586873  [44928/344985]
loss: 1.522817  [51328/344985]
loss: 1.368751  [57728/344985]
loss: 1.472909  [64128/344985]
loss: 1.363445  [70528/344985]
loss: 1.578239  [76928/344985]
loss: 1.512580  [83328/344985]
loss: 1.320793  [89728/344985]
loss: 1.516673  [96128/344985]
loss: 1.495142  [102528/344985]
loss: 1.621095  [108928/344985]
loss: 1.512447  [115328/344985]
loss: 1.359935  [121728/344985]
loss: 1.746219  [128128/344985]
loss: 1.683692  [134528/344985]
loss: 1.453529  [140928/344985]
loss: 1.305988  [147328/344985]
loss: 1.505130  [153728/344985]
loss: 1.446256  [160128/344985]
loss: 1.794120  [166528/344985]
loss: 1.572371  [172928/344985]
loss: 1.499211  [179328/344985]
loss: 1.642333  [185728/344985]
loss: 1.610484  [192128/344985]
loss: 1.644158  [198528/344985]
loss: 1.686017  [204928/344985]
loss: 1.698804  [211328/344985]
loss: 1.781431  [217728/344985]
loss: 1.962651  [224128/344985]
loss: 1.760141  [230528/344985]
loss: 2.123958  [236928/344985]
loss: 1.943570  [243328/344985]
loss: 1.763930  [249728/344985]
loss: 1.670767  [256128/344985]
loss: 1.522533  [262528/344985]
loss: 1.720473  [268928/344985]
loss: 1.697097  [275328/344985]
loss: 1.760913  [281728/344985]
loss: 1.825102  [288128/344985]
loss: 1.830400  [294528/344985]
loss: 1.730391  [300928/344985]
loss: 1.549066  [307328/344985]
loss: 1.825535  [313728/344985]
loss: 1.831403  [320128/344985]
loss: 1.728820  [326528/344985]
loss: 1.694760  [332928/344985]
loss: 1.667779  [339328/344985]

epoch avg train loss: 1.5753865   epoch avg train accuracy: 0.5839471

-------------------------------

Epoch 28

loss: 1.410456  [  128/344985]
loss: 1.483051  [ 6528/344985]
loss: 1.120024  [12928/344985]
loss: 1.003412  [19328/344985]
loss: 1.389409  [25728/344985]
loss: 1.401165  [32128/344985]
loss: 1.528198  [38528/344985]
loss: 1.314234  [44928/344985]
loss: 1.264095  [51328/344985]
loss: 1.470674  [57728/344985]
loss: 1.531779  [64128/344985]
loss: 1.604866  [70528/344985]
loss: 1.498727  [76928/344985]
loss: 1.497330  [83328/344985]
loss: 1.355437  [89728/344985]
loss: 1.348057  [96128/344985]
loss: 1.630843  [102528/344985]
loss: 1.483370  [108928/344985]
loss: 1.154939  [115328/344985]
loss: 0.960925  [121728/344985]
loss: 1.763646  [128128/344985]
loss: 1.440825  [134528/344985]
loss: 1.696556  [140928/344985]
loss: 1.419029  [147328/344985]
loss: 1.488722  [153728/344985]
loss: 1.688358  [160128/344985]
loss: 1.770665  [166528/344985]
loss: 1.592325  [172928/344985]
loss: 1.630666  [179328/344985]
loss: 1.264379  [185728/344985]
loss: 1.435417  [192128/344985]
loss: 1.804315  [198528/344985]
loss: 1.420383  [204928/344985]
loss: 1.552221  [211328/344985]
loss: 1.475204  [217728/344985]
loss: 1.727736  [224128/344985]
loss: 1.734799  [230528/344985]
loss: 1.475059  [236928/344985]
loss: 1.585913  [243328/344985]
loss: 1.475971  [249728/344985]
loss: 1.683366  [256128/344985]
loss: 1.235245  [262528/344985]
loss: 1.641717  [268928/344985]
loss: 1.419188  [275328/344985]
loss: 1.734402  [281728/344985]
loss: 1.769693  [288128/344985]
loss: 1.998497  [294528/344985]
loss: 1.571947  [300928/344985]
loss: 1.799035  [307328/344985]
loss: 1.614628  [313728/344985]
loss: 1.884716  [320128/344985]
loss: 1.339899  [326528/344985]
loss: 1.754386  [332928/344985]
loss: 1.605910  [339328/344985]

epoch avg train loss: 1.5410185   epoch avg train accuracy: 0.5910866

-------------------------------

Epoch 29

loss: 1.569200  [  128/344985]
loss: 1.228630  [ 6528/344985]
loss: 1.284480  [12928/344985]
loss: 1.196325  [19328/344985]
loss: 1.056076  [25728/344985]
loss: 1.148622  [32128/344985]
loss: 1.063556  [38528/344985]
loss: 1.340644  [44928/344985]
loss: 1.274307  [51328/344985]
loss: 1.385183  [57728/344985]
loss: 1.275075  [64128/344985]
loss: 1.492566  [70528/344985]
loss: 1.357866  [76928/344985]
loss: 1.336815  [83328/344985]
loss: 1.325815  [89728/344985]
loss: 1.907422  [96128/344985]
loss: 1.527619  [102528/344985]
loss: 1.364345  [108928/344985]
loss: 1.411312  [115328/344985]
loss: 1.151379  [121728/344985]
loss: 1.426276  [128128/344985]
loss: 1.305214  [134528/344985]
loss: 1.709417  [140928/344985]
loss: 1.488323  [147328/344985]
loss: 1.370430  [153728/344985]
loss: 1.357438  [160128/344985]
loss: 1.554764  [166528/344985]
loss: 1.286638  [172928/344985]
loss: 1.384883  [179328/344985]
loss: 1.558268  [185728/344985]
loss: 1.656257  [192128/344985]
loss: 1.640861  [198528/344985]
loss: 1.650364  [204928/344985]
loss: 1.789226  [211328/344985]
loss: 1.532099  [217728/344985]
loss: 1.691084  [224128/344985]
loss: 1.736505  [230528/344985]
loss: 1.613809  [236928/344985]
loss: 1.555796  [243328/344985]
loss: 1.688375  [249728/344985]
loss: 1.352560  [256128/344985]
loss: 1.773927  [262528/344985]
loss: 1.451972  [268928/344985]
loss: 1.373582  [275328/344985]
loss: 1.606534  [281728/344985]
loss: 1.452099  [288128/344985]
loss: 1.532858  [294528/344985]
loss: 1.758230  [300928/344985]
loss: 1.495823  [307328/344985]
loss: 1.641790  [313728/344985]
loss: 1.567140  [320128/344985]
loss: 1.656297  [326528/344985]
loss: 1.477400  [332928/344985]
loss: 1.778708  [339328/344985]

epoch avg train loss: 1.5204685   epoch avg train accuracy: 0.5957563

-------------------------------

Epoch 30

loss: 1.333415  [  128/344985]
loss: 0.957166  [ 6528/344985]
loss: 1.413432  [12928/344985]
loss: 1.494160  [19328/344985]
loss: 1.311522  [25728/344985]
loss: 1.458257  [32128/344985]
loss: 1.160392  [38528/344985]
loss: 1.446931  [44928/344985]
loss: 1.240343  [51328/344985]
loss: 1.553569  [57728/344985]
loss: 1.253120  [64128/344985]
loss: 1.448915  [70528/344985]
loss: 1.104121  [76928/344985]
loss: 1.239182  [83328/344985]
loss: 1.375066  [89728/344985]
loss: 1.263127  [96128/344985]
loss: 1.133880  [102528/344985]
loss: 1.219430  [108928/344985]
loss: 1.414454  [115328/344985]
loss: 1.459211  [121728/344985]
loss: 1.241565  [128128/344985]
loss: 1.591025  [134528/344985]
loss: 1.693138  [140928/344985]
loss: 1.476808  [147328/344985]
loss: 1.554250  [153728/344985]
loss: 1.389947  [160128/344985]
loss: 1.410532  [166528/344985]
loss: 1.277516  [172928/344985]
loss: 1.156720  [179328/344985]
loss: 1.410536  [185728/344985]
loss: 1.414550  [192128/344985]
loss: 1.258962  [198528/344985]
loss: 1.557185  [204928/344985]
loss: 1.460831  [211328/344985]
loss: 1.851983  [217728/344985]
loss: 1.413611  [224128/344985]
loss: 1.625076  [230528/344985]
loss: 1.694948  [236928/344985]
loss: 1.750169  [243328/344985]
loss: 1.556722  [249728/344985]
loss: 1.552188  [256128/344985]
loss: 1.604503  [262528/344985]
loss: 1.459840  [268928/344985]
loss: 1.523262  [275328/344985]
loss: 1.716620  [281728/344985]
loss: 1.486549  [288128/344985]
loss: 1.722377  [294528/344985]
loss: 1.655573  [300928/344985]
loss: 1.937948  [307328/344985]
loss: 1.536427  [313728/344985]
loss: 1.707726  [320128/344985]
loss: 1.484729  [326528/344985]
loss: 1.475294  [332928/344985]
loss: 1.474595  [339328/344985]

epoch avg train loss: 1.4892320   epoch avg train accuracy: 0.6025016

-------------------------------

Epoch 31

loss: 1.193013  [  128/344985]
loss: 1.119133  [ 6528/344985]
loss: 1.253508  [12928/344985]
loss: 1.353887  [19328/344985]
loss: 1.399244  [25728/344985]
loss: 1.522764  [32128/344985]
loss: 1.311058  [38528/344985]
loss: 1.500311  [44928/344985]
loss: 1.364360  [51328/344985]
loss: 1.404223  [57728/344985]
loss: 1.447287  [64128/344985]
loss: 1.475178  [70528/344985]
loss: 1.546633  [76928/344985]
loss: 1.694576  [83328/344985]
loss: 1.536056  [89728/344985]
loss: 1.191857  [96128/344985]
loss: 1.537015  [102528/344985]
loss: 1.356198  [108928/344985]
loss: 1.615190  [115328/344985]
loss: 1.459322  [121728/344985]
loss: 1.537035  [128128/344985]
loss: 1.559522  [134528/344985]
loss: 1.259551  [140928/344985]
loss: 1.429507  [147328/344985]
loss: 1.419659  [153728/344985]
loss: 1.547558  [160128/344985]
loss: 1.463463  [166528/344985]
loss: 1.497355  [172928/344985]
loss: 1.641863  [179328/344985]
loss: 1.669529  [185728/344985]
loss: 1.514215  [192128/344985]
loss: 1.604357  [198528/344985]
loss: 1.488581  [204928/344985]
loss: 1.491215  [211328/344985]
loss: 1.510862  [217728/344985]
loss: 1.951220  [224128/344985]
loss: 1.458273  [230528/344985]
loss: 1.636113  [236928/344985]
loss: 1.528437  [243328/344985]
loss: 1.744568  [249728/344985]
loss: 1.527080  [256128/344985]
loss: 1.790310  [262528/344985]
loss: 1.607101  [268928/344985]
loss: 1.524890  [275328/344985]
loss: 1.848878  [281728/344985]
loss: 1.510594  [288128/344985]
loss: 1.293204  [294528/344985]
loss: 1.613636  [300928/344985]
loss: 1.514085  [307328/344985]
loss: 1.797418  [313728/344985]
loss: 1.546963  [320128/344985]
loss: 1.545279  [326528/344985]
loss: 1.703936  [332928/344985]
loss: 1.712603  [339328/344985]

epoch avg train loss: 1.4728002   epoch avg train accuracy: 0.6071945

-------------------------------

Epoch 32

loss: 1.154591  [  128/344985]
loss: 1.107414  [ 6528/344985]
loss: 1.250286  [12928/344985]
loss: 1.565684  [19328/344985]
loss: 1.022877  [25728/344985]
loss: 1.157691  [32128/344985]
loss: 1.210619  [38528/344985]
loss: 1.159385  [44928/344985]
loss: 1.289670  [51328/344985]
loss: 1.259290  [57728/344985]
loss: 1.356386  [64128/344985]
loss: 1.423062  [70528/344985]
loss: 1.355418  [76928/344985]
loss: 1.462314  [83328/344985]
loss: 1.433741  [89728/344985]
loss: 1.324655  [96128/344985]
loss: 1.353566  [102528/344985]
loss: 1.612224  [108928/344985]
loss: 1.493749  [115328/344985]
loss: 1.231138  [121728/344985]
loss: 1.479514  [128128/344985]
loss: 1.115329  [134528/344985]
loss: 1.248551  [140928/344985]
loss: 1.362363  [147328/344985]
loss: 1.639461  [153728/344985]
loss: 1.671458  [160128/344985]
loss: 1.476480  [166528/344985]
loss: 1.327438  [172928/344985]
loss: 1.540701  [179328/344985]
loss: 1.795419  [185728/344985]
loss: 1.229828  [192128/344985]
loss: 1.888544  [198528/344985]
loss: 1.875119  [204928/344985]
loss: 1.181649  [211328/344985]
loss: 1.491183  [217728/344985]
loss: 1.775519  [224128/344985]
loss: 1.381535  [230528/344985]
loss: 1.496267  [236928/344985]
loss: 1.487337  [243328/344985]
loss: 1.144475  [249728/344985]
loss: 1.526756  [256128/344985]
loss: 1.482125  [262528/344985]
loss: 1.696332  [268928/344985]
loss: 1.531524  [275328/344985]
loss: 1.702362  [281728/344985]
loss: 1.493275  [288128/344985]
loss: 1.504405  [294528/344985]
loss: 1.744991  [300928/344985]
loss: 1.297168  [307328/344985]
loss: 1.695456  [313728/344985]
loss: 1.301618  [320128/344985]
loss: 1.558448  [326528/344985]
loss: 1.413573  [332928/344985]
loss: 1.710877  [339328/344985]

epoch avg train loss: 1.4447674   epoch avg train accuracy: 0.6122411

-------------------------------

Epoch 33

loss: 1.333184  [  128/344985]
loss: 1.173167  [ 6528/344985]
loss: 1.277612  [12928/344985]
loss: 1.179825  [19328/344985]
loss: 1.349440  [25728/344985]
loss: 1.179722  [32128/344985]
loss: 1.401460  [38528/344985]
loss: 1.278257  [44928/344985]
loss: 1.073729  [51328/344985]
loss: 1.254770  [57728/344985]
loss: 1.403558  [64128/344985]
loss: 1.277726  [70528/344985]
loss: 1.234176  [76928/344985]
loss: 1.363191  [83328/344985]
loss: 1.433249  [89728/344985]
loss: 1.433295  [96128/344985]
loss: 1.163136  [102528/344985]
loss: 1.601962  [108928/344985]
loss: 1.222289  [115328/344985]
loss: 1.634461  [121728/344985]
loss: 1.288930  [128128/344985]
loss: 1.479033  [134528/344985]
loss: 1.222924  [140928/344985]
loss: 1.521185  [147328/344985]
loss: 1.424140  [153728/344985]
loss: 1.429593  [160128/344985]
loss: 1.328616  [166528/344985]
loss: 1.617643  [172928/344985]
loss: 1.985980  [179328/344985]
loss: 1.294178  [185728/344985]
loss: 1.732830  [192128/344985]
loss: 1.559192  [198528/344985]
loss: 1.821225  [204928/344985]
loss: 1.418634  [211328/344985]
loss: 1.524250  [217728/344985]
loss: 1.282530  [224128/344985]
loss: 1.562168  [230528/344985]
loss: 1.234388  [236928/344985]
loss: 1.393319  [243328/344985]
loss: 1.381165  [249728/344985]
loss: 1.253499  [256128/344985]
loss: 1.440359  [262528/344985]
loss: 1.343283  [268928/344985]
loss: 1.757101  [275328/344985]
loss: 1.328402  [281728/344985]
loss: 1.554872  [288128/344985]
loss: 1.510112  [294528/344985]
loss: 1.841781  [300928/344985]
loss: 1.616006  [307328/344985]
loss: 1.639090  [313728/344985]
loss: 1.599537  [320128/344985]
loss: 1.682787  [326528/344985]
loss: 1.616939  [332928/344985]
loss: 1.482681  [339328/344985]

epoch avg train loss: 1.4214405   epoch avg train accuracy: 0.6178182

-------------------------------

Epoch 34

loss: 1.277038  [  128/344985]
loss: 1.344349  [ 6528/344985]
loss: 1.064424  [12928/344985]
loss: 1.194290  [19328/344985]
loss: 1.423125  [25728/344985]
loss: 1.055556  [32128/344985]
loss: 1.794072  [38528/344985]
loss: 1.216448  [44928/344985]
loss: 1.546850  [51328/344985]
loss: 1.183281  [57728/344985]
loss: 1.313983  [64128/344985]
loss: 1.283916  [70528/344985]
loss: 1.336907  [76928/344985]
loss: 1.357463  [83328/344985]
loss: 1.254886  [89728/344985]
loss: 1.109122  [96128/344985]
loss: 1.252855  [102528/344985]
loss: 1.529514  [108928/344985]
loss: 1.666479  [115328/344985]
loss: 1.338449  [121728/344985]
loss: 1.563476  [128128/344985]
loss: 1.612679  [134528/344985]
loss: 1.563701  [140928/344985]
loss: 1.306412  [147328/344985]
loss: 1.338169  [153728/344985]
loss: 1.272864  [160128/344985]
loss: 1.582826  [166528/344985]
loss: 1.418687  [172928/344985]
loss: 1.327285  [179328/344985]
loss: 1.395149  [185728/344985]
loss: 1.219856  [192128/344985]
loss: 1.381812  [198528/344985]
loss: 1.414195  [204928/344985]
loss: 1.369939  [211328/344985]
loss: 1.350874  [217728/344985]
loss: 1.585417  [224128/344985]
loss: 1.610694  [230528/344985]
loss: 1.593518  [236928/344985]
loss: 1.510684  [243328/344985]
loss: 1.679825  [249728/344985]
loss: 1.418314  [256128/344985]
loss: 1.384664  [262528/344985]
loss: 1.684281  [268928/344985]
loss: 1.486895  [275328/344985]
loss: 1.449690  [281728/344985]
loss: 1.444158  [288128/344985]
loss: 1.935450  [294528/344985]
loss: 1.370719  [300928/344985]
loss: 1.529945  [307328/344985]
loss: 1.346702  [313728/344985]
loss: 1.392752  [320128/344985]
loss: 1.665084  [326528/344985]
loss: 1.530351  [332928/344985]
loss: 1.469164  [339328/344985]

epoch avg train loss: 1.4016816   epoch avg train accuracy: 0.6226213

-------------------------------

Epoch 35

loss: 1.163099  [  128/344985]
loss: 1.466258  [ 6528/344985]
loss: 1.120875  [12928/344985]
loss: 1.128507  [19328/344985]
loss: 1.328656  [25728/344985]
loss: 1.470360  [32128/344985]
loss: 1.060425  [38528/344985]
loss: 1.216646  [44928/344985]
loss: 1.229227  [51328/344985]
loss: 1.102370  [57728/344985]
loss: 1.202131  [64128/344985]
loss: 1.237408  [70528/344985]
loss: 1.249744  [76928/344985]
loss: 1.571845  [83328/344985]
loss: 1.669749  [89728/344985]
loss: 1.142728  [96128/344985]
loss: 1.402082  [102528/344985]
loss: 1.230681  [108928/344985]
loss: 1.454490  [115328/344985]
loss: 1.278147  [121728/344985]
loss: 1.459157  [128128/344985]
loss: 1.375408  [134528/344985]
loss: 1.397798  [140928/344985]
loss: 1.187648  [147328/344985]
loss: 1.417328  [153728/344985]
loss: 1.524975  [160128/344985]
loss: 1.192956  [166528/344985]
loss: 1.351140  [172928/344985]
loss: 1.483184  [179328/344985]
loss: 1.523121  [185728/344985]
loss: 1.467554  [192128/344985]
loss: 1.536561  [198528/344985]
loss: 1.337339  [204928/344985]
loss: 1.700017  [211328/344985]
loss: 1.239457  [217728/344985]
loss: 1.442980  [224128/344985]
loss: 1.659000  [230528/344985]
loss: 1.492903  [236928/344985]
loss: 1.218703  [243328/344985]
loss: 1.651839  [249728/344985]
loss: 1.139922  [256128/344985]
loss: 1.648153  [262528/344985]
loss: 1.464338  [268928/344985]
loss: 1.464191  [275328/344985]
loss: 1.638530  [281728/344985]
loss: 1.649726  [288128/344985]
loss: 1.302238  [294528/344985]
loss: 1.572056  [300928/344985]
loss: 1.394021  [307328/344985]
loss: 1.385824  [313728/344985]
loss: 1.394107  [320128/344985]
loss: 1.442348  [326528/344985]
loss: 1.621900  [332928/344985]
loss: 1.456844  [339328/344985]

epoch avg train loss: 1.3833055   epoch avg train accuracy: 0.6266736

-------------------------------

Epoch 36

loss: 1.101375  [  128/344985]
loss: 1.302089  [ 6528/344985]
loss: 1.105105  [12928/344985]
loss: 1.130837  [19328/344985]
loss: 1.116755  [25728/344985]
loss: 1.252615  [32128/344985]
loss: 1.196115  [38528/344985]
loss: 1.183126  [44928/344985]
loss: 1.114030  [51328/344985]
loss: 0.968478  [57728/344985]
loss: 1.339922  [64128/344985]
loss: 1.322531  [70528/344985]
loss: 1.510929  [76928/344985]
loss: 1.008964  [83328/344985]
loss: 1.351398  [89728/344985]
loss: 1.303420  [96128/344985]
loss: 1.183266  [102528/344985]
loss: 0.941771  [108928/344985]
loss: 1.417158  [115328/344985]
loss: 1.139411  [121728/344985]
loss: 1.279652  [128128/344985]
loss: 1.760100  [134528/344985]
loss: 1.545224  [140928/344985]
loss: 1.369673  [147328/344985]
loss: 1.196420  [153728/344985]
loss: 1.664288  [160128/344985]
loss: 1.281478  [166528/344985]
loss: 1.346406  [172928/344985]
loss: 1.413730  [179328/344985]
loss: 1.753442  [185728/344985]
loss: 1.244871  [192128/344985]
loss: 1.210235  [198528/344985]
loss: 1.122005  [204928/344985]
loss: 1.270227  [211328/344985]
loss: 1.154740  [217728/344985]
loss: 1.349106  [224128/344985]
loss: 1.624840  [230528/344985]
loss: 1.661829  [236928/344985]
loss: 1.559789  [243328/344985]
loss: 1.125769  [249728/344985]
loss: 1.122255  [256128/344985]
loss: 1.417582  [262528/344985]
loss: 1.209000  [268928/344985]
loss: 1.426702  [275328/344985]
loss: 1.289859  [281728/344985]
loss: 1.240930  [288128/344985]
loss: 1.626584  [294528/344985]
loss: 1.782531  [300928/344985]
loss: 1.263427  [307328/344985]
loss: 1.599133  [313728/344985]
loss: 1.320156  [320128/344985]
loss: 1.641562  [326528/344985]
loss: 1.270603  [332928/344985]
loss: 1.474770  [339328/344985]

epoch avg train loss: 1.3577248   epoch avg train accuracy: 0.6340073

-------------------------------

Epoch 37

loss: 1.157997  [  128/344985]
loss: 1.094682  [ 6528/344985]
loss: 1.008620  [12928/344985]
loss: 1.288828  [19328/344985]
loss: 1.433538  [25728/344985]
loss: 1.091697  [32128/344985]
loss: 1.056798  [38528/344985]
loss: 1.278527  [44928/344985]
loss: 1.173707  [51328/344985]
loss: 1.304730  [57728/344985]
loss: 1.189539  [64128/344985]
loss: 1.157426  [70528/344985]
loss: 1.185270  [76928/344985]
loss: 1.365517  [83328/344985]
loss: 1.470054  [89728/344985]
loss: 1.201322  [96128/344985]
loss: 1.370498  [102528/344985]
loss: 1.372614  [108928/344985]
loss: 1.292220  [115328/344985]
loss: 1.430285  [121728/344985]
loss: 1.368877  [128128/344985]
loss: 1.378211  [134528/344985]
loss: 1.356326  [140928/344985]
loss: 1.199217  [147328/344985]
loss: 1.382612  [153728/344985]
loss: 1.399580  [160128/344985]
loss: 1.276748  [166528/344985]
loss: 1.504344  [172928/344985]
loss: 1.688192  [179328/344985]
loss: 1.180889  [185728/344985]
loss: 1.426620  [192128/344985]
loss: 1.569276  [198528/344985]
loss: 1.486175  [204928/344985]
loss: 1.530923  [211328/344985]
loss: 1.767610  [217728/344985]
loss: 1.387388  [224128/344985]
loss: 1.462468  [230528/344985]
loss: 1.497464  [236928/344985]
loss: 1.509769  [243328/344985]
loss: 1.427026  [249728/344985]
loss: 1.415739  [256128/344985]
loss: 1.711836  [262528/344985]
loss: 1.250232  [268928/344985]
loss: 1.532410  [275328/344985]
loss: 1.537615  [281728/344985]
loss: 1.703848  [288128/344985]
loss: 1.263914  [294528/344985]
loss: 1.476903  [300928/344985]
loss: 1.510682  [307328/344985]
loss: 1.858242  [313728/344985]
loss: 1.566130  [320128/344985]
loss: 1.519708  [326528/344985]
loss: 1.520228  [332928/344985]
loss: 1.424035  [339328/344985]

epoch avg train loss: 1.3435269   epoch avg train accuracy: 0.6367871

-------------------------------

Epoch 38

loss: 0.981931  [  128/344985]
loss: 1.044232  [ 6528/344985]
loss: 0.860017  [12928/344985]
loss: 1.034839  [19328/344985]
loss: 1.118373  [25728/344985]
loss: 1.067108  [32128/344985]
loss: 1.107025  [38528/344985]
loss: 1.050928  [44928/344985]
loss: 1.031072  [51328/344985]
loss: 1.180936  [57728/344985]
loss: 0.932000  [64128/344985]
loss: 1.224749  [70528/344985]
loss: 1.011702  [76928/344985]
loss: 1.218741  [83328/344985]
loss: 1.322839  [89728/344985]
loss: 1.147088  [96128/344985]
loss: 1.423764  [102528/344985]
loss: 1.089394  [108928/344985]
loss: 1.288234  [115328/344985]
loss: 1.469364  [121728/344985]
loss: 1.295151  [128128/344985]
loss: 1.134710  [134528/344985]
loss: 1.478557  [140928/344985]
loss: 1.600893  [147328/344985]
loss: 1.377279  [153728/344985]
loss: 1.466708  [160128/344985]
loss: 1.355256  [166528/344985]
loss: 1.030495  [172928/344985]
loss: 1.486415  [179328/344985]
loss: 1.215842  [185728/344985]
loss: 1.436357  [192128/344985]
loss: 1.436209  [198528/344985]
loss: 1.206386  [204928/344985]
loss: 1.454165  [211328/344985]
loss: 1.459729  [217728/344985]
loss: 1.529543  [224128/344985]
loss: 1.496685  [230528/344985]
loss: 1.387093  [236928/344985]
loss: 1.197880  [243328/344985]
loss: 1.277699  [249728/344985]
loss: 1.533061  [256128/344985]
loss: 1.220451  [262528/344985]
loss: 1.615111  [268928/344985]
loss: 1.740369  [275328/344985]
loss: 1.271180  [281728/344985]
loss: 1.149163  [288128/344985]
loss: 1.830327  [294528/344985]
loss: 1.486039  [300928/344985]
loss: 1.572676  [307328/344985]
loss: 1.647923  [313728/344985]
loss: 1.576438  [320128/344985]
loss: 1.580484  [326528/344985]
loss: 1.414925  [332928/344985]
loss: 1.471252  [339328/344985]

epoch avg train loss: 1.3374061   epoch avg train accuracy: 0.6388742

-------------------------------

Epoch 39

loss: 1.308822  [  128/344985]
loss: 1.194139  [ 6528/344985]
loss: 0.920906  [12928/344985]
loss: 1.032278  [19328/344985]
loss: 1.033524  [25728/344985]
loss: 1.186677  [32128/344985]
loss: 1.074881  [38528/344985]
loss: 1.166444  [44928/344985]
loss: 0.835828  [51328/344985]
loss: 1.210927  [57728/344985]
loss: 1.387345  [64128/344985]
loss: 1.253290  [70528/344985]
loss: 0.903500  [76928/344985]
loss: 1.326403  [83328/344985]
loss: 1.395777  [89728/344985]
loss: 1.474312  [96128/344985]
loss: 0.958435  [102528/344985]
loss: 1.044025  [108928/344985]
loss: 1.413396  [115328/344985]
loss: 1.286073  [121728/344985]
loss: 1.433341  [128128/344985]
loss: 1.406032  [134528/344985]
loss: 1.266342  [140928/344985]
loss: 1.397869  [147328/344985]
loss: 1.174122  [153728/344985]
loss: 1.412428  [160128/344985]
loss: 1.254382  [166528/344985]
loss: 1.040319  [172928/344985]
loss: 1.099270  [179328/344985]
loss: 1.915743  [185728/344985]
loss: 1.421737  [192128/344985]
loss: 0.927092  [198528/344985]
loss: 1.480496  [204928/344985]
loss: 1.500591  [211328/344985]
loss: 1.424771  [217728/344985]
loss: 1.486609  [224128/344985]
loss: 1.697271  [230528/344985]
loss: 1.282981  [236928/344985]
loss: 1.538173  [243328/344985]
loss: 1.032986  [249728/344985]
loss: 1.538817  [256128/344985]
loss: 1.426154  [262528/344985]
loss: 1.191076  [268928/344985]
loss: 1.361776  [275328/344985]
loss: 1.357520  [281728/344985]
loss: 1.392542  [288128/344985]
loss: 1.388797  [294528/344985]
loss: 1.952071  [300928/344985]
loss: 1.323104  [307328/344985]
loss: 1.528574  [313728/344985]
loss: 1.239808  [320128/344985]
loss: 1.643102  [326528/344985]
loss: 1.744231  [332928/344985]
loss: 1.566222  [339328/344985]

epoch avg train loss: 1.3092655   epoch avg train accuracy: 0.6449498

-------------------------------

Epoch 40

loss: 0.950873  [  128/344985]
loss: 0.927735  [ 6528/344985]
loss: 1.071258  [12928/344985]
loss: 1.110798  [19328/344985]
loss: 1.167031  [25728/344985]
loss: 1.225718  [32128/344985]
loss: 1.156874  [38528/344985]
loss: 0.996469  [44928/344985]
loss: 1.138582  [51328/344985]
loss: 0.981129  [57728/344985]
loss: 0.842302  [64128/344985]
loss: 1.222550  [70528/344985]
loss: 1.285403  [76928/344985]
loss: 1.249655  [83328/344985]
loss: 1.138373  [89728/344985]
loss: 1.231777  [96128/344985]
loss: 1.270412  [102528/344985]
loss: 1.471347  [108928/344985]
loss: 1.281501  [115328/344985]
loss: 1.361128  [121728/344985]
loss: 1.245340  [128128/344985]
loss: 0.956750  [134528/344985]
loss: 1.411510  [140928/344985]
loss: 1.453776  [147328/344985]
loss: 1.367551  [153728/344985]
loss: 1.290359  [160128/344985]
loss: 1.373326  [166528/344985]
loss: 1.471736  [172928/344985]
loss: 1.294738  [179328/344985]
loss: 1.571032  [185728/344985]
loss: 1.187818  [192128/344985]
loss: 1.418751  [198528/344985]
loss: 1.295558  [204928/344985]
loss: 1.322843  [211328/344985]
loss: 1.103254  [217728/344985]
loss: 1.577377  [224128/344985]
loss: 1.455488  [230528/344985]
loss: 1.265151  [236928/344985]
loss: 1.157295  [243328/344985]
loss: 1.130860  [249728/344985]
loss: 1.745387  [256128/344985]
loss: 1.545020  [262528/344985]
loss: 1.392888  [268928/344985]
loss: 1.476471  [275328/344985]
loss: 1.522438  [281728/344985]
loss: 1.375693  [288128/344985]
loss: 1.525493  [294528/344985]
loss: 1.417472  [300928/344985]
loss: 1.531696  [307328/344985]
loss: 1.428381  [313728/344985]
loss: 1.416321  [320128/344985]
loss: 1.166980  [326528/344985]
loss: 1.849802  [332928/344985]
loss: 1.641238  [339328/344985]

epoch avg train loss: 1.3042522   epoch avg train accuracy: 0.6467499

-------------------------------

Epoch 41

loss: 1.483781  [  128/344985]
loss: 1.271391  [ 6528/344985]
loss: 1.123068  [12928/344985]
loss: 1.001263  [19328/344985]
loss: 0.890360  [25728/344985]
loss: 1.298363  [32128/344985]
loss: 0.881146  [38528/344985]
loss: 1.023856  [44928/344985]
loss: 1.224420  [51328/344985]
loss: 1.114484  [57728/344985]
loss: 1.087372  [64128/344985]
loss: 0.956910  [70528/344985]
loss: 1.096949  [76928/344985]
loss: 1.125893  [83328/344985]
loss: 0.918519  [89728/344985]
loss: 1.041363  [96128/344985]
loss: 1.478531  [102528/344985]
loss: 1.205812  [108928/344985]
loss: 1.149422  [115328/344985]
loss: 1.300987  [121728/344985]
loss: 1.335728  [128128/344985]
loss: 1.116722  [134528/344985]
loss: 1.294521  [140928/344985]
loss: 1.266652  [147328/344985]
loss: 1.440249  [153728/344985]
loss: 1.376112  [160128/344985]
loss: 1.392378  [166528/344985]
loss: 1.711266  [172928/344985]
loss: 1.324675  [179328/344985]
loss: 1.274763  [185728/344985]
loss: 1.249649  [192128/344985]
loss: 1.334163  [198528/344985]
loss: 1.341894  [204928/344985]
loss: 1.325341  [211328/344985]
loss: 1.574314  [217728/344985]
loss: 1.340868  [224128/344985]
loss: 1.499079  [230528/344985]
loss: 1.376913  [236928/344985]
loss: 1.584789  [243328/344985]
loss: 1.508603  [249728/344985]
loss: 1.284844  [256128/344985]
loss: 1.261932  [262528/344985]
loss: 1.446777  [268928/344985]
loss: 1.552767  [275328/344985]
loss: 1.412231  [281728/344985]
loss: 1.521484  [288128/344985]
loss: 1.547879  [294528/344985]
loss: 1.347489  [300928/344985]
loss: 1.119484  [307328/344985]
loss: 1.210102  [313728/344985]
loss: 1.561090  [320128/344985]
loss: 1.676069  [326528/344985]
loss: 1.170212  [332928/344985]
loss: 1.242483  [339328/344985]

epoch avg train loss: 1.2887412   epoch avg train accuracy: 0.6508950

-------------------------------

Epoch 42

loss: 1.164134  [  128/344985]
loss: 1.197297  [ 6528/344985]
loss: 1.296344  [12928/344985]
loss: 0.968956  [19328/344985]
loss: 1.064626  [25728/344985]
loss: 0.942497  [32128/344985]
loss: 1.045138  [38528/344985]
loss: 1.201760  [44928/344985]
loss: 1.220664  [51328/344985]
loss: 1.211149  [57728/344985]
loss: 1.221172  [64128/344985]
loss: 1.128299  [70528/344985]
loss: 1.178537  [76928/344985]
loss: 1.051868  [83328/344985]
loss: 1.173736  [89728/344985]
loss: 1.489255  [96128/344985]
loss: 1.097743  [102528/344985]
loss: 1.000184  [108928/344985]
loss: 1.097850  [115328/344985]
loss: 0.879698  [121728/344985]
loss: 1.083666  [128128/344985]
loss: 1.037929  [134528/344985]
loss: 1.029507  [140928/344985]
loss: 1.195912  [147328/344985]
loss: 1.517336  [153728/344985]
loss: 0.939017  [160128/344985]
loss: 1.398014  [166528/344985]
loss: 1.419439  [172928/344985]
loss: 1.566630  [179328/344985]
loss: 1.087418  [185728/344985]
loss: 1.313132  [192128/344985]
loss: 0.990963  [198528/344985]
loss: 1.417046  [204928/344985]
loss: 1.271455  [211328/344985]
loss: 1.302803  [217728/344985]
loss: 1.351859  [224128/344985]
loss: 1.608926  [230528/344985]
loss: 1.487769  [236928/344985]
loss: 1.373357  [243328/344985]
loss: 1.081030  [249728/344985]
loss: 1.271625  [256128/344985]
loss: 1.190846  [262528/344985]
loss: 1.832645  [268928/344985]
loss: 1.584072  [275328/344985]
loss: 1.136328  [281728/344985]
loss: 1.696056  [288128/344985]
loss: 1.214408  [294528/344985]
loss: 1.538143  [300928/344985]
loss: 1.538758  [307328/344985]
loss: 1.575939  [313728/344985]
loss: 1.196693  [320128/344985]
loss: 1.333840  [326528/344985]
loss: 1.460446  [332928/344985]
loss: 1.423154  [339328/344985]

epoch avg train loss: 1.2660898   epoch avg train accuracy: 0.6569068

-------------------------------

Epoch 43

loss: 1.057082  [  128/344985]
loss: 1.304189  [ 6528/344985]
loss: 1.202479  [12928/344985]
loss: 0.926074  [19328/344985]
loss: 0.821969  [25728/344985]
loss: 1.226910  [32128/344985]
loss: 1.295101  [38528/344985]
loss: 1.434873  [44928/344985]
loss: 1.193502  [51328/344985]
loss: 1.121243  [57728/344985]
loss: 1.054515  [64128/344985]
loss: 1.348455  [70528/344985]
loss: 1.179801  [76928/344985]
loss: 1.384154  [83328/344985]
loss: 1.201987  [89728/344985]
loss: 1.867890  [96128/344985]
loss: 1.209216  [102528/344985]
loss: 1.226144  [108928/344985]
loss: 1.164666  [115328/344985]
loss: 0.992139  [121728/344985]
loss: 1.373438  [128128/344985]
loss: 1.050933  [134528/344985]
loss: 1.179730  [140928/344985]
loss: 1.128568  [147328/344985]
loss: 1.408599  [153728/344985]
loss: 1.210212  [160128/344985]
loss: 1.341758  [166528/344985]
loss: 1.274611  [172928/344985]
loss: 1.189640  [179328/344985]
loss: 1.176807  [185728/344985]
loss: 1.501028  [192128/344985]
loss: 1.142780  [198528/344985]
loss: 1.611664  [204928/344985]
loss: 1.043153  [211328/344985]
loss: 1.379475  [217728/344985]
loss: 1.153859  [224128/344985]
loss: 1.446341  [230528/344985]
loss: 1.120738  [236928/344985]
loss: 1.667677  [243328/344985]
loss: 1.192948  [249728/344985]
loss: 1.505196  [256128/344985]
loss: 1.351315  [262528/344985]
loss: 1.073624  [268928/344985]
loss: 1.649865  [275328/344985]
loss: 1.298670  [281728/344985]
loss: 1.496091  [288128/344985]
loss: 1.669750  [294528/344985]
loss: 1.453427  [300928/344985]
loss: 1.414947  [307328/344985]
loss: 1.545113  [313728/344985]
loss: 1.312744  [320128/344985]
loss: 1.600911  [326528/344985]
loss: 1.310370  [332928/344985]
loss: 1.449400  [339328/344985]

epoch avg train loss: 1.2629056   epoch avg train accuracy: 0.6570865

-------------------------------

Epoch 44

loss: 1.194706  [  128/344985]
loss: 0.918300  [ 6528/344985]
loss: 0.968779  [12928/344985]
loss: 1.167097  [19328/344985]
loss: 1.117576  [25728/344985]
loss: 0.953358  [32128/344985]
loss: 1.009626  [38528/344985]
loss: 1.136227  [44928/344985]
loss: 1.193462  [51328/344985]
loss: 1.067502  [57728/344985]
loss: 1.185528  [64128/344985]
loss: 1.138597  [70528/344985]
loss: 1.154773  [76928/344985]
loss: 0.994363  [83328/344985]
loss: 1.567363  [89728/344985]
loss: 0.906300  [96128/344985]
loss: 1.350434  [102528/344985]
loss: 1.347200  [108928/344985]
loss: 1.500917  [115328/344985]
loss: 1.420753  [121728/344985]
loss: 1.286195  [128128/344985]
loss: 1.203633  [134528/344985]
loss: 1.254401  [140928/344985]
loss: 1.175067  [147328/344985]
loss: 1.663269  [153728/344985]
loss: 1.115921  [160128/344985]
loss: 1.301222  [166528/344985]
loss: 1.419680  [172928/344985]
loss: 0.998387  [179328/344985]
loss: 1.577173  [185728/344985]
loss: 1.467391  [192128/344985]
loss: 1.332464  [198528/344985]
loss: 1.422210  [204928/344985]
loss: 1.092337  [211328/344985]
loss: 1.002188  [217728/344985]
loss: 1.318095  [224128/344985]
loss: 1.106965  [230528/344985]
loss: 1.387151  [236928/344985]
loss: 1.305106  [243328/344985]
loss: 1.133865  [249728/344985]
loss: 1.191236  [256128/344985]
loss: 1.197121  [262528/344985]
loss: 1.464884  [268928/344985]
loss: 1.563526  [275328/344985]
loss: 1.074802  [281728/344985]
loss: 1.424595  [288128/344985]
loss: 1.327489  [294528/344985]
loss: 1.442631  [300928/344985]
loss: 1.356939  [307328/344985]
loss: 1.253288  [313728/344985]
loss: 1.210346  [320128/344985]
loss: 1.359808  [326528/344985]
loss: 1.493516  [332928/344985]
loss: 1.389043  [339328/344985]

epoch avg train loss: 1.2455187   epoch avg train accuracy: 0.6615186

-------------------------------

Epoch 45

loss: 1.088472  [  128/344985]
loss: 1.052562  [ 6528/344985]
loss: 0.968794  [12928/344985]
loss: 1.162886  [19328/344985]
loss: 1.130606  [25728/344985]
loss: 1.161458  [32128/344985]
loss: 1.172453  [38528/344985]
loss: 1.167241  [44928/344985]
loss: 1.693390  [51328/344985]
loss: 1.040788  [57728/344985]
loss: 1.321828  [64128/344985]
loss: 0.874318  [70528/344985]
loss: 1.418561  [76928/344985]
loss: 0.916830  [83328/344985]
loss: 1.248761  [89728/344985]
loss: 1.107663  [96128/344985]
loss: 0.898784  [102528/344985]
loss: 1.452319  [108928/344985]
loss: 0.995573  [115328/344985]
loss: 1.258157  [121728/344985]
loss: 1.194771  [128128/344985]
loss: 1.205833  [134528/344985]
loss: 1.144857  [140928/344985]
loss: 1.310792  [147328/344985]
loss: 1.110525  [153728/344985]
loss: 1.360860  [160128/344985]
loss: 1.114523  [166528/344985]
loss: 1.327567  [172928/344985]
loss: 1.200802  [179328/344985]
loss: 1.329657  [185728/344985]
loss: 1.192741  [192128/344985]
loss: 1.462424  [198528/344985]
loss: 1.321432  [204928/344985]
loss: 1.327007  [211328/344985]
loss: 1.446744  [217728/344985]
loss: 1.395619  [224128/344985]
loss: 1.440789  [230528/344985]
loss: 1.455164  [236928/344985]
loss: 1.143916  [243328/344985]
loss: 1.511830  [249728/344985]
loss: 1.459643  [256128/344985]
loss: 1.185299  [262528/344985]
loss: 1.184428  [268928/344985]
loss: 1.378862  [275328/344985]
loss: 1.600177  [281728/344985]
loss: 1.537770  [288128/344985]
loss: 1.439323  [294528/344985]
loss: 1.072787  [300928/344985]
loss: 1.312373  [307328/344985]
loss: 1.427905  [313728/344985]
loss: 1.563999  [320128/344985]
loss: 1.449464  [326528/344985]
loss: 1.387080  [332928/344985]
loss: 1.600748  [339328/344985]

epoch avg train loss: 1.2436935   epoch avg train accuracy: 0.6632636

-------------------------------

Epoch 46

loss: 1.023629  [  128/344985]
loss: 0.900012  [ 6528/344985]
loss: 1.193603  [12928/344985]
loss: 1.033291  [19328/344985]
loss: 1.089452  [25728/344985]
loss: 0.963097  [32128/344985]
loss: 0.983324  [38528/344985]
loss: 1.093563  [44928/344985]
loss: 1.283091  [51328/344985]
loss: 1.183767  [57728/344985]
loss: 1.210590  [64128/344985]
loss: 1.132242  [70528/344985]
loss: 0.962243  [76928/344985]
loss: 1.181201  [83328/344985]
loss: 1.149251  [89728/344985]
loss: 0.998210  [96128/344985]
loss: 1.091896  [102528/344985]
loss: 1.417669  [108928/344985]
loss: 1.124193  [115328/344985]
loss: 1.374087  [121728/344985]
loss: 1.192989  [128128/344985]
loss: 0.913483  [134528/344985]
loss: 0.974844  [140928/344985]
loss: 1.013642  [147328/344985]
loss: 1.042147  [153728/344985]
loss: 1.006038  [160128/344985]
loss: 1.102313  [166528/344985]
loss: 1.195317  [172928/344985]
loss: 1.155358  [179328/344985]
loss: 1.393658  [185728/344985]
loss: 1.105527  [192128/344985]
loss: 1.224158  [198528/344985]
loss: 1.209617  [204928/344985]
loss: 1.289587  [211328/344985]
loss: 1.400726  [217728/344985]
loss: 1.343483  [224128/344985]
loss: 1.306374  [230528/344985]
loss: 1.104617  [236928/344985]
loss: 1.214538  [243328/344985]
loss: 1.704696  [249728/344985]
loss: 1.334643  [256128/344985]
loss: 1.291024  [262528/344985]
loss: 1.607894  [268928/344985]
loss: 1.280535  [275328/344985]
loss: 1.386961  [281728/344985]
loss: 1.361385  [288128/344985]
loss: 1.255348  [294528/344985]
loss: 1.380334  [300928/344985]
loss: 1.359970  [307328/344985]
loss: 1.553659  [313728/344985]
loss: 1.465545  [320128/344985]
loss: 1.339994  [326528/344985]
loss: 1.595919  [332928/344985]
loss: 1.077458  [339328/344985]

epoch avg train loss: 1.2331184   epoch avg train accuracy: 0.6666667

-------------------------------

Epoch 47

loss: 1.072103  [  128/344985]
loss: 0.926368  [ 6528/344985]
loss: 1.272003  [12928/344985]
loss: 1.089481  [19328/344985]
loss: 1.068033  [25728/344985]
loss: 1.103715  [32128/344985]
loss: 1.118232  [38528/344985]
loss: 1.261362  [44928/344985]
loss: 1.005172  [51328/344985]
loss: 1.243178  [57728/344985]
loss: 0.871453  [64128/344985]
loss: 1.150590  [70528/344985]
loss: 0.948635  [76928/344985]
loss: 1.035552  [83328/344985]
loss: 0.876121  [89728/344985]
loss: 1.168679  [96128/344985]
loss: 1.260254  [102528/344985]
loss: 1.366176  [108928/344985]
loss: 1.193343  [115328/344985]
loss: 1.288008  [121728/344985]
loss: 1.001369  [128128/344985]
loss: 1.114388  [134528/344985]
loss: 1.066584  [140928/344985]
loss: 1.328152  [147328/344985]
loss: 1.221573  [153728/344985]
loss: 1.216047  [160128/344985]
loss: 0.998277  [166528/344985]
loss: 1.109653  [172928/344985]
loss: 1.578288  [179328/344985]
loss: 1.389567  [185728/344985]
loss: 1.378883  [192128/344985]
loss: 0.852389  [198528/344985]
loss: 1.546612  [204928/344985]
loss: 1.313809  [211328/344985]
loss: 1.038095  [217728/344985]
loss: 1.128373  [224128/344985]
loss: 1.265421  [230528/344985]
loss: 1.151163  [236928/344985]
loss: 1.207817  [243328/344985]
loss: 1.369833  [249728/344985]
loss: 1.130764  [256128/344985]
loss: 1.577848  [262528/344985]
loss: 1.511952  [268928/344985]
loss: 1.334996  [275328/344985]
loss: 1.529831  [281728/344985]
loss: 1.338138  [288128/344985]
loss: 1.270765  [294528/344985]
loss: 1.057555  [300928/344985]
loss: 1.449725  [307328/344985]
loss: 1.282681  [313728/344985]
loss: 1.286827  [320128/344985]
loss: 1.103237  [326528/344985]
loss: 1.096504  [332928/344985]
loss: 1.357126  [339328/344985]

epoch avg train loss: 1.2185644   epoch avg train accuracy: 0.6697740

-------------------------------

Epoch 48

loss: 1.189337  [  128/344985]
loss: 0.955372  [ 6528/344985]
loss: 1.323367  [12928/344985]
loss: 1.117193  [19328/344985]
loss: 1.311505  [25728/344985]
loss: 1.261608  [32128/344985]
loss: 1.023273  [38528/344985]
loss: 1.147738  [44928/344985]
loss: 1.105649  [51328/344985]
loss: 1.284787  [57728/344985]
loss: 0.906648  [64128/344985]
loss: 1.322098  [70528/344985]
loss: 1.026856  [76928/344985]
loss: 1.050484  [83328/344985]
loss: 0.888466  [89728/344985]
loss: 0.907803  [96128/344985]
loss: 1.199115  [102528/344985]
loss: 0.965510  [108928/344985]
loss: 0.910365  [115328/344985]
loss: 1.048343  [121728/344985]
loss: 1.049044  [128128/344985]
loss: 1.199013  [134528/344985]
loss: 1.340777  [140928/344985]
loss: 1.469269  [147328/344985]
loss: 1.147063  [153728/344985]
loss: 0.850123  [160128/344985]
loss: 1.307385  [166528/344985]
loss: 1.336125  [172928/344985]
loss: 1.163462  [179328/344985]
loss: 1.224277  [185728/344985]
loss: 1.352157  [192128/344985]
loss: 1.145026  [198528/344985]
loss: 0.883723  [204928/344985]
loss: 1.199172  [211328/344985]
loss: 1.368448  [217728/344985]
loss: 1.226726  [224128/344985]
loss: 1.145888  [230528/344985]
loss: 1.334235  [236928/344985]
loss: 1.126544  [243328/344985]
loss: 1.473042  [249728/344985]
loss: 1.160419  [256128/344985]
loss: 1.482633  [262528/344985]
loss: 1.404664  [268928/344985]
loss: 1.363012  [275328/344985]
loss: 1.392540  [281728/344985]
loss: 1.504489  [288128/344985]
loss: 1.540653  [294528/344985]
loss: 1.498352  [300928/344985]
loss: 1.254174  [307328/344985]
loss: 1.268646  [313728/344985]
loss: 1.590672  [320128/344985]
loss: 1.603165  [326528/344985]
loss: 1.357745  [332928/344985]
loss: 1.116688  [339328/344985]

epoch avg train loss: 1.2167006   epoch avg train accuracy: 0.6706263

-------------------------------

Epoch 49

loss: 0.966793  [  128/344985]
loss: 1.000475  [ 6528/344985]
loss: 0.914726  [12928/344985]
loss: 0.919594  [19328/344985]
loss: 0.983874  [25728/344985]
loss: 0.916312  [32128/344985]
loss: 1.139457  [38528/344985]
loss: 1.033709  [44928/344985]
loss: 0.972751  [51328/344985]
loss: 0.886916  [57728/344985]
loss: 1.133438  [64128/344985]
loss: 1.149759  [70528/344985]
loss: 1.350007  [76928/344985]
loss: 1.182325  [83328/344985]
loss: 1.276629  [89728/344985]
loss: 0.909156  [96128/344985]
loss: 1.271644  [102528/344985]
loss: 1.195430  [108928/344985]
loss: 1.237079  [115328/344985]
loss: 1.217490  [121728/344985]
loss: 1.196043  [128128/344985]
loss: 1.166667  [134528/344985]
loss: 1.184231  [140928/344985]
loss: 0.975501  [147328/344985]
loss: 0.907922  [153728/344985]
loss: 1.206716  [160128/344985]
loss: 1.103828  [166528/344985]
loss: 1.147347  [172928/344985]
loss: 1.246670  [179328/344985]
loss: 0.905832  [185728/344985]
loss: 1.340876  [192128/344985]
loss: 1.158637  [198528/344985]
loss: 1.245959  [204928/344985]
loss: 1.593421  [211328/344985]
loss: 1.208732  [217728/344985]
loss: 1.173463  [224128/344985]
loss: 1.336283  [230528/344985]
loss: 1.074100  [236928/344985]
loss: 1.275591  [243328/344985]
loss: 1.186213  [249728/344985]
loss: 1.180968  [256128/344985]
loss: 1.382591  [262528/344985]
loss: 1.371092  [268928/344985]
loss: 1.225654  [275328/344985]
loss: 1.795850  [281728/344985]
loss: 1.282203  [288128/344985]
loss: 1.432425  [294528/344985]
loss: 0.923471  [300928/344985]
loss: 1.600209  [307328/344985]
loss: 1.342294  [313728/344985]
loss: 1.374356  [320128/344985]
loss: 1.017271  [326528/344985]
loss: 1.208974  [332928/344985]
loss: 1.652156  [339328/344985]

epoch avg train loss: 1.1983298   epoch avg train accuracy: 0.6747105

-------------------------------

Epoch 50

loss: 1.076134  [  128/344985]
loss: 1.054913  [ 6528/344985]
loss: 1.191432  [12928/344985]
loss: 0.856194  [19328/344985]
loss: 0.920691  [25728/344985]
loss: 0.828114  [32128/344985]
loss: 0.901795  [38528/344985]
loss: 1.069631  [44928/344985]
loss: 1.094218  [51328/344985]
loss: 1.011515  [57728/344985]
loss: 0.971100  [64128/344985]
loss: 1.053267  [70528/344985]
loss: 1.026688  [76928/344985]
loss: 1.288839  [83328/344985]
loss: 1.201551  [89728/344985]
loss: 1.187765  [96128/344985]
loss: 1.085370  [102528/344985]
loss: 1.210459  [108928/344985]
loss: 1.466993  [115328/344985]
loss: 1.014348  [121728/344985]
loss: 1.161385  [128128/344985]
loss: 1.180536  [134528/344985]
loss: 1.229698  [140928/344985]
loss: 1.072393  [147328/344985]
loss: 1.038578  [153728/344985]
loss: 1.653492  [160128/344985]
loss: 0.864669  [166528/344985]
loss: 1.042725  [172928/344985]
loss: 1.086074  [179328/344985]
loss: 1.405613  [185728/344985]
loss: 1.059009  [192128/344985]
loss: 1.415253  [198528/344985]
loss: 1.180743  [204928/344985]
loss: 1.196527  [211328/344985]
loss: 1.510192  [217728/344985]
loss: 1.715694  [224128/344985]
loss: 1.077538  [230528/344985]
loss: 1.319194  [236928/344985]
loss: 1.425087  [243328/344985]
loss: 1.374198  [249728/344985]
loss: 0.865864  [256128/344985]
loss: 1.219536  [262528/344985]
loss: 1.485792  [268928/344985]
loss: 1.417741  [275328/344985]
loss: 1.460111  [281728/344985]
loss: 1.465888  [288128/344985]
loss: 1.396800  [294528/344985]
loss: 1.322171  [300928/344985]
loss: 1.157763  [307328/344985]
loss: 1.329282  [313728/344985]
loss: 1.294063  [320128/344985]
loss: 1.250707  [326528/344985]
loss: 1.189756  [332928/344985]
loss: 1.276170  [339328/344985]

epoch avg train loss: 1.1908145   epoch avg train accuracy: 0.6770004

-------------------------------

Epoch 51

loss: 1.036693  [  128/344985]
loss: 0.878523  [ 6528/344985]
loss: 1.404186  [12928/344985]
loss: 1.016582  [19328/344985]
loss: 1.010767  [25728/344985]
loss: 0.901268  [32128/344985]
loss: 0.920292  [38528/344985]
loss: 0.972105  [44928/344985]
loss: 1.058088  [51328/344985]
loss: 0.984618  [57728/344985]
loss: 0.909885  [64128/344985]
loss: 0.950136  [70528/344985]
loss: 1.018663  [76928/344985]
loss: 1.080147  [83328/344985]
loss: 1.172315  [89728/344985]
loss: 1.380442  [96128/344985]
loss: 0.964887  [102528/344985]
loss: 1.003967  [108928/344985]
loss: 1.312639  [115328/344985]
loss: 1.264914  [121728/344985]
loss: 0.959830  [128128/344985]
loss: 1.297237  [134528/344985]
loss: 1.355979  [140928/344985]
loss: 1.219262  [147328/344985]
loss: 1.071501  [153728/344985]
loss: 1.192233  [160128/344985]
loss: 1.366634  [166528/344985]
loss: 1.027391  [172928/344985]
loss: 1.528697  [179328/344985]
loss: 1.137666  [185728/344985]
loss: 1.475665  [192128/344985]
loss: 1.678178  [198528/344985]
loss: 1.227853  [204928/344985]
loss: 1.109680  [211328/344985]
loss: 1.368334  [217728/344985]
loss: 1.392834  [224128/344985]
loss: 1.153624  [230528/344985]
loss: 1.111318  [236928/344985]
loss: 1.080052  [243328/344985]
loss: 0.936266  [249728/344985]
loss: 1.009032  [256128/344985]
loss: 1.298944  [262528/344985]
loss: 1.443068  [268928/344985]
loss: 1.526754  [275328/344985]
loss: 1.268767  [281728/344985]
loss: 1.360669  [288128/344985]
loss: 1.401086  [294528/344985]
loss: 1.224874  [300928/344985]
loss: 1.310523  [307328/344985]
loss: 1.480046  [313728/344985]
loss: 1.267284  [320128/344985]
loss: 1.046940  [326528/344985]
loss: 1.636328  [332928/344985]
loss: 1.249118  [339328/344985]

epoch avg train loss: 1.1807682   epoch avg train accuracy: 0.6796382

-------------------------------

Epoch 52

loss: 0.928264  [  128/344985]
loss: 0.914058  [ 6528/344985]
loss: 0.822250  [12928/344985]
loss: 0.993682  [19328/344985]
loss: 0.686924  [25728/344985]
loss: 0.888236  [32128/344985]
loss: 0.913442  [38528/344985]
loss: 0.892843  [44928/344985]
loss: 1.162833  [51328/344985]
loss: 1.171874  [57728/344985]
loss: 1.212403  [64128/344985]
loss: 0.935490  [70528/344985]
loss: 1.183674  [76928/344985]
loss: 1.000543  [83328/344985]
loss: 0.896640  [89728/344985]
loss: 1.098940  [96128/344985]
loss: 1.390328  [102528/344985]
loss: 1.252590  [108928/344985]
loss: 1.314415  [115328/344985]
loss: 1.115472  [121728/344985]
loss: 1.426960  [128128/344985]
loss: 1.028691  [134528/344985]
loss: 1.124377  [140928/344985]
loss: 1.280937  [147328/344985]
loss: 1.448587  [153728/344985]
loss: 1.487405  [160128/344985]
loss: 1.264057  [166528/344985]
loss: 1.324711  [172928/344985]
loss: 1.380709  [179328/344985]
loss: 1.132531  [185728/344985]
loss: 1.214455  [192128/344985]
loss: 1.225787  [198528/344985]
loss: 1.266011  [204928/344985]
loss: 1.378843  [211328/344985]
loss: 1.597896  [217728/344985]
loss: 0.964040  [224128/344985]
loss: 1.223384  [230528/344985]
loss: 1.322479  [236928/344985]
loss: 1.599952  [243328/344985]
loss: 1.090347  [249728/344985]
loss: 1.336965  [256128/344985]
loss: 1.436220  [262528/344985]
loss: 1.309214  [268928/344985]
loss: 1.342897  [275328/344985]
loss: 1.126053  [281728/344985]
loss: 1.271757  [288128/344985]
loss: 1.584330  [294528/344985]
loss: 1.130345  [300928/344985]
loss: 1.179364  [307328/344985]
loss: 1.132210  [313728/344985]
loss: 1.376313  [320128/344985]
loss: 1.456455  [326528/344985]
loss: 1.451567  [332928/344985]
loss: 1.257860  [339328/344985]

epoch avg train loss: 1.1668211   epoch avg train accuracy: 0.6835109

-------------------------------

Epoch 53

loss: 0.709481  [  128/344985]
loss: 0.828783  [ 6528/344985]
loss: 1.038159  [12928/344985]
loss: 0.692563  [19328/344985]
loss: 0.957661  [25728/344985]
loss: 0.916638  [32128/344985]
loss: 1.122153  [38528/344985]
loss: 1.092661  [44928/344985]
loss: 0.843201  [51328/344985]
loss: 1.181901  [57728/344985]
loss: 0.969831  [64128/344985]
loss: 0.883494  [70528/344985]
loss: 1.188255  [76928/344985]
loss: 0.841866  [83328/344985]
loss: 1.119155  [89728/344985]
loss: 0.982706  [96128/344985]
loss: 1.055648  [102528/344985]
loss: 0.887742  [108928/344985]
loss: 1.043048  [115328/344985]
loss: 1.060023  [121728/344985]
loss: 0.930148  [128128/344985]
loss: 1.045259  [134528/344985]
loss: 0.703884  [140928/344985]
loss: 1.254527  [147328/344985]
loss: 1.206416  [153728/344985]
loss: 1.414503  [160128/344985]
loss: 1.069665  [166528/344985]
loss: 1.184926  [172928/344985]
loss: 1.284122  [179328/344985]
loss: 1.407024  [185728/344985]
loss: 1.082525  [192128/344985]
loss: 1.084132  [198528/344985]
loss: 1.600706  [204928/344985]
loss: 1.116882  [211328/344985]
loss: 1.219461  [217728/344985]
loss: 1.177172  [224128/344985]
loss: 0.959922  [230528/344985]
loss: 1.460585  [236928/344985]
loss: 1.258505  [243328/344985]
loss: 1.393093  [249728/344985]
loss: 1.194757  [256128/344985]
loss: 1.702689  [262528/344985]
loss: 1.281483  [268928/344985]
loss: 1.313292  [275328/344985]
loss: 1.194393  [281728/344985]
loss: 0.996066  [288128/344985]
loss: 1.519320  [294528/344985]
loss: 1.300163  [300928/344985]
loss: 1.165380  [307328/344985]
loss: 1.077725  [313728/344985]
loss: 1.130592  [320128/344985]
loss: 1.416103  [326528/344985]
loss: 1.336386  [332928/344985]
loss: 1.350811  [339328/344985]

epoch avg train loss: 1.1810519   epoch avg train accuracy: 0.6814702

-------------------------------

Epoch 54

loss: 0.776431  [  128/344985]
loss: 0.789335  [ 6528/344985]
loss: 1.023256  [12928/344985]
loss: 1.211960  [19328/344985]
loss: 0.797152  [25728/344985]
loss: 0.822360  [32128/344985]
loss: 0.994577  [38528/344985]
loss: 0.823696  [44928/344985]
loss: 0.972241  [51328/344985]
loss: 0.956666  [57728/344985]
loss: 1.105847  [64128/344985]
loss: 0.865881  [70528/344985]
loss: 0.820446  [76928/344985]
loss: 0.877495  [83328/344985]
loss: 1.117590  [89728/344985]
loss: 1.072026  [96128/344985]
loss: 1.157476  [102528/344985]
loss: 0.908379  [108928/344985]
loss: 1.005030  [115328/344985]
loss: 1.062636  [121728/344985]
loss: 1.038289  [128128/344985]
loss: 1.120591  [134528/344985]
loss: 0.792729  [140928/344985]
loss: 1.039463  [147328/344985]
loss: 1.212978  [153728/344985]
loss: 1.048305  [160128/344985]
loss: 0.903920  [166528/344985]
loss: 1.231105  [172928/344985]
loss: 1.701677  [179328/344985]
loss: 1.224593  [185728/344985]
loss: 1.094771  [192128/344985]
loss: 1.581195  [198528/344985]
loss: 1.271855  [204928/344985]
loss: 1.110538  [211328/344985]
loss: 1.182636  [217728/344985]
loss: 1.047994  [224128/344985]
loss: 1.261375  [230528/344985]
loss: 1.834586  [236928/344985]
loss: 1.343307  [243328/344985]
loss: 0.977191  [249728/344985]
loss: 1.006060  [256128/344985]
loss: 1.278215  [262528/344985]
loss: 1.260071  [268928/344985]
loss: 1.218683  [275328/344985]
loss: 1.280940  [281728/344985]
loss: 1.309614  [288128/344985]
loss: 1.347809  [294528/344985]
loss: 1.407415  [300928/344985]
loss: 1.139843  [307328/344985]
loss: 1.367095  [313728/344985]
loss: 0.945497  [320128/344985]
loss: 1.661124  [326528/344985]
loss: 1.360348  [332928/344985]
loss: 1.389771  [339328/344985]

epoch avg train loss: 1.1554514   epoch avg train accuracy: 0.6878212

-------------------------------

Epoch 55

loss: 1.168705  [  128/344985]
loss: 1.176020  [ 6528/344985]
loss: 1.036278  [12928/344985]
loss: 1.074281  [19328/344985]
loss: 0.977696  [25728/344985]
loss: 0.822997  [32128/344985]
loss: 1.084714  [38528/344985]
loss: 1.052557  [44928/344985]
loss: 1.192958  [51328/344985]
loss: 1.101194  [57728/344985]
loss: 0.867897  [64128/344985]
loss: 0.887291  [70528/344985]
loss: 1.355487  [76928/344985]
loss: 0.883191  [83328/344985]
loss: 1.256127  [89728/344985]
loss: 1.289314  [96128/344985]
loss: 1.125164  [102528/344985]
loss: 0.973434  [108928/344985]
loss: 0.969044  [115328/344985]
loss: 1.135578  [121728/344985]
loss: 1.274683  [128128/344985]
loss: 1.240935  [134528/344985]
loss: 1.108481  [140928/344985]
loss: 1.043047  [147328/344985]
loss: 1.237468  [153728/344985]
loss: 1.020961  [160128/344985]
loss: 1.079771  [166528/344985]
loss: 1.268155  [172928/344985]
loss: 1.063329  [179328/344985]
loss: 1.285280  [185728/344985]
loss: 1.206516  [192128/344985]
loss: 1.339168  [198528/344985]
loss: 1.144727  [204928/344985]
loss: 1.089881  [211328/344985]
loss: 1.084370  [217728/344985]
loss: 1.074620  [224128/344985]
loss: 1.105210  [230528/344985]
loss: 0.670391  [236928/344985]
loss: 1.348588  [243328/344985]
loss: 1.577544  [249728/344985]
loss: 1.166141  [256128/344985]
loss: 1.153885  [262528/344985]
loss: 1.160829  [268928/344985]
loss: 1.139437  [275328/344985]
loss: 1.504076  [281728/344985]
loss: 1.571370  [288128/344985]
loss: 1.613573  [294528/344985]
loss: 1.404358  [300928/344985]
loss: 1.535476  [307328/344985]
loss: 1.315132  [313728/344985]
loss: 1.300754  [320128/344985]
loss: 1.510563  [326528/344985]
loss: 1.093560  [332928/344985]
loss: 1.021273  [339328/344985]

epoch avg train loss: 1.1641766   epoch avg train accuracy: 0.6852559

-------------------------------

Epoch 56

loss: 0.878598  [  128/344985]
loss: 1.002733  [ 6528/344985]
loss: 1.299219  [12928/344985]
loss: 0.954149  [19328/344985]
loss: 1.025463  [25728/344985]
loss: 1.165123  [32128/344985]
loss: 0.982027  [38528/344985]
loss: 0.946206  [44928/344985]
loss: 1.042835  [51328/344985]
loss: 0.963730  [57728/344985]
loss: 1.022581  [64128/344985]
loss: 1.066883  [70528/344985]
loss: 0.949591  [76928/344985]
loss: 1.364294  [83328/344985]
loss: 0.905025  [89728/344985]
loss: 0.871434  [96128/344985]
loss: 1.059397  [102528/344985]
loss: 0.959038  [108928/344985]
loss: 1.003413  [115328/344985]
loss: 0.854554  [121728/344985]
loss: 0.950657  [128128/344985]
loss: 1.019753  [134528/344985]
loss: 1.175816  [140928/344985]
loss: 1.076892  [147328/344985]
loss: 1.032590  [153728/344985]
loss: 1.435175  [160128/344985]
loss: 1.474751  [166528/344985]
loss: 1.086705  [172928/344985]
loss: 0.953722  [179328/344985]
loss: 0.894159  [185728/344985]
loss: 1.290112  [192128/344985]
loss: 1.103232  [198528/344985]
loss: 1.233668  [204928/344985]
loss: 1.693658  [211328/344985]
loss: 1.604342  [217728/344985]
loss: 1.314925  [224128/344985]
loss: 1.253669  [230528/344985]
loss: 1.071365  [236928/344985]
loss: 1.289932  [243328/344985]
loss: 1.115035  [249728/344985]
loss: 1.245455  [256128/344985]
loss: 1.415143  [262528/344985]
loss: 1.235791  [268928/344985]
loss: 1.023773  [275328/344985]
loss: 1.308690  [281728/344985]
loss: 1.475489  [288128/344985]
loss: 0.971422  [294528/344985]
loss: 1.308607  [300928/344985]
loss: 1.232316  [307328/344985]
loss: 1.126142  [313728/344985]
loss: 1.143427  [320128/344985]
loss: 1.150817  [326528/344985]
loss: 1.342752  [332928/344985]
loss: 1.628317  [339328/344985]

epoch avg train loss: 1.1605435   epoch avg train accuracy: 0.6880850

-------------------------------

Epoch 57

loss: 0.924164  [  128/344985]
loss: 0.989358  [ 6528/344985]
loss: 0.898853  [12928/344985]
loss: 1.104782  [19328/344985]
loss: 0.948381  [25728/344985]
loss: 0.953459  [32128/344985]
loss: 0.826157  [38528/344985]
loss: 0.802535  [44928/344985]
loss: 1.089044  [51328/344985]
loss: 0.910624  [57728/344985]
loss: 1.078846  [64128/344985]
loss: 1.060667  [70528/344985]
loss: 0.904861  [76928/344985]
loss: 1.268940  [83328/344985]
loss: 1.439929  [89728/344985]
loss: 1.081635  [96128/344985]
loss: 0.984633  [102528/344985]
loss: 1.000704  [108928/344985]
loss: 1.210258  [115328/344985]
loss: 0.876150  [121728/344985]
loss: 0.796305  [128128/344985]
loss: 0.897942  [134528/344985]
loss: 0.960404  [140928/344985]
loss: 0.932895  [147328/344985]
loss: 0.856653  [153728/344985]
loss: 1.025590  [160128/344985]
loss: 1.342257  [166528/344985]
loss: 1.549840  [172928/344985]
loss: 0.852686  [179328/344985]
loss: 1.126446  [185728/344985]
loss: 1.242781  [192128/344985]
loss: 0.958465  [198528/344985]
loss: 1.342791  [204928/344985]
loss: 1.241119  [211328/344985]
loss: 1.101998  [217728/344985]
loss: 1.557629  [224128/344985]
loss: 1.385011  [230528/344985]
loss: 1.158677  [236928/344985]
loss: 1.185119  [243328/344985]
loss: 0.966607  [249728/344985]
loss: 1.171245  [256128/344985]
loss: 1.323937  [262528/344985]
loss: 1.030540  [268928/344985]
loss: 1.288671  [275328/344985]
loss: 1.262900  [281728/344985]
loss: 1.133311  [288128/344985]
loss: 1.157369  [294528/344985]
loss: 1.284182  [300928/344985]
loss: 1.345891  [307328/344985]
loss: 1.212774  [313728/344985]
loss: 1.368132  [320128/344985]
loss: 1.545869  [326528/344985]
loss: 1.571359  [332928/344985]
loss: 1.497829  [339328/344985]

epoch avg train loss: 1.1487679   epoch avg train accuracy: 0.6912330

-------------------------------

Epoch 58

loss: 1.028163  [  128/344985]
loss: 0.888057  [ 6528/344985]
loss: 1.168960  [12928/344985]
loss: 1.090238  [19328/344985]
loss: 0.986931  [25728/344985]
loss: 1.248006  [32128/344985]
loss: 1.160561  [38528/344985]
loss: 1.232604  [44928/344985]
loss: 0.654045  [51328/344985]
loss: 1.207606  [57728/344985]
loss: 1.193468  [64128/344985]
loss: 0.935699  [70528/344985]
loss: 0.994312  [76928/344985]
loss: 0.801671  [83328/344985]
loss: 1.107834  [89728/344985]
loss: 0.972425  [96128/344985]
loss: 0.812618  [102528/344985]
loss: 0.934331  [108928/344985]
loss: 0.948586  [115328/344985]
loss: 1.275321  [121728/344985]
loss: 1.361527  [128128/344985]
loss: 1.078373  [134528/344985]
loss: 1.046198  [140928/344985]
loss: 1.294890  [147328/344985]
loss: 1.325870  [153728/344985]
loss: 0.993529  [160128/344985]
loss: 1.055513  [166528/344985]
loss: 1.252067  [172928/344985]
loss: 1.081746  [179328/344985]
loss: 1.526435  [185728/344985]
loss: 1.147341  [192128/344985]
loss: 1.204132  [198528/344985]
loss: 1.338057  [204928/344985]
loss: 1.338194  [211328/344985]
loss: 1.080975  [217728/344985]
loss: 1.229323  [224128/344985]
loss: 1.405295  [230528/344985]
loss: 0.894184  [236928/344985]
loss: 1.134827  [243328/344985]
loss: 1.120552  [249728/344985]
loss: 1.380893  [256128/344985]
loss: 1.195728  [262528/344985]
loss: 1.425978  [268928/344985]
loss: 1.101580  [275328/344985]
loss: 1.295831  [281728/344985]
loss: 1.339467  [288128/344985]
loss: 1.035709  [294528/344985]
loss: 1.275632  [300928/344985]
loss: 1.280350  [307328/344985]
loss: 1.035873  [313728/344985]
loss: 1.538706  [320128/344985]
loss: 1.544742  [326528/344985]
loss: 1.243884  [332928/344985]
loss: 1.326880  [339328/344985]

epoch avg train loss: 1.1328426   epoch avg train accuracy: 0.6939664

-------------------------------

Epoch 59

loss: 0.984079  [  128/344985]
loss: 0.920640  [ 6528/344985]
loss: 1.387708  [12928/344985]
loss: 1.094812  [19328/344985]
loss: 0.658287  [25728/344985]
loss: 0.885305  [32128/344985]
loss: 0.994719  [38528/344985]
loss: 1.073751  [44928/344985]
loss: 0.999561  [51328/344985]
loss: 0.785576  [57728/344985]
loss: 0.971713  [64128/344985]
loss: 1.118651  [70528/344985]
loss: 1.194953  [76928/344985]
loss: 0.959438  [83328/344985]
loss: 0.766256  [89728/344985]
loss: 0.927386  [96128/344985]
loss: 1.003829  [102528/344985]
loss: 0.944971  [108928/344985]
loss: 0.826691  [115328/344985]
loss: 1.112921  [121728/344985]
loss: 1.103119  [128128/344985]
loss: 1.001059  [134528/344985]
loss: 1.204708  [140928/344985]
loss: 0.998212  [147328/344985]
loss: 1.015884  [153728/344985]
loss: 1.366014  [160128/344985]
loss: 1.078554  [166528/344985]
loss: 1.231462  [172928/344985]
loss: 1.498250  [179328/344985]
loss: 1.447677  [185728/344985]
loss: 1.265830  [192128/344985]
loss: 1.559314  [198528/344985]
loss: 1.263292  [204928/344985]
loss: 0.840839  [211328/344985]
loss: 1.036236  [217728/344985]
loss: 1.416301  [224128/344985]
loss: 1.213991  [230528/344985]
loss: 1.481747  [236928/344985]
loss: 1.195077  [243328/344985]
loss: 1.450499  [249728/344985]
loss: 1.370268  [256128/344985]
loss: 1.149019  [262528/344985]
loss: 1.060122  [268928/344985]
loss: 1.096307  [275328/344985]
loss: 1.176313  [281728/344985]
loss: 1.372333  [288128/344985]
loss: 1.192343  [294528/344985]
loss: 1.592797  [300928/344985]
loss: 1.303607  [307328/344985]
loss: 1.505164  [313728/344985]
loss: 1.222870  [320128/344985]
loss: 1.468784  [326528/344985]
loss: 1.291567  [332928/344985]
loss: 1.301818  [339328/344985]

epoch avg train loss: 1.1362630   epoch avg train accuracy: 0.6939693

-------------------------------

Epoch 60

loss: 1.178246  [  128/344985]
loss: 1.000655  [ 6528/344985]
loss: 0.967406  [12928/344985]
loss: 0.689469  [19328/344985]
loss: 0.951859  [25728/344985]
loss: 0.735050  [32128/344985]
loss: 1.019242  [38528/344985]
loss: 0.964543  [44928/344985]
loss: 1.211445  [51328/344985]
loss: 0.984445  [57728/344985]
loss: 0.904445  [64128/344985]
loss: 1.041838  [70528/344985]
loss: 0.950493  [76928/344985]
loss: 1.030158  [83328/344985]
loss: 1.040650  [89728/344985]
loss: 1.269235  [96128/344985]
loss: 1.108238  [102528/344985]
loss: 1.122056  [108928/344985]
loss: 1.098888  [115328/344985]
loss: 1.064863  [121728/344985]
loss: 1.434898  [128128/344985]
loss: 1.419875  [134528/344985]
loss: 1.091779  [140928/344985]
loss: 1.056910  [147328/344985]
loss: 1.186102  [153728/344985]
loss: 1.053798  [160128/344985]
loss: 1.320273  [166528/344985]
loss: 0.951477  [172928/344985]
loss: 1.038343  [179328/344985]
loss: 1.137954  [185728/344985]
loss: 1.304467  [192128/344985]
loss: 1.370578  [198528/344985]
loss: 1.099481  [204928/344985]
loss: 0.980559  [211328/344985]
loss: 1.658015  [217728/344985]
loss: 1.018461  [224128/344985]
loss: 1.115967  [230528/344985]
loss: 1.079965  [236928/344985]
loss: 0.954632  [243328/344985]
loss: 1.475163  [249728/344985]
loss: 1.664453  [256128/344985]
loss: 1.582487  [262528/344985]
loss: 1.310257  [268928/344985]
loss: 1.162004  [275328/344985]
loss: 1.102793  [281728/344985]
loss: 1.148632  [288128/344985]
loss: 1.418618  [294528/344985]
loss: 0.977460  [300928/344985]
loss: 1.596018  [307328/344985]
loss: 1.067755  [313728/344985]
loss: 1.179287  [320128/344985]
loss: 1.037505  [326528/344985]
loss: 1.323172  [332928/344985]
loss: 1.275982  [339328/344985]

epoch avg train loss: 1.1349522   epoch avg train accuracy: 0.6958042

-------------------------------

Evaluating against random transformations...
Mean acc: 0.2897
Acc std: 0.0014882
