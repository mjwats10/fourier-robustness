Epoch 1

loss: 5.845234  [  128/344985]
loss: 5.771500  [ 6528/344985]
loss: 5.689723  [12928/344985]
loss: 5.547463  [19328/344985]
loss: 5.499391  [25728/344985]
loss: 5.412924  [32128/344985]
loss: 5.359265  [38528/344985]
loss: 5.142374  [44928/344985]
loss: 5.277162  [51328/344985]
loss: 5.299030  [57728/344985]
loss: 5.054760  [64128/344985]
loss: 5.066494  [70528/344985]
loss: 5.038212  [76928/344985]
loss: 5.124928  [83328/344985]
loss: 4.832463  [89728/344985]
loss: 4.863105  [96128/344985]
loss: 4.777815  [102528/344985]
loss: 4.694314  [108928/344985]
loss: 4.708049  [115328/344985]
loss: 4.750793  [121728/344985]
loss: 4.582276  [128128/344985]
loss: 4.704939  [134528/344985]
loss: 4.548842  [140928/344985]
loss: 4.472465  [147328/344985]
loss: 4.417769  [153728/344985]
loss: 4.425349  [160128/344985]
loss: 4.321772  [166528/344985]
loss: 4.263998  [172928/344985]
loss: 4.650966  [179328/344985]
loss: 4.099284  [185728/344985]
loss: 4.348360  [192128/344985]
loss: 4.232025  [198528/344985]
loss: 4.313883  [204928/344985]
loss: 4.324908  [211328/344985]
loss: 4.180928  [217728/344985]
loss: 4.123146  [224128/344985]
loss: 4.227376  [230528/344985]
loss: 3.918835  [236928/344985]
loss: 4.475306  [243328/344985]
loss: 3.890294  [249728/344985]
loss: 3.803164  [256128/344985]
loss: 3.975276  [262528/344985]
loss: 4.153341  [268928/344985]
loss: 4.405155  [275328/344985]
loss: 4.050222  [281728/344985]
loss: 3.744496  [288128/344985]
loss: 4.018402  [294528/344985]
loss: 3.826824  [300928/344985]
loss: 4.194319  [307328/344985]
loss: 3.856554  [313728/344985]
loss: 3.895220  [320128/344985]
loss: 3.917471  [326528/344985]
loss: 3.718368  [332928/344985]
loss: 3.622067  [339328/344985]

epoch avg train loss: 4.4967189   epoch avg train accuracy: 0.1057785

-------------------------------

Epoch 2

loss: 3.599618  [  128/344985]
loss: 4.083501  [ 6528/344985]
loss: 3.569329  [12928/344985]
loss: 3.699805  [19328/344985]
loss: 3.456611  [25728/344985]
loss: 3.522473  [32128/344985]
loss: 3.977529  [38528/344985]
loss: 3.989996  [44928/344985]
loss: 3.548566  [51328/344985]
loss: 3.767040  [57728/344985]
loss: 3.837330  [64128/344985]
loss: 3.799964  [70528/344985]
loss: 3.688728  [76928/344985]
loss: 3.376025  [83328/344985]
loss: 3.962144  [89728/344985]
loss: 3.328024  [96128/344985]
loss: 3.887898  [102528/344985]
loss: 3.786134  [108928/344985]
loss: 3.668099  [115328/344985]
loss: 3.873537  [121728/344985]
loss: 3.583287  [128128/344985]
loss: 3.829206  [134528/344985]
loss: 3.833782  [140928/344985]
loss: 3.642576  [147328/344985]
loss: 3.590000  [153728/344985]
loss: 3.831048  [160128/344985]
loss: 3.584888  [166528/344985]
loss: 3.550499  [172928/344985]
loss: 3.590688  [179328/344985]
loss: 3.412072  [185728/344985]
loss: 3.950370  [192128/344985]
loss: 3.400452  [198528/344985]
loss: 3.194055  [204928/344985]
loss: 3.298333  [211328/344985]
loss: 3.237993  [217728/344985]
loss: 3.635611  [224128/344985]
loss: 3.843413  [230528/344985]
loss: 3.378774  [236928/344985]
loss: 3.807238  [243328/344985]
loss: 3.643000  [249728/344985]
loss: 2.851702  [256128/344985]
loss: 3.526138  [262528/344985]
loss: 3.593310  [268928/344985]
loss: 3.588149  [275328/344985]
loss: 3.493237  [281728/344985]
loss: 3.563168  [288128/344985]
loss: 3.639930  [294528/344985]
loss: 3.640267  [300928/344985]
loss: 3.287738  [307328/344985]
loss: 3.474756  [313728/344985]
loss: 3.505507  [320128/344985]
loss: 3.661155  [326528/344985]
loss: 3.079147  [332928/344985]
loss: 3.214720  [339328/344985]

epoch avg train loss: 3.5868625   epoch avg train accuracy: 0.2221633

-------------------------------

Epoch 3

loss: 3.463877  [  128/344985]
loss: 3.175000  [ 6528/344985]
loss: 3.379387  [12928/344985]
loss: 3.274199  [19328/344985]
loss: 3.268643  [25728/344985]
loss: 3.367507  [32128/344985]
loss: 2.872978  [38528/344985]
loss: 3.295725  [44928/344985]
loss: 3.667034  [51328/344985]
loss: 3.381856  [57728/344985]
loss: 3.489910  [64128/344985]
loss: 3.375784  [70528/344985]
loss: 3.324939  [76928/344985]
loss: 2.954947  [83328/344985]
loss: 3.237172  [89728/344985]
loss: 3.253160  [96128/344985]
loss: 3.359326  [102528/344985]
loss: 3.285584  [108928/344985]
loss: 3.112249  [115328/344985]
loss: 3.274753  [121728/344985]
loss: 3.010979  [128128/344985]
loss: 3.105741  [134528/344985]
loss: 3.274626  [140928/344985]
loss: 3.541509  [147328/344985]
loss: 3.102568  [153728/344985]
loss: 3.103950  [160128/344985]
loss: 3.410994  [166528/344985]
loss: 3.058812  [172928/344985]
loss: 2.935868  [179328/344985]
loss: 3.341702  [185728/344985]
loss: 3.057844  [192128/344985]
loss: 3.299358  [198528/344985]
loss: 3.418523  [204928/344985]
loss: 3.709359  [211328/344985]
loss: 3.521556  [217728/344985]
loss: 3.408027  [224128/344985]
loss: 3.319057  [230528/344985]
loss: 3.515449  [236928/344985]
loss: 3.329139  [243328/344985]
loss: 3.246742  [249728/344985]
loss: 3.407632  [256128/344985]
loss: 3.212620  [262528/344985]
loss: 3.266609  [268928/344985]
loss: 3.212787  [275328/344985]
loss: 3.020598  [281728/344985]
loss: 2.991533  [288128/344985]
loss: 3.028393  [294528/344985]
loss: 3.217663  [300928/344985]
loss: 3.219996  [307328/344985]
loss: 3.383699  [313728/344985]
loss: 3.358459  [320128/344985]
loss: 3.306445  [326528/344985]
loss: 3.116737  [332928/344985]
loss: 3.235663  [339328/344985]

epoch avg train loss: 3.2717491   epoch avg train accuracy: 0.2727481

-------------------------------

Epoch 4

loss: 2.997998  [  128/344985]
loss: 3.122011  [ 6528/344985]
loss: 3.073257  [12928/344985]
loss: 3.019845  [19328/344985]
loss: 2.894377  [25728/344985]
loss: 2.886374  [32128/344985]
loss: 3.182348  [38528/344985]
loss: 3.168186  [44928/344985]
loss: 3.184882  [51328/344985]
loss: 2.910798  [57728/344985]
loss: 3.193214  [64128/344985]
loss: 2.926654  [70528/344985]
loss: 2.943067  [76928/344985]
loss: 2.987526  [83328/344985]
loss: 3.215355  [89728/344985]
loss: 3.238542  [96128/344985]
loss: 3.280833  [102528/344985]
loss: 2.800901  [108928/344985]
loss: 2.714877  [115328/344985]
loss: 2.689325  [121728/344985]
loss: 3.298775  [128128/344985]
loss: 3.121766  [134528/344985]
loss: 3.364531  [140928/344985]
loss: 3.035179  [147328/344985]
loss: 2.899277  [153728/344985]
loss: 2.954145  [160128/344985]
loss: 2.904945  [166528/344985]
loss: 2.975578  [172928/344985]
loss: 2.987744  [179328/344985]
loss: 3.008682  [185728/344985]
loss: 2.967368  [192128/344985]
loss: 3.052440  [198528/344985]
loss: 3.262609  [204928/344985]
loss: 2.999107  [211328/344985]
loss: 2.883327  [217728/344985]
loss: 2.789882  [224128/344985]
loss: 2.991693  [230528/344985]
loss: 2.969971  [236928/344985]
loss: 2.850116  [243328/344985]
loss: 2.902013  [249728/344985]
loss: 3.322261  [256128/344985]
loss: 3.255225  [262528/344985]
loss: 2.917663  [268928/344985]
loss: 2.803524  [275328/344985]
loss: 2.921860  [281728/344985]
loss: 3.223789  [288128/344985]
loss: 3.039315  [294528/344985]
loss: 3.003252  [300928/344985]
loss: 3.227479  [307328/344985]
loss: 3.011563  [313728/344985]
loss: 3.281972  [320128/344985]
loss: 3.174817  [326528/344985]
loss: 3.145789  [332928/344985]
loss: 3.378095  [339328/344985]

epoch avg train loss: 3.0671910   epoch avg train accuracy: 0.3074684

-------------------------------

Epoch 5

loss: 3.078960  [  128/344985]
loss: 2.803404  [ 6528/344985]
loss: 2.966770  [12928/344985]
loss: 3.060644  [19328/344985]
loss: 2.762113  [25728/344985]
loss: 3.088734  [32128/344985]
loss: 2.780213  [38528/344985]
loss: 2.785137  [44928/344985]
loss: 2.974069  [51328/344985]
loss: 2.733214  [57728/344985]
loss: 2.849131  [64128/344985]
loss: 2.973556  [70528/344985]
loss: 2.853296  [76928/344985]
loss: 2.623985  [83328/344985]
loss: 2.971232  [89728/344985]
loss: 2.952219  [96128/344985]
loss: 3.024085  [102528/344985]
loss: 2.638803  [108928/344985]
loss: 2.966601  [115328/344985]
loss: 3.027695  [121728/344985]
loss: 2.673875  [128128/344985]
loss: 2.772309  [134528/344985]
loss: 2.976045  [140928/344985]
loss: 2.802078  [147328/344985]
loss: 3.157665  [153728/344985]
loss: 2.756750  [160128/344985]
loss: 3.056446  [166528/344985]
loss: 2.658424  [172928/344985]
loss: 2.987552  [179328/344985]
loss: 2.692265  [185728/344985]
loss: 3.045379  [192128/344985]
loss: 2.799597  [198528/344985]
loss: 2.989698  [204928/344985]
loss: 2.964707  [211328/344985]
loss: 2.930540  [217728/344985]
loss: 3.080831  [224128/344985]
loss: 2.859419  [230528/344985]
loss: 2.857836  [236928/344985]
loss: 3.283739  [243328/344985]
loss: 3.094857  [249728/344985]
loss: 2.990768  [256128/344985]
loss: 2.762034  [262528/344985]
loss: 3.137968  [268928/344985]
loss: 2.852418  [275328/344985]
loss: 3.072470  [281728/344985]
loss: 2.760939  [288128/344985]
loss: 3.078422  [294528/344985]
loss: 2.568234  [300928/344985]
loss: 2.877925  [307328/344985]
loss: 2.521261  [313728/344985]
loss: 2.803722  [320128/344985]
loss: 2.866188  [326528/344985]
loss: 2.924227  [332928/344985]
loss: 2.750401  [339328/344985]

epoch avg train loss: 2.9083507   epoch avg train accuracy: 0.3333797

-------------------------------

Epoch 6

loss: 2.767572  [  128/344985]
loss: 2.792204  [ 6528/344985]
loss: 2.734449  [12928/344985]
loss: 2.758645  [19328/344985]
loss: 2.735305  [25728/344985]
loss: 2.772222  [32128/344985]
loss: 2.796735  [38528/344985]
loss: 2.552867  [44928/344985]
loss: 2.712187  [51328/344985]
loss: 2.602565  [57728/344985]
loss: 2.751795  [64128/344985]
loss: 2.671968  [70528/344985]
loss: 2.681297  [76928/344985]
loss: 2.755662  [83328/344985]
loss: 2.620088  [89728/344985]
loss: 2.810127  [96128/344985]
loss: 3.096161  [102528/344985]
loss: 2.780293  [108928/344985]
loss: 2.835765  [115328/344985]
loss: 2.578894  [121728/344985]
loss: 3.158295  [128128/344985]
loss: 2.797102  [134528/344985]
loss: 2.847440  [140928/344985]
loss: 3.115192  [147328/344985]
loss: 3.132391  [153728/344985]
loss: 3.165536  [160128/344985]
loss: 2.562805  [166528/344985]
loss: 2.785470  [172928/344985]
loss: 2.671550  [179328/344985]
loss: 2.932604  [185728/344985]
loss: 2.809041  [192128/344985]
loss: 2.720459  [198528/344985]
loss: 2.774284  [204928/344985]
loss: 2.409417  [211328/344985]
loss: 2.912586  [217728/344985]
loss: 2.816715  [224128/344985]
loss: 2.890933  [230528/344985]
loss: 2.978144  [236928/344985]
loss: 3.001525  [243328/344985]
loss: 2.963724  [249728/344985]
loss: 2.695026  [256128/344985]
loss: 3.036885  [262528/344985]
loss: 2.513101  [268928/344985]
loss: 2.937422  [275328/344985]
loss: 2.534702  [281728/344985]
loss: 2.984327  [288128/344985]
loss: 2.606492  [294528/344985]
loss: 2.877524  [300928/344985]
loss: 3.270559  [307328/344985]
loss: 2.760039  [313728/344985]
loss: 2.793476  [320128/344985]
loss: 2.909137  [326528/344985]
loss: 2.918413  [332928/344985]
loss: 2.667808  [339328/344985]

epoch avg train loss: 2.7766082   epoch avg train accuracy: 0.3566648

-------------------------------

Epoch 7

loss: 2.582978  [  128/344985]
loss: 2.363318  [ 6528/344985]
loss: 2.717526  [12928/344985]
loss: 2.271784  [19328/344985]
loss: 2.565234  [25728/344985]
loss: 2.619652  [32128/344985]
loss: 2.650733  [38528/344985]
loss: 2.669360  [44928/344985]
loss: 2.481284  [51328/344985]
loss: 2.645087  [57728/344985]
loss: 2.508070  [64128/344985]
loss: 2.700136  [70528/344985]
loss: 2.788104  [76928/344985]
loss: 2.799731  [83328/344985]
loss: 2.609379  [89728/344985]
loss: 2.332078  [96128/344985]
loss: 2.633464  [102528/344985]
loss: 2.617335  [108928/344985]
loss: 2.907305  [115328/344985]
loss: 2.871237  [121728/344985]
loss: 2.544829  [128128/344985]
loss: 2.912838  [134528/344985]
loss: 2.690559  [140928/344985]
loss: 2.840013  [147328/344985]
loss: 3.019375  [153728/344985]
loss: 2.379704  [160128/344985]
loss: 2.692153  [166528/344985]
loss: 2.565120  [172928/344985]
loss: 2.637398  [179328/344985]
loss: 2.739363  [185728/344985]
loss: 2.682156  [192128/344985]
loss: 2.587951  [198528/344985]
loss: 2.822630  [204928/344985]
loss: 2.900932  [211328/344985]
loss: 2.607624  [217728/344985]
loss: 2.697011  [224128/344985]
loss: 2.977220  [230528/344985]
loss: 2.671236  [236928/344985]
loss: 2.484691  [243328/344985]
loss: 2.684420  [249728/344985]
loss: 2.760016  [256128/344985]
loss: 2.791014  [262528/344985]
loss: 2.718674  [268928/344985]
loss: 3.124574  [275328/344985]
loss: 2.812865  [281728/344985]
loss: 2.989656  [288128/344985]
loss: 2.801873  [294528/344985]
loss: 2.859938  [300928/344985]
loss: 2.500010  [307328/344985]
loss: 2.852725  [313728/344985]
loss: 2.689047  [320128/344985]
loss: 2.538147  [326528/344985]
loss: 2.628637  [332928/344985]
loss: 2.854202  [339328/344985]

epoch avg train loss: 2.6639798   epoch avg train accuracy: 0.3759990

-------------------------------

Epoch 8

loss: 3.004280  [  128/344985]
loss: 2.313390  [ 6528/344985]
loss: 2.535013  [12928/344985]
loss: 2.264415  [19328/344985]
loss: 2.520676  [25728/344985]
loss: 2.227353  [32128/344985]
loss: 2.577237  [38528/344985]
loss: 2.249290  [44928/344985]
loss: 2.603054  [51328/344985]
loss: 2.486972  [57728/344985]
loss: 2.726875  [64128/344985]
loss: 2.258656  [70528/344985]
loss: 3.002309  [76928/344985]
loss: 2.742215  [83328/344985]
loss: 2.625855  [89728/344985]
loss: 2.593587  [96128/344985]
loss: 2.509348  [102528/344985]
loss: 2.771373  [108928/344985]
loss: 2.413842  [115328/344985]
loss: 2.636343  [121728/344985]
loss: 2.595287  [128128/344985]
loss: 2.827802  [134528/344985]
loss: 2.653534  [140928/344985]
loss: 2.366745  [147328/344985]
loss: 2.671218  [153728/344985]
loss: 2.446591  [160128/344985]
loss: 2.758693  [166528/344985]
loss: 2.438956  [172928/344985]
loss: 2.587151  [179328/344985]
loss: 2.383249  [185728/344985]
loss: 2.473254  [192128/344985]
loss: 2.475271  [198528/344985]
loss: 2.812447  [204928/344985]
loss: 2.665518  [211328/344985]
loss: 2.532484  [217728/344985]
loss: 2.359743  [224128/344985]
loss: 2.421068  [230528/344985]
loss: 2.472054  [236928/344985]
loss: 2.675433  [243328/344985]
loss: 2.524529  [249728/344985]
loss: 2.543768  [256128/344985]
loss: 2.359198  [262528/344985]
loss: 2.582504  [268928/344985]
loss: 2.556999  [275328/344985]
loss: 2.308677  [281728/344985]
loss: 2.754745  [288128/344985]
loss: 2.507406  [294528/344985]
loss: 2.394172  [300928/344985]
loss: 2.507313  [307328/344985]
loss: 2.489536  [313728/344985]
loss: 2.539317  [320128/344985]
loss: 2.346912  [326528/344985]
loss: 2.529267  [332928/344985]
loss: 2.729165  [339328/344985]

epoch avg train loss: 2.5634253   epoch avg train accuracy: 0.3940983

-------------------------------

Epoch 9

loss: 2.399187  [  128/344985]
loss: 2.941875  [ 6528/344985]
loss: 2.434621  [12928/344985]
loss: 2.566409  [19328/344985]
loss: 2.451424  [25728/344985]
loss: 2.058022  [32128/344985]
loss: 2.502174  [38528/344985]
loss: 2.254193  [44928/344985]
loss: 2.995513  [51328/344985]
loss: 2.161513  [57728/344985]
loss: 2.128356  [64128/344985]
loss: 2.331192  [70528/344985]
loss: 2.648278  [76928/344985]
loss: 2.535859  [83328/344985]
loss: 2.926055  [89728/344985]
loss: 2.565883  [96128/344985]
loss: 2.641196  [102528/344985]
loss: 2.447062  [108928/344985]
loss: 2.508712  [115328/344985]
loss: 2.742864  [121728/344985]
loss: 2.188294  [128128/344985]
loss: 2.531087  [134528/344985]
loss: 2.177687  [140928/344985]
loss: 2.022951  [147328/344985]
loss: 2.759223  [153728/344985]
loss: 2.501718  [160128/344985]
loss: 2.346086  [166528/344985]
loss: 2.563789  [172928/344985]
loss: 2.601613  [179328/344985]
loss: 2.143779  [185728/344985]
loss: 2.370405  [192128/344985]
loss: 2.678187  [198528/344985]
loss: 2.253794  [204928/344985]
loss: 2.587817  [211328/344985]
loss: 2.599182  [217728/344985]
loss: 2.310044  [224128/344985]
loss: 2.475358  [230528/344985]
loss: 2.539760  [236928/344985]
loss: 2.325305  [243328/344985]
loss: 2.155960  [249728/344985]
loss: 2.318959  [256128/344985]
loss: 2.530150  [262528/344985]
loss: 2.563175  [268928/344985]
loss: 2.757624  [275328/344985]
loss: 2.542272  [281728/344985]
loss: 2.628329  [288128/344985]
loss: 2.568985  [294528/344985]
loss: 2.476191  [300928/344985]
loss: 2.416316  [307328/344985]
loss: 2.208236  [313728/344985]
loss: 2.717778  [320128/344985]
loss: 2.569165  [326528/344985]
loss: 2.929086  [332928/344985]
loss: 2.659088  [339328/344985]

epoch avg train loss: 2.4742225   epoch avg train accuracy: 0.4087076

-------------------------------

Epoch 10

loss: 2.394444  [  128/344985]
loss: 2.385282  [ 6528/344985]
loss: 2.296645  [12928/344985]
loss: 2.458971  [19328/344985]
loss: 2.447184  [25728/344985]
loss: 1.886548  [32128/344985]
loss: 2.546888  [38528/344985]
loss: 2.147963  [44928/344985]
loss: 2.302635  [51328/344985]
loss: 2.166574  [57728/344985]
loss: 2.673657  [64128/344985]
loss: 2.455467  [70528/344985]
loss: 2.478492  [76928/344985]
loss: 2.553147  [83328/344985]
loss: 2.677609  [89728/344985]
loss: 2.440191  [96128/344985]
loss: 2.363901  [102528/344985]
loss: 2.470370  [108928/344985]
loss: 2.400209  [115328/344985]
loss: 2.494039  [121728/344985]
loss: 2.649257  [128128/344985]
loss: 2.440501  [134528/344985]
loss: 2.602360  [140928/344985]
loss: 2.158322  [147328/344985]
loss: 2.533826  [153728/344985]
loss: 2.425714  [160128/344985]
loss: 2.302327  [166528/344985]
loss: 2.634569  [172928/344985]
loss: 2.297996  [179328/344985]
loss: 2.334992  [185728/344985]
loss: 2.687079  [192128/344985]
loss: 2.799641  [198528/344985]
loss: 2.347206  [204928/344985]
loss: 2.568370  [211328/344985]
loss: 2.745245  [217728/344985]
loss: 2.661206  [224128/344985]
loss: 2.338608  [230528/344985]
loss: 2.557315  [236928/344985]
loss: 2.139861  [243328/344985]
loss: 2.272426  [249728/344985]
loss: 2.389536  [256128/344985]
loss: 2.329299  [262528/344985]
loss: 2.173815  [268928/344985]
loss: 2.163170  [275328/344985]
loss: 2.191614  [281728/344985]
loss: 2.718037  [288128/344985]
loss: 2.416931  [294528/344985]
loss: 2.455895  [300928/344985]
loss: 2.576985  [307328/344985]
loss: 2.259164  [313728/344985]
loss: 2.699352  [320128/344985]
loss: 2.334037  [326528/344985]
loss: 2.144558  [332928/344985]
loss: 2.635159  [339328/344985]

epoch avg train loss: 2.3907324   epoch avg train accuracy: 0.4243779

-------------------------------

Evaluating against random transformations...
Mean acc: 0.3290
Acc std: 0.0011106
