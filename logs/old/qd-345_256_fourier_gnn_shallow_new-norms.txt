Epoch 1

loss: 5.845556  [  128/344985]
loss: 5.744629  [ 6528/344985]
loss: 5.697204  [12928/344985]
loss: 5.656313  [19328/344985]
loss: 5.546644  [25728/344985]
loss: 5.436589  [32128/344985]
loss: 5.437139  [38528/344985]
loss: 5.197626  [44928/344985]
loss: 5.298802  [51328/344985]
loss: 5.370777  [57728/344985]
loss: 5.103415  [64128/344985]
loss: 5.160810  [70528/344985]
loss: 5.139074  [76928/344985]
loss: 5.208478  [83328/344985]
loss: 4.932832  [89728/344985]
loss: 4.832615  [96128/344985]
loss: 4.772909  [102528/344985]
loss: 4.761478  [108928/344985]
loss: 4.754498  [115328/344985]
loss: 4.829909  [121728/344985]
loss: 4.758648  [128128/344985]
loss: 4.768402  [134528/344985]
loss: 4.575930  [140928/344985]
loss: 4.533993  [147328/344985]
loss: 4.506615  [153728/344985]
loss: 4.417759  [160128/344985]
loss: 4.505005  [166528/344985]
loss: 4.382392  [172928/344985]
loss: 4.661920  [179328/344985]
loss: 4.172968  [185728/344985]
loss: 4.446963  [192128/344985]
loss: 4.357666  [198528/344985]
loss: 4.364563  [204928/344985]
loss: 4.369616  [211328/344985]
loss: 4.187705  [217728/344985]
loss: 4.048713  [224128/344985]
loss: 4.245634  [230528/344985]
loss: 3.884548  [236928/344985]
loss: 4.534232  [243328/344985]
loss: 3.871984  [249728/344985]
loss: 3.862837  [256128/344985]
loss: 4.168991  [262528/344985]
loss: 4.158816  [268928/344985]
loss: 4.322024  [275328/344985]
loss: 4.137567  [281728/344985]
loss: 3.856100  [288128/344985]
loss: 4.025023  [294528/344985]
loss: 3.854502  [300928/344985]
loss: 4.335506  [307328/344985]
loss: 3.839138  [313728/344985]
loss: 3.921830  [320128/344985]
loss: 3.903525  [326528/344985]
loss: 3.740952  [332928/344985]
loss: 3.708086  [339328/344985]

epoch avg train loss: 4.5437001   epoch avg train accuracy: 0.1009464

-------------------------------

Epoch 2

loss: 3.718058  [  128/344985]
loss: 4.157408  [ 6528/344985]
loss: 3.608305  [12928/344985]
loss: 3.781812  [19328/344985]
loss: 3.525113  [25728/344985]
loss: 3.555998  [32128/344985]
loss: 3.915726  [38528/344985]
loss: 3.912606  [44928/344985]
loss: 3.630048  [51328/344985]
loss: 3.743592  [57728/344985]
loss: 3.812427  [64128/344985]
loss: 3.816107  [70528/344985]
loss: 3.679081  [76928/344985]
loss: 3.397311  [83328/344985]
loss: 3.817340  [89728/344985]
loss: 3.486249  [96128/344985]
loss: 3.864625  [102528/344985]
loss: 3.853873  [108928/344985]
loss: 3.702710  [115328/344985]
loss: 3.850996  [121728/344985]
loss: 3.576258  [128128/344985]
loss: 3.916525  [134528/344985]
loss: 3.736000  [140928/344985]
loss: 3.778673  [147328/344985]
loss: 3.750863  [153728/344985]
loss: 3.774100  [160128/344985]
loss: 3.620456  [166528/344985]
loss: 3.544138  [172928/344985]
loss: 3.645285  [179328/344985]
loss: 3.451345  [185728/344985]
loss: 3.867806  [192128/344985]
loss: 3.311180  [198528/344985]
loss: 3.282464  [204928/344985]
loss: 3.285432  [211328/344985]
loss: 3.191302  [217728/344985]
loss: 3.677491  [224128/344985]
loss: 3.858980  [230528/344985]
loss: 3.399257  [236928/344985]
loss: 3.871581  [243328/344985]
loss: 3.744308  [249728/344985]
loss: 2.912657  [256128/344985]
loss: 3.491074  [262528/344985]
loss: 3.670431  [268928/344985]
loss: 3.540050  [275328/344985]
loss: 3.413659  [281728/344985]
loss: 3.645535  [288128/344985]
loss: 3.579842  [294528/344985]
loss: 3.598587  [300928/344985]
loss: 3.329206  [307328/344985]
loss: 3.459171  [313728/344985]
loss: 3.594618  [320128/344985]
loss: 3.602353  [326528/344985]
loss: 3.125637  [332928/344985]
loss: 3.222912  [339328/344985]

epoch avg train loss: 3.6082624   epoch avg train accuracy: 0.2200965

-------------------------------

Epoch 3

loss: 3.465402  [  128/344985]
loss: 3.323348  [ 6528/344985]
loss: 3.425107  [12928/344985]
loss: 3.294423  [19328/344985]
loss: 3.076089  [25728/344985]
loss: 3.410892  [32128/344985]
loss: 2.889585  [38528/344985]
loss: 3.320896  [44928/344985]
loss: 3.723730  [51328/344985]
loss: 3.443991  [57728/344985]
loss: 3.570670  [64128/344985]
loss: 3.446069  [70528/344985]
loss: 3.348935  [76928/344985]
loss: 2.992225  [83328/344985]
loss: 3.271981  [89728/344985]
loss: 3.287953  [96128/344985]
loss: 3.363223  [102528/344985]
loss: 3.263905  [108928/344985]
loss: 3.169061  [115328/344985]
loss: 3.281770  [121728/344985]
loss: 3.098175  [128128/344985]
loss: 3.115781  [134528/344985]
loss: 3.270721  [140928/344985]
loss: 3.569911  [147328/344985]
loss: 2.988563  [153728/344985]
loss: 3.174140  [160128/344985]
loss: 3.552543  [166528/344985]
loss: 3.020805  [172928/344985]
loss: 2.752001  [179328/344985]
loss: 3.510744  [185728/344985]
loss: 3.093035  [192128/344985]
loss: 3.331907  [198528/344985]
loss: 3.336161  [204928/344985]
loss: 3.547755  [211328/344985]
loss: 3.496762  [217728/344985]
loss: 3.447422  [224128/344985]
loss: 3.378785  [230528/344985]
loss: 3.467645  [236928/344985]
loss: 3.344508  [243328/344985]
loss: 3.245906  [249728/344985]
loss: 3.540386  [256128/344985]
loss: 3.165167  [262528/344985]
loss: 3.191400  [268928/344985]
loss: 3.279037  [275328/344985]
loss: 2.974253  [281728/344985]
loss: 3.074589  [288128/344985]
loss: 3.023991  [294528/344985]
loss: 3.260184  [300928/344985]
loss: 3.249555  [307328/344985]
loss: 3.403860  [313728/344985]
loss: 3.303120  [320128/344985]
loss: 3.275890  [326528/344985]
loss: 3.126278  [332928/344985]
loss: 3.287748  [339328/344985]

epoch avg train loss: 3.2814164   epoch avg train accuracy: 0.2715017

-------------------------------

Epoch 4

loss: 3.063251  [  128/344985]
loss: 3.257428  [ 6528/344985]
loss: 3.114710  [12928/344985]
loss: 3.016988  [19328/344985]
loss: 2.812091  [25728/344985]
loss: 2.793799  [32128/344985]
loss: 3.056493  [38528/344985]
loss: 3.156751  [44928/344985]
loss: 3.052468  [51328/344985]
loss: 2.919771  [57728/344985]
loss: 3.178864  [64128/344985]
loss: 2.930674  [70528/344985]
loss: 2.909375  [76928/344985]
loss: 2.964112  [83328/344985]
loss: 3.122409  [89728/344985]
loss: 3.331584  [96128/344985]
loss: 3.344577  [102528/344985]
loss: 2.696976  [108928/344985]
loss: 2.856851  [115328/344985]
loss: 2.779354  [121728/344985]
loss: 3.286073  [128128/344985]
loss: 3.194895  [134528/344985]
loss: 3.293837  [140928/344985]
loss: 3.058159  [147328/344985]
loss: 2.961289  [153728/344985]
loss: 2.962888  [160128/344985]
loss: 2.920074  [166528/344985]
loss: 3.001641  [172928/344985]
loss: 2.965425  [179328/344985]
loss: 2.862844  [185728/344985]
loss: 3.017026  [192128/344985]
loss: 3.140372  [198528/344985]
loss: 3.471037  [204928/344985]
loss: 2.995149  [211328/344985]
loss: 2.915720  [217728/344985]
loss: 2.835637  [224128/344985]
loss: 2.814379  [230528/344985]
loss: 3.150911  [236928/344985]
loss: 2.861471  [243328/344985]
loss: 3.060795  [249728/344985]
loss: 3.216228  [256128/344985]
loss: 3.377269  [262528/344985]
loss: 3.001645  [268928/344985]
loss: 2.704592  [275328/344985]
loss: 2.937372  [281728/344985]
loss: 3.178349  [288128/344985]
loss: 3.243785  [294528/344985]
loss: 3.018494  [300928/344985]
loss: 3.092900  [307328/344985]
loss: 2.969134  [313728/344985]
loss: 3.208415  [320128/344985]
loss: 3.240884  [326528/344985]
loss: 3.130464  [332928/344985]
loss: 3.491331  [339328/344985]

epoch avg train loss: 3.0705537   epoch avg train accuracy: 0.3077641

-------------------------------

Epoch 5

loss: 3.200886  [  128/344985]
loss: 2.702410  [ 6528/344985]
loss: 2.913056  [12928/344985]
loss: 3.067029  [19328/344985]
loss: 2.787279  [25728/344985]
loss: 3.015865  [32128/344985]
loss: 2.764565  [38528/344985]
loss: 2.823329  [44928/344985]
loss: 2.990643  [51328/344985]
loss: 2.700266  [57728/344985]
loss: 2.691757  [64128/344985]
loss: 3.145914  [70528/344985]
loss: 2.855744  [76928/344985]
loss: 2.566272  [83328/344985]
loss: 2.902412  [89728/344985]
loss: 2.984923  [96128/344985]
loss: 2.903547  [102528/344985]
loss: 2.688351  [108928/344985]
loss: 3.082433  [115328/344985]
loss: 2.904174  [121728/344985]
loss: 2.840760  [128128/344985]
loss: 2.936303  [134528/344985]
loss: 3.014365  [140928/344985]
loss: 2.818929  [147328/344985]
loss: 2.983421  [153728/344985]
loss: 2.644840  [160128/344985]
loss: 3.217844  [166528/344985]
loss: 2.646844  [172928/344985]
loss: 2.908739  [179328/344985]
loss: 2.763647  [185728/344985]
loss: 3.063473  [192128/344985]
loss: 2.822749  [198528/344985]
loss: 2.896937  [204928/344985]
loss: 2.862331  [211328/344985]
loss: 2.915330  [217728/344985]
loss: 3.029547  [224128/344985]
loss: 2.791534  [230528/344985]
loss: 2.706077  [236928/344985]
loss: 3.339974  [243328/344985]
loss: 3.063643  [249728/344985]
loss: 2.989120  [256128/344985]
loss: 2.846126  [262528/344985]
loss: 2.966546  [268928/344985]
loss: 2.694733  [275328/344985]
loss: 3.013459  [281728/344985]
loss: 2.760264  [288128/344985]
loss: 3.173142  [294528/344985]
loss: 2.689262  [300928/344985]
loss: 2.833552  [307328/344985]
loss: 2.706355  [313728/344985]
loss: 2.819252  [320128/344985]
loss: 2.679132  [326528/344985]
loss: 2.884820  [332928/344985]
loss: 2.844377  [339328/344985]

epoch avg train loss: 2.9112510   epoch avg train accuracy: 0.3341855

-------------------------------

Epoch 6

loss: 2.689558  [  128/344985]
loss: 2.787465  [ 6528/344985]
loss: 2.844987  [12928/344985]
loss: 2.745906  [19328/344985]
loss: 2.865052  [25728/344985]
loss: 2.865299  [32128/344985]
loss: 2.923277  [38528/344985]
loss: 2.461845  [44928/344985]
loss: 2.719404  [51328/344985]
loss: 2.631105  [57728/344985]
loss: 2.818936  [64128/344985]
loss: 2.604364  [70528/344985]
loss: 2.606699  [76928/344985]
loss: 2.683189  [83328/344985]
loss: 2.602867  [89728/344985]
loss: 2.746828  [96128/344985]
loss: 2.935839  [102528/344985]
loss: 2.834011  [108928/344985]
loss: 2.877493  [115328/344985]
loss: 2.524602  [121728/344985]
loss: 3.167328  [128128/344985]
loss: 2.834251  [134528/344985]
loss: 2.795917  [140928/344985]
loss: 3.004668  [147328/344985]
loss: 3.178202  [153728/344985]
loss: 3.162671  [160128/344985]
loss: 2.576040  [166528/344985]
loss: 2.737262  [172928/344985]
loss: 2.775304  [179328/344985]
loss: 2.773940  [185728/344985]
loss: 2.756608  [192128/344985]
loss: 2.621267  [198528/344985]
loss: 2.829543  [204928/344985]
loss: 2.394032  [211328/344985]
loss: 3.115878  [217728/344985]
loss: 2.915334  [224128/344985]
loss: 2.796175  [230528/344985]
loss: 2.856101  [236928/344985]
loss: 2.980287  [243328/344985]
loss: 3.115885  [249728/344985]
loss: 2.621193  [256128/344985]
loss: 3.133812  [262528/344985]
loss: 2.611014  [268928/344985]
loss: 3.041504  [275328/344985]
loss: 2.631832  [281728/344985]
loss: 3.011801  [288128/344985]
loss: 2.655664  [294528/344985]
loss: 2.937044  [300928/344985]
loss: 3.201425  [307328/344985]
loss: 2.616666  [313728/344985]
loss: 2.754004  [320128/344985]
loss: 3.011908  [326528/344985]
loss: 2.825640  [332928/344985]
loss: 2.534729  [339328/344985]

epoch avg train loss: 2.7791379   epoch avg train accuracy: 0.3572097

-------------------------------

Epoch 7

loss: 2.576043  [  128/344985]
loss: 2.344347  [ 6528/344985]
loss: 2.683347  [12928/344985]
loss: 2.194951  [19328/344985]
loss: 2.672322  [25728/344985]
loss: 2.946789  [32128/344985]
loss: 2.735414  [38528/344985]
loss: 2.692574  [44928/344985]
loss: 2.434614  [51328/344985]
loss: 2.856235  [57728/344985]
loss: 2.586443  [64128/344985]
loss: 2.681227  [70528/344985]
loss: 2.706446  [76928/344985]
loss: 2.839333  [83328/344985]
loss: 2.553623  [89728/344985]
loss: 2.224083  [96128/344985]
loss: 2.612734  [102528/344985]
loss: 2.573959  [108928/344985]
loss: 2.786874  [115328/344985]
loss: 2.847639  [121728/344985]
loss: 2.608449  [128128/344985]
loss: 2.856175  [134528/344985]
loss: 2.714312  [140928/344985]
loss: 2.865063  [147328/344985]
loss: 3.045782  [153728/344985]
loss: 2.421200  [160128/344985]
loss: 2.730964  [166528/344985]
loss: 2.609252  [172928/344985]
loss: 2.479106  [179328/344985]
loss: 2.714163  [185728/344985]
loss: 2.897140  [192128/344985]
loss: 2.520773  [198528/344985]
loss: 2.834882  [204928/344985]
loss: 2.932540  [211328/344985]
loss: 2.339033  [217728/344985]
loss: 2.772031  [224128/344985]
loss: 2.949834  [230528/344985]
loss: 2.670901  [236928/344985]
loss: 2.481182  [243328/344985]
loss: 2.769001  [249728/344985]
loss: 2.804973  [256128/344985]
loss: 2.734836  [262528/344985]
loss: 2.749505  [268928/344985]
loss: 3.076544  [275328/344985]
loss: 2.692638  [281728/344985]
loss: 3.020675  [288128/344985]
loss: 2.755965  [294528/344985]
loss: 2.986764  [300928/344985]
loss: 2.689685  [307328/344985]
loss: 2.969307  [313728/344985]
loss: 2.567925  [320128/344985]
loss: 2.796002  [326528/344985]
loss: 2.440126  [332928/344985]
loss: 2.803854  [339328/344985]

epoch avg train loss: 2.6666895   epoch avg train accuracy: 0.3770135

-------------------------------

Epoch 8

loss: 2.865738  [  128/344985]
loss: 2.365898  [ 6528/344985]
loss: 2.540827  [12928/344985]
loss: 2.319049  [19328/344985]
loss: 2.490926  [25728/344985]
loss: 2.401093  [32128/344985]
loss: 2.455440  [38528/344985]
loss: 2.220275  [44928/344985]
loss: 2.492708  [51328/344985]
loss: 2.271529  [57728/344985]
loss: 2.696787  [64128/344985]
loss: 2.394491  [70528/344985]
loss: 3.008522  [76928/344985]
loss: 2.770936  [83328/344985]
loss: 2.536299  [89728/344985]
loss: 2.587655  [96128/344985]
loss: 2.530649  [102528/344985]
loss: 2.855700  [108928/344985]
loss: 2.148665  [115328/344985]
loss: 2.619192  [121728/344985]
loss: 2.578079  [128128/344985]
loss: 2.758062  [134528/344985]
loss: 2.720747  [140928/344985]
loss: 2.361338  [147328/344985]
loss: 2.637303  [153728/344985]
loss: 2.404242  [160128/344985]
loss: 2.824753  [166528/344985]
loss: 2.464820  [172928/344985]
loss: 2.663927  [179328/344985]
loss: 2.388509  [185728/344985]
loss: 2.436386  [192128/344985]
loss: 2.534398  [198528/344985]
loss: 2.948412  [204928/344985]
loss: 2.745414  [211328/344985]
loss: 2.525245  [217728/344985]
loss: 2.391927  [224128/344985]
loss: 2.441811  [230528/344985]
loss: 2.537693  [236928/344985]
loss: 2.562596  [243328/344985]
loss: 2.427207  [249728/344985]
loss: 2.537691  [256128/344985]
loss: 2.328563  [262528/344985]
loss: 2.687779  [268928/344985]
loss: 2.459822  [275328/344985]
loss: 2.458059  [281728/344985]
loss: 2.705318  [288128/344985]
loss: 2.519680  [294528/344985]
loss: 2.320569  [300928/344985]
loss: 2.566283  [307328/344985]
loss: 2.464881  [313728/344985]
loss: 2.574565  [320128/344985]
loss: 2.317797  [326528/344985]
loss: 2.487639  [332928/344985]
loss: 2.607109  [339328/344985]

epoch avg train loss: 2.5674505   epoch avg train accuracy: 0.3945128

-------------------------------

Epoch 9

loss: 2.419488  [  128/344985]
loss: 2.816329  [ 6528/344985]
loss: 2.413592  [12928/344985]
loss: 2.691298  [19328/344985]
loss: 2.517097  [25728/344985]
loss: 2.070379  [32128/344985]
loss: 2.579864  [38528/344985]
loss: 2.359820  [44928/344985]
loss: 2.831152  [51328/344985]
loss: 2.305341  [57728/344985]
loss: 2.011979  [64128/344985]
loss: 2.239377  [70528/344985]
loss: 2.642290  [76928/344985]
loss: 2.434909  [83328/344985]
loss: 2.871300  [89728/344985]
loss: 2.505892  [96128/344985]
loss: 2.580863  [102528/344985]
loss: 2.378638  [108928/344985]
loss: 2.663301  [115328/344985]
loss: 2.722915  [121728/344985]
loss: 2.374253  [128128/344985]
loss: 2.548236  [134528/344985]
loss: 2.184294  [140928/344985]
loss: 2.129352  [147328/344985]
loss: 2.753750  [153728/344985]
loss: 2.424315  [160128/344985]
loss: 2.471690  [166528/344985]
loss: 2.617738  [172928/344985]
loss: 2.520983  [179328/344985]
loss: 2.144334  [185728/344985]
loss: 2.490883  [192128/344985]
loss: 2.627391  [198528/344985]
loss: 2.235387  [204928/344985]
loss: 2.611116  [211328/344985]
loss: 2.811891  [217728/344985]
loss: 2.325668  [224128/344985]
loss: 2.484586  [230528/344985]
loss: 2.507826  [236928/344985]
loss: 2.278461  [243328/344985]
loss: 2.048187  [249728/344985]
loss: 2.391613  [256128/344985]
loss: 2.549070  [262528/344985]
loss: 2.308662  [268928/344985]
loss: 2.809407  [275328/344985]
loss: 2.533886  [281728/344985]
loss: 2.476527  [288128/344985]
loss: 2.492195  [294528/344985]
loss: 2.378640  [300928/344985]
loss: 2.609732  [307328/344985]
loss: 2.278921  [313728/344985]
loss: 2.844786  [320128/344985]
loss: 2.341295  [326528/344985]
loss: 2.678482  [332928/344985]
loss: 2.446280  [339328/344985]

epoch avg train loss: 2.4807268   epoch avg train accuracy: 0.4105222

-------------------------------

Epoch 10

loss: 2.385204  [  128/344985]
loss: 2.332322  [ 6528/344985]
loss: 2.211070  [12928/344985]
loss: 2.405726  [19328/344985]
loss: 2.274224  [25728/344985]
loss: 2.126822  [32128/344985]
loss: 2.527045  [38528/344985]
loss: 2.184323  [44928/344985]
loss: 2.322067  [51328/344985]
loss: 2.130325  [57728/344985]
loss: 2.777947  [64128/344985]
loss: 2.540701  [70528/344985]
loss: 2.601290  [76928/344985]
loss: 2.566372  [83328/344985]
loss: 2.487950  [89728/344985]
loss: 2.426224  [96128/344985]
loss: 2.368206  [102528/344985]
loss: 2.643835  [108928/344985]
loss: 2.370701  [115328/344985]
loss: 2.687366  [121728/344985]
loss: 2.620974  [128128/344985]
loss: 2.216534  [134528/344985]
loss: 2.558275  [140928/344985]
loss: 2.272044  [147328/344985]
loss: 2.218621  [153728/344985]
loss: 2.413841  [160128/344985]
loss: 2.417039  [166528/344985]
loss: 2.609580  [172928/344985]
loss: 2.332505  [179328/344985]
loss: 2.367303  [185728/344985]
loss: 2.605968  [192128/344985]
loss: 2.666725  [198528/344985]
loss: 2.348468  [204928/344985]
loss: 2.526678  [211328/344985]
loss: 2.517718  [217728/344985]
loss: 2.653784  [224128/344985]
loss: 2.326887  [230528/344985]
loss: 2.565731  [236928/344985]
loss: 2.160417  [243328/344985]
loss: 2.509687  [249728/344985]
loss: 2.441195  [256128/344985]
loss: 2.313469  [262528/344985]
loss: 2.408201  [268928/344985]
loss: 2.360470  [275328/344985]
loss: 2.286019  [281728/344985]
loss: 2.583673  [288128/344985]
loss: 2.238354  [294528/344985]
loss: 2.436572  [300928/344985]
loss: 2.659182  [307328/344985]
loss: 2.546333  [313728/344985]
loss: 2.511837  [320128/344985]
loss: 2.423634  [326528/344985]
loss: 2.279237  [332928/344985]
loss: 2.699585  [339328/344985]

epoch avg train loss: 2.3989222   epoch avg train accuracy: 0.4236822

-------------------------------

Evaluating against random transformations...
Mean acc: 0.3329
Acc std: 0.0010019
