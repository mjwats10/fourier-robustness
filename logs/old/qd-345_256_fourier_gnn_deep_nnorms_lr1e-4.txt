Epoch 1

loss: 5.843552  [  128/344985]
loss: 5.795595  [ 6528/344985]
loss: 5.740228  [12928/344985]
loss: 5.774501  [19328/344985]
loss: 5.679542  [25728/344985]
loss: 5.702126  [32128/344985]
loss: 5.745021  [38528/344985]
loss: 5.561383  [44928/344985]
loss: 5.631658  [51328/344985]
loss: 5.590006  [57728/344985]
loss: 5.538618  [64128/344985]
loss: 5.543729  [70528/344985]
loss: 5.451292  [76928/344985]
loss: 5.569228  [83328/344985]
loss: 5.367768  [89728/344985]
loss: 5.448903  [96128/344985]
loss: 5.413080  [102528/344985]
loss: 5.226177  [108928/344985]
loss: 5.358971  [115328/344985]
loss: 5.393837  [121728/344985]
loss: 5.288548  [128128/344985]
loss: 5.475719  [134528/344985]
loss: 5.284804  [140928/344985]
loss: 5.253545  [147328/344985]
loss: 5.341139  [153728/344985]
loss: 5.266811  [160128/344985]
loss: 5.254002  [166528/344985]
loss: 5.308838  [172928/344985]
loss: 5.304945  [179328/344985]
loss: 5.059763  [185728/344985]
loss: 5.190716  [192128/344985]
loss: 5.262009  [198528/344985]
loss: 5.104894  [204928/344985]
loss: 5.029017  [211328/344985]
loss: 5.004098  [217728/344985]
loss: 5.002600  [224128/344985]
loss: 5.128588  [230528/344985]
loss: 4.972691  [236928/344985]
loss: 5.148548  [243328/344985]
loss: 4.873963  [249728/344985]
loss: 4.889091  [256128/344985]
loss: 4.913238  [262528/344985]
loss: 4.990879  [268928/344985]
loss: 5.091365  [275328/344985]
loss: 4.985081  [281728/344985]
loss: 4.837776  [288128/344985]
loss: 4.959145  [294528/344985]
loss: 4.961321  [300928/344985]
loss: 5.023142  [307328/344985]
loss: 4.819672  [313728/344985]
loss: 4.801157  [320128/344985]
loss: 4.756231  [326528/344985]
loss: 4.940227  [332928/344985]
loss: 4.789776  [339328/344985]

epoch avg train loss: 5.2268145   epoch avg train accuracy: 0.0270128

-------------------------------

Epoch 2

loss: 4.807897  [  128/344985]
loss: 4.847584  [ 6528/344985]
loss: 4.726282  [12928/344985]
loss: 4.812669  [19328/344985]
loss: 4.636178  [25728/344985]
loss: 4.682113  [32128/344985]
loss: 4.805936  [38528/344985]
loss: 4.888474  [44928/344985]
loss: 4.649288  [51328/344985]
loss: 4.690414  [57728/344985]
loss: 4.786266  [64128/344985]
loss: 4.893755  [70528/344985]
loss: 4.707312  [76928/344985]
loss: 4.662210  [83328/344985]
loss: 4.622601  [89728/344985]
loss: 4.353502  [96128/344985]
loss: 4.731354  [102528/344985]
loss: 4.621943  [108928/344985]
loss: 4.729677  [115328/344985]
loss: 4.735191  [121728/344985]
loss: 4.774112  [128128/344985]
loss: 4.843306  [134528/344985]
loss: 4.815248  [140928/344985]
loss: 4.575628  [147328/344985]
loss: 4.692066  [153728/344985]
loss: 4.533105  [160128/344985]
loss: 4.562402  [166528/344985]
loss: 4.514540  [172928/344985]
loss: 4.471225  [179328/344985]
loss: 4.462633  [185728/344985]
loss: 4.874171  [192128/344985]
loss: 4.471728  [198528/344985]
loss: 4.364133  [204928/344985]
loss: 4.404718  [211328/344985]
loss: 4.214654  [217728/344985]
loss: 4.527707  [224128/344985]
loss: 4.672040  [230528/344985]
loss: 4.312736  [236928/344985]
loss: 4.891432  [243328/344985]
loss: 4.474216  [249728/344985]
loss: 4.185147  [256128/344985]
loss: 4.529140  [262528/344985]
loss: 4.490365  [268928/344985]
loss: 4.601475  [275328/344985]
loss: 4.551784  [281728/344985]
loss: 4.451298  [288128/344985]
loss: 4.542846  [294528/344985]
loss: 4.679481  [300928/344985]
loss: 4.588473  [307328/344985]
loss: 4.450679  [313728/344985]
loss: 4.318438  [320128/344985]
loss: 4.587971  [326528/344985]
loss: 4.334319  [332928/344985]
loss: 4.197299  [339328/344985]

epoch avg train loss: 4.6032336   epoch avg train accuracy: 0.0728930

-------------------------------

Epoch 3

loss: 4.584368  [  128/344985]
loss: 4.329807  [ 6528/344985]
loss: 4.570106  [12928/344985]
loss: 4.399294  [19328/344985]
loss: 4.181320  [25728/344985]
loss: 4.558315  [32128/344985]
loss: 4.184894  [38528/344985]
loss: 4.324472  [44928/344985]
loss: 4.699767  [51328/344985]
loss: 4.368091  [57728/344985]
loss: 4.699560  [64128/344985]
loss: 4.365969  [70528/344985]
loss: 4.321086  [76928/344985]
loss: 4.139418  [83328/344985]
loss: 4.444070  [89728/344985]
loss: 4.307778  [96128/344985]
loss: 4.513709  [102528/344985]
loss: 4.256343  [108928/344985]
loss: 4.295433  [115328/344985]
loss: 4.240626  [121728/344985]
loss: 4.093268  [128128/344985]
loss: 4.241187  [134528/344985]
loss: 4.317170  [140928/344985]
loss: 4.258934  [147328/344985]
loss: 4.079679  [153728/344985]
loss: 4.200002  [160128/344985]
loss: 4.344043  [166528/344985]
loss: 4.377796  [172928/344985]
loss: 4.135399  [179328/344985]
loss: 4.273817  [185728/344985]
loss: 4.207451  [192128/344985]
loss: 4.406521  [198528/344985]
loss: 4.359755  [204928/344985]
loss: 4.501326  [211328/344985]
loss: 4.259919  [217728/344985]
loss: 4.485567  [224128/344985]
loss: 4.211434  [230528/344985]
loss: 4.226737  [236928/344985]
loss: 4.322909  [243328/344985]
loss: 4.384511  [249728/344985]
loss: 4.449806  [256128/344985]
loss: 4.138515  [262528/344985]
loss: 4.216850  [268928/344985]
loss: 4.188737  [275328/344985]
loss: 4.012061  [281728/344985]
loss: 4.154878  [288128/344985]
loss: 4.054865  [294528/344985]
loss: 4.186555  [300928/344985]
loss: 4.271613  [307328/344985]
loss: 4.256349  [313728/344985]
loss: 4.277157  [320128/344985]
loss: 4.489914  [326528/344985]
loss: 4.360292  [332928/344985]
loss: 4.346059  [339328/344985]

epoch avg train loss: 4.3146856   epoch avg train accuracy: 0.1071235

-------------------------------

Epoch 4

loss: 4.077672  [  128/344985]
loss: 4.250693  [ 6528/344985]
loss: 4.238239  [12928/344985]
loss: 4.196947  [19328/344985]
loss: 3.963647  [25728/344985]
loss: 4.061293  [32128/344985]
loss: 4.096374  [38528/344985]
loss: 4.216099  [44928/344985]
loss: 4.005270  [51328/344985]
loss: 4.006721  [57728/344985]
loss: 4.314235  [64128/344985]
loss: 4.097626  [70528/344985]
loss: 4.010123  [76928/344985]
loss: 3.986227  [83328/344985]
loss: 4.131355  [89728/344985]
loss: 4.219604  [96128/344985]
loss: 4.309731  [102528/344985]
loss: 3.738601  [108928/344985]
loss: 4.072513  [115328/344985]
loss: 4.041278  [121728/344985]
loss: 4.359299  [128128/344985]
loss: 4.300837  [134528/344985]
loss: 4.075374  [140928/344985]
loss: 4.164621  [147328/344985]
loss: 3.875444  [153728/344985]
loss: 4.101121  [160128/344985]
loss: 4.098660  [166528/344985]
loss: 4.150218  [172928/344985]
loss: 4.097699  [179328/344985]
loss: 3.987219  [185728/344985]
loss: 4.071289  [192128/344985]
loss: 4.316766  [198528/344985]
loss: 4.322501  [204928/344985]
loss: 4.049725  [211328/344985]
loss: 4.077488  [217728/344985]
loss: 4.031146  [224128/344985]
loss: 3.894666  [230528/344985]
loss: 3.938903  [236928/344985]
loss: 3.878426  [243328/344985]
loss: 4.195257  [249728/344985]
loss: 4.004469  [256128/344985]
loss: 4.255769  [262528/344985]
loss: 3.959036  [268928/344985]
loss: 3.840638  [275328/344985]
loss: 4.075966  [281728/344985]
loss: 4.221015  [288128/344985]
loss: 4.096932  [294528/344985]
loss: 3.901438  [300928/344985]
loss: 4.151570  [307328/344985]
loss: 4.077477  [313728/344985]
loss: 4.404706  [320128/344985]
loss: 4.339890  [326528/344985]
loss: 4.214825  [332928/344985]
loss: 4.355017  [339328/344985]

epoch avg train loss: 4.1352715   epoch avg train accuracy: 0.1304520

-------------------------------

Epoch 5

loss: 4.176956  [  128/344985]
loss: 4.072744  [ 6528/344985]
loss: 4.013732  [12928/344985]
loss: 4.246334  [19328/344985]
loss: 4.061798  [25728/344985]
loss: 3.942204  [32128/344985]
loss: 3.902909  [38528/344985]
loss: 3.906647  [44928/344985]
loss: 3.969391  [51328/344985]
loss: 3.836139  [57728/344985]
loss: 3.952612  [64128/344985]
loss: 3.877461  [70528/344985]
loss: 3.959507  [76928/344985]
loss: 3.969764  [83328/344985]
loss: 3.950891  [89728/344985]
loss: 4.053809  [96128/344985]
loss: 4.132203  [102528/344985]
loss: 4.011034  [108928/344985]
loss: 4.211908  [115328/344985]
loss: 3.952540  [121728/344985]
loss: 3.886802  [128128/344985]
loss: 4.179864  [134528/344985]
loss: 4.003933  [140928/344985]
loss: 3.882656  [147328/344985]
loss: 3.972280  [153728/344985]
loss: 3.848343  [160128/344985]
loss: 4.017220  [166528/344985]
loss: 4.022844  [172928/344985]
loss: 4.141805  [179328/344985]
loss: 4.004358  [185728/344985]
loss: 3.903623  [192128/344985]
loss: 3.821782  [198528/344985]
loss: 4.248119  [204928/344985]
loss: 3.963377  [211328/344985]
loss: 3.851028  [217728/344985]
loss: 4.001188  [224128/344985]
loss: 3.823393  [230528/344985]
loss: 3.960269  [236928/344985]
loss: 4.151707  [243328/344985]
loss: 4.304288  [249728/344985]
loss: 4.036859  [256128/344985]
loss: 3.704221  [262528/344985]
loss: 3.830372  [268928/344985]
loss: 3.632180  [275328/344985]
loss: 3.999452  [281728/344985]
loss: 3.612819  [288128/344985]
loss: 4.096278  [294528/344985]
loss: 3.614427  [300928/344985]
loss: 3.881884  [307328/344985]
loss: 3.617656  [313728/344985]
loss: 3.896617  [320128/344985]
loss: 3.946130  [326528/344985]
loss: 4.051219  [332928/344985]
loss: 3.732857  [339328/344985]

epoch avg train loss: 3.9786374   epoch avg train accuracy: 0.1522443

-------------------------------

Epoch 6

loss: 3.931073  [  128/344985]
loss: 3.762221  [ 6528/344985]
loss: 3.828691  [12928/344985]
loss: 3.862218  [19328/344985]
loss: 3.851304  [25728/344985]
loss: 3.960490  [32128/344985]
loss: 3.736080  [38528/344985]
loss: 3.693358  [44928/344985]
loss: 3.691181  [51328/344985]
loss: 3.781993  [57728/344985]
loss: 3.758998  [64128/344985]
loss: 3.779171  [70528/344985]
loss: 3.825440  [76928/344985]
loss: 3.682999  [83328/344985]
loss: 3.621099  [89728/344985]
loss: 3.828746  [96128/344985]
loss: 4.103544  [102528/344985]
loss: 3.708179  [108928/344985]
loss: 3.853778  [115328/344985]
loss: 3.689488  [121728/344985]
loss: 3.957922  [128128/344985]
loss: 3.821610  [134528/344985]
loss: 3.767955  [140928/344985]
loss: 4.002074  [147328/344985]
loss: 4.082979  [153728/344985]
loss: 4.227849  [160128/344985]
loss: 3.787989  [166528/344985]
loss: 3.734570  [172928/344985]
loss: 3.868792  [179328/344985]
loss: 3.900582  [185728/344985]
loss: 3.625561  [192128/344985]
loss: 3.719156  [198528/344985]
loss: 3.835927  [204928/344985]
loss: 3.352778  [211328/344985]
loss: 3.958044  [217728/344985]
loss: 3.733147  [224128/344985]
loss: 3.876241  [230528/344985]
loss: 3.893932  [236928/344985]
loss: 3.936387  [243328/344985]
loss: 3.850822  [249728/344985]
loss: 3.705161  [256128/344985]
loss: 3.969660  [262528/344985]
loss: 3.624158  [268928/344985]
loss: 3.648791  [275328/344985]
loss: 3.710861  [281728/344985]
loss: 3.795891  [288128/344985]
loss: 3.640843  [294528/344985]
loss: 3.888963  [300928/344985]
loss: 4.045846  [307328/344985]
loss: 3.587162  [313728/344985]
loss: 3.481268  [320128/344985]
loss: 3.745429  [326528/344985]
loss: 3.801677  [332928/344985]
loss: 3.652835  [339328/344985]

epoch avg train loss: 3.7903877   epoch avg train accuracy: 0.1808137

-------------------------------

Epoch 7

loss: 3.590078  [  128/344985]
loss: 3.572213  [ 6528/344985]
loss: 3.665320  [12928/344985]
loss: 3.336644  [19328/344985]
loss: 3.562126  [25728/344985]
loss: 3.722202  [32128/344985]
loss: 3.697684  [38528/344985]
loss: 3.589679  [44928/344985]
loss: 3.697078  [51328/344985]
loss: 3.701520  [57728/344985]
loss: 3.663400  [64128/344985]
loss: 3.685897  [70528/344985]
loss: 3.817694  [76928/344985]
loss: 3.601163  [83328/344985]
loss: 3.513681  [89728/344985]
loss: 3.419532  [96128/344985]
loss: 3.615885  [102528/344985]
loss: 3.610953  [108928/344985]
loss: 3.817992  [115328/344985]
loss: 3.640526  [121728/344985]
loss: 3.511541  [128128/344985]
loss: 3.782444  [134528/344985]
loss: 3.723171  [140928/344985]
loss: 3.889778  [147328/344985]
loss: 4.069145  [153728/344985]
loss: 3.463084  [160128/344985]
loss: 3.612804  [166528/344985]
loss: 3.769544  [172928/344985]
loss: 3.626356  [179328/344985]
loss: 3.723146  [185728/344985]
loss: 3.727747  [192128/344985]
loss: 3.493301  [198528/344985]
loss: 3.965810  [204928/344985]
loss: 3.580633  [211328/344985]
loss: 3.615930  [217728/344985]
loss: 3.688275  [224128/344985]
loss: 3.872890  [230528/344985]
loss: 3.691470  [236928/344985]
loss: 3.377503  [243328/344985]
loss: 3.859896  [249728/344985]
loss: 3.763552  [256128/344985]
loss: 3.715851  [262528/344985]
loss: 3.744177  [268928/344985]
loss: 3.833441  [275328/344985]
loss: 3.660656  [281728/344985]
loss: 3.750661  [288128/344985]
loss: 3.692607  [294528/344985]
loss: 3.903757  [300928/344985]
loss: 3.445521  [307328/344985]
loss: 3.903956  [313728/344985]
loss: 3.510655  [320128/344985]
loss: 3.595385  [326528/344985]
loss: 3.629174  [332928/344985]
loss: 3.657968  [339328/344985]

epoch avg train loss: 3.6333829   epoch avg train accuracy: 0.2057539

-------------------------------

Epoch 8

loss: 3.726000  [  128/344985]
loss: 3.387719  [ 6528/344985]
loss: 3.768297  [12928/344985]
loss: 3.586905  [19328/344985]
loss: 3.556093  [25728/344985]
loss: 3.437554  [32128/344985]
loss: 3.527632  [38528/344985]
loss: 3.173938  [44928/344985]
loss: 3.442774  [51328/344985]
loss: 3.430959  [57728/344985]
loss: 3.750571  [64128/344985]
loss: 3.330331  [70528/344985]
loss: 3.881137  [76928/344985]
loss: 3.469449  [83328/344985]
loss: 3.691973  [89728/344985]
loss: 3.494436  [96128/344985]
loss: 3.437260  [102528/344985]
loss: 3.698829  [108928/344985]
loss: 3.165152  [115328/344985]
loss: 3.654995  [121728/344985]
loss: 3.332921  [128128/344985]
loss: 3.481142  [134528/344985]
loss: 3.733839  [140928/344985]
loss: 3.486897  [147328/344985]
loss: 3.522652  [153728/344985]
loss: 3.360562  [160128/344985]
loss: 3.627162  [166528/344985]
loss: 3.516723  [172928/344985]
loss: 3.636950  [179328/344985]
loss: 3.298992  [185728/344985]
loss: 3.375060  [192128/344985]
loss: 3.459638  [198528/344985]
loss: 3.998678  [204928/344985]
loss: 3.753521  [211328/344985]
loss: 3.524430  [217728/344985]
loss: 3.313139  [224128/344985]
loss: 3.421553  [230528/344985]
loss: 3.262199  [236928/344985]
loss: 3.594922  [243328/344985]
loss: 3.338088  [249728/344985]
loss: 3.280079  [256128/344985]
loss: 3.411145  [262528/344985]
loss: 3.430620  [268928/344985]
loss: 3.518918  [275328/344985]
loss: 3.301264  [281728/344985]
loss: 3.589467  [288128/344985]
loss: 3.399052  [294528/344985]
loss: 3.559118  [300928/344985]
loss: 3.496754  [307328/344985]
loss: 3.306424  [313728/344985]
loss: 3.248534  [320128/344985]
loss: 3.257128  [326528/344985]
loss: 3.408222  [332928/344985]
loss: 3.576122  [339328/344985]

epoch avg train loss: 3.5057217   epoch avg train accuracy: 0.2258678

-------------------------------

Epoch 9

loss: 3.370793  [  128/344985]
loss: 3.713328  [ 6528/344985]
loss: 3.355578  [12928/344985]
loss: 3.334132  [19328/344985]
loss: 3.547869  [25728/344985]
loss: 2.806617  [32128/344985]
loss: 3.564800  [38528/344985]
loss: 3.228729  [44928/344985]
loss: 3.730791  [51328/344985]
loss: 3.076179  [57728/344985]
loss: 3.440069  [64128/344985]
loss: 3.334909  [70528/344985]
loss: 3.695921  [76928/344985]
loss: 3.370273  [83328/344985]
loss: 3.534346  [89728/344985]
loss: 3.445765  [96128/344985]
loss: 3.354779  [102528/344985]
loss: 3.614741  [108928/344985]
loss: 3.547264  [115328/344985]
loss: 3.647874  [121728/344985]
loss: 3.244300  [128128/344985]
loss: 3.605658  [134528/344985]
loss: 3.134181  [140928/344985]
loss: 2.928043  [147328/344985]
loss: 3.642447  [153728/344985]
loss: 3.499041  [160128/344985]
loss: 3.311885  [166528/344985]
loss: 3.517044  [172928/344985]
loss: 3.498700  [179328/344985]
loss: 3.037133  [185728/344985]
loss: 3.313730  [192128/344985]
loss: 3.454356  [198528/344985]
loss: 3.311634  [204928/344985]
loss: 3.649298  [211328/344985]
loss: 3.432064  [217728/344985]
loss: 3.168830  [224128/344985]
loss: 3.432693  [230528/344985]
loss: 3.519828  [236928/344985]
loss: 3.149677  [243328/344985]
loss: 3.336243  [249728/344985]
loss: 3.299801  [256128/344985]
loss: 3.532065  [262528/344985]
loss: 3.398951  [268928/344985]
loss: 3.548748  [275328/344985]
loss: 3.513448  [281728/344985]
loss: 3.455594  [288128/344985]
loss: 3.257866  [294528/344985]
loss: 3.199710  [300928/344985]
loss: 3.324616  [307328/344985]
loss: 3.194064  [313728/344985]
loss: 3.742613  [320128/344985]
loss: 3.449373  [326528/344985]
loss: 3.499596  [332928/344985]
loss: 3.208860  [339328/344985]

epoch avg train loss: 3.3977601   epoch avg train accuracy: 0.2436222

-------------------------------

Epoch 10

loss: 3.465356  [  128/344985]
loss: 3.206644  [ 6528/344985]
loss: 3.229674  [12928/344985]
loss: 3.384523  [19328/344985]
loss: 3.315791  [25728/344985]
loss: 3.189079  [32128/344985]
loss: 3.332249  [38528/344985]
loss: 3.101106  [44928/344985]
loss: 3.150126  [51328/344985]
loss: 3.111345  [57728/344985]
loss: 3.575282  [64128/344985]
loss: 3.514876  [70528/344985]
loss: 3.485885  [76928/344985]
loss: 3.463689  [83328/344985]
loss: 3.504298  [89728/344985]
loss: 3.455184  [96128/344985]
loss: 3.418589  [102528/344985]
loss: 3.418320  [108928/344985]
loss: 3.350334  [115328/344985]
loss: 3.705261  [121728/344985]
loss: 3.473488  [128128/344985]
loss: 3.126332  [134528/344985]
loss: 3.339599  [140928/344985]
loss: 3.170680  [147328/344985]
loss: 3.092237  [153728/344985]
loss: 3.433354  [160128/344985]
loss: 3.332400  [166528/344985]
loss: 3.486129  [172928/344985]
loss: 3.229043  [179328/344985]
loss: 3.258490  [185728/344985]
loss: 3.644273  [192128/344985]
loss: 3.438464  [198528/344985]
loss: 3.189321  [204928/344985]
loss: 3.405242  [211328/344985]
loss: 3.436137  [217728/344985]
loss: 3.426563  [224128/344985]
loss: 3.287596  [230528/344985]
loss: 3.603842  [236928/344985]
loss: 3.083281  [243328/344985]
loss: 3.075993  [249728/344985]
loss: 3.449604  [256128/344985]
loss: 3.090741  [262528/344985]
loss: 3.169105  [268928/344985]
loss: 3.218596  [275328/344985]
loss: 3.198118  [281728/344985]
loss: 3.720358  [288128/344985]
loss: 3.295806  [294528/344985]
loss: 3.236387  [300928/344985]
loss: 3.417286  [307328/344985]
loss: 3.155700  [313728/344985]
loss: 3.479655  [320128/344985]
loss: 3.320208  [326528/344985]
loss: 3.071517  [332928/344985]
loss: 3.683385  [339328/344985]

epoch avg train loss: 3.2970697   epoch avg train accuracy: 0.2607331

-------------------------------

Evaluating against random transformations...
Mean acc: 0.2262
Acc std: 0.0010128
