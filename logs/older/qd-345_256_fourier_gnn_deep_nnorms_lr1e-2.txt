Epoch 1

loss: 5.843552  [  128/344985]
loss: 6.987719  [ 6528/344985]
loss: 5.855157  [12928/344985]
loss: 5.847886  [19328/344985]
loss: 5.847615  [25728/344985]
loss: 5.840534  [32128/344985]
loss: 5.846298  [38528/344985]
loss: 5.849339  [44928/344985]
loss: 5.849308  [51328/344985]
loss: 5.831928  [57728/344985]
loss: 5.853006  [64128/344985]
loss: 5.848782  [70528/344985]
loss: 5.857107  [76928/344985]
loss: 5.852275  [83328/344985]
loss: 5.844160  [89728/344985]
loss: 5.840829  [96128/344985]
loss: 5.851168  [102528/344985]
loss: 5.834322  [108928/344985]
loss: 5.850815  [115328/344985]
loss: 5.843349  [121728/344985]
loss: 5.857874  [128128/344985]
loss: 5.842138  [134528/344985]
loss: 5.847528  [140928/344985]
loss: 5.842508  [147328/344985]
loss: 5.834149  [153728/344985]
loss: 5.838342  [160128/344985]
loss: 5.855490  [166528/344985]
loss: 5.853418  [172928/344985]
loss: 5.854305  [179328/344985]
loss: 5.848135  [185728/344985]
loss: 5.843719  [192128/344985]
loss: 5.850483  [198528/344985]
loss: 5.856216  [204928/344985]
loss: 5.847827  [211328/344985]
loss: 5.851619  [217728/344985]
loss: 5.849329  [224128/344985]
loss: 5.860233  [230528/344985]
loss: 5.848011  [236928/344985]
loss: 5.842127  [243328/344985]
loss: 5.848899  [249728/344985]
loss: 5.842253  [256128/344985]
loss: 5.844317  [262528/344985]
loss: 5.845218  [268928/344985]
loss: 5.855334  [275328/344985]
loss: 5.844359  [281728/344985]
loss: 5.837579  [288128/344985]
loss: 5.851904  [294528/344985]
loss: 5.846756  [300928/344985]
loss: 5.825126  [307328/344985]
loss: 5.847714  [313728/344985]
loss: 5.843073  [320128/344985]
loss: 5.839253  [326528/344985]
loss: 5.848254  [332928/344985]
loss: 5.838505  [339328/344985]

epoch avg train loss: 14.4925049   epoch avg train accuracy: 0.0027277

-------------------------------

Epoch 2

loss: 5.829450  [  128/344985]
loss: 5.852103  [ 6528/344985]
loss: 5.862766  [12928/344985]
loss: 5.847267  [19328/344985]
loss: 5.848976  [25728/344985]
loss: 5.842451  [32128/344985]
loss: 5.840739  [38528/344985]
loss: 5.846264  [44928/344985]
loss: 5.841372  [51328/344985]
loss: 5.856163  [57728/344985]
loss: 5.859303  [64128/344985]
loss: 5.849797  [70528/344985]
loss: 5.842009  [76928/344985]
loss: 5.842986  [83328/344985]
loss: 5.850511  [89728/344985]
loss: 5.848681  [96128/344985]
loss: 5.843508  [102528/344985]
loss: 5.848300  [108928/344985]
loss: 5.864212  [115328/344985]
loss: 5.839375  [121728/344985]
loss: 5.852923  [128128/344985]
loss: 5.832239  [134528/344985]
loss: 5.846948  [140928/344985]
loss: 5.838705  [147328/344985]
loss: 5.856566  [153728/344985]
loss: 5.853050  [160128/344985]
loss: 5.838160  [166528/344985]
loss: 5.856284  [172928/344985]
loss: 5.839325  [179328/344985]
loss: 5.844063  [185728/344985]
loss: 5.842346  [192128/344985]
loss: 5.840691  [198528/344985]
loss: 5.851075  [204928/344985]
loss: 5.850128  [211328/344985]
loss: 5.853136  [217728/344985]
loss: 5.846591  [224128/344985]
loss: 5.851646  [230528/344985]
loss: 5.846422  [236928/344985]
loss: 5.867830  [243328/344985]
loss: 5.837978  [249728/344985]
loss: 5.838575  [256128/344985]
loss: 5.864873  [262528/344985]
loss: 5.844131  [268928/344985]
loss: 5.865450  [275328/344985]
loss: 5.859249  [281728/344985]
loss: 5.832024  [288128/344985]
loss: 5.839135  [294528/344985]
loss: 5.852413  [300928/344985]
loss: 5.842330  [307328/344985]
loss: 5.852645  [313728/344985]
loss: 5.837985  [320128/344985]
loss: 5.848945  [326528/344985]
loss: 5.841315  [332928/344985]
loss: 5.849261  [339328/344985]

epoch avg train loss: 5.8481376   epoch avg train accuracy: 0.0027277

-------------------------------

Epoch 3

loss: 5.847460  [  128/344985]
loss: 5.839816  [ 6528/344985]
loss: 5.849162  [12928/344985]
loss: 5.860769  [19328/344985]
loss: 5.847969  [25728/344985]
loss: 5.837403  [32128/344985]
loss: 5.837780  [38528/344985]
loss: 5.838821  [44928/344985]
loss: 5.853371  [51328/344985]
loss: 5.853387  [57728/344985]
loss: 5.851122  [64128/344985]
loss: 5.835353  [70528/344985]
loss: 5.844196  [76928/344985]
loss: 5.848045  [83328/344985]
loss: 5.851038  [89728/344985]
loss: 5.841819  [96128/344985]
loss: 5.853465  [102528/344985]
loss: 5.846935  [108928/344985]
loss: 5.840610  [115328/344985]
loss: 5.840186  [121728/344985]
loss: 5.844728  [128128/344985]
loss: 5.856105  [134528/344985]
loss: 5.851045  [140928/344985]
loss: 5.849444  [147328/344985]
loss: 5.848286  [153728/344985]
loss: 5.844682  [160128/344985]
loss: 5.842726  [166528/344985]
loss: 5.852553  [172928/344985]
loss: 5.828444  [179328/344985]
loss: 5.845098  [185728/344985]
loss: 5.857000  [192128/344985]
loss: 5.850667  [198528/344985]
loss: 5.856399  [204928/344985]
loss: 5.835131  [211328/344985]
loss: 5.846686  [217728/344985]
loss: 5.859550  [224128/344985]
loss: 5.861104  [230528/344985]
loss: 5.848807  [236928/344985]
loss: 5.846635  [243328/344985]
loss: 5.848952  [249728/344985]
loss: 5.846624  [256128/344985]
loss: 5.841544  [262528/344985]
loss: 5.848249  [268928/344985]
loss: 5.861856  [275328/344985]
loss: 5.843409  [281728/344985]
loss: 5.854105  [288128/344985]
loss: 5.848808  [294528/344985]
loss: 5.841251  [300928/344985]
loss: 5.849425  [307328/344985]
loss: 5.853842  [313728/344985]
loss: 5.853389  [320128/344985]
loss: 5.840963  [326528/344985]
loss: 5.859494  [332928/344985]
loss: 5.853202  [339328/344985]

epoch avg train loss: 5.8480664   epoch avg train accuracy: 0.0028842

-------------------------------

Epoch 4

loss: 5.852621  [  128/344985]
loss: 5.842929  [ 6528/344985]
loss: 5.856290  [12928/344985]
loss: 5.855863  [19328/344985]
loss: 5.852626  [25728/344985]
loss: 5.840151  [32128/344985]
loss: 5.842691  [38528/344985]
loss: 5.846323  [44928/344985]
loss: 5.848750  [51328/344985]
loss: 5.843731  [57728/344985]
loss: 5.855560  [64128/344985]
loss: 5.843474  [70528/344985]
loss: 5.830398  [76928/344985]
loss: 5.850007  [83328/344985]
loss: 5.847031  [89728/344985]
loss: 5.835489  [96128/344985]
loss: 5.845362  [102528/344985]
loss: 5.844614  [108928/344985]
loss: 5.854267  [115328/344985]
loss: 5.846679  [121728/344985]
loss: 5.861179  [128128/344985]
loss: 5.858609  [134528/344985]
loss: 5.861118  [140928/344985]
loss: 5.863321  [147328/344985]
loss: 5.840847  [153728/344985]
loss: 5.842526  [160128/344985]
loss: 5.842828  [166528/344985]
loss: 5.849888  [172928/344985]
loss: 5.841144  [179328/344985]
loss: 5.845106  [185728/344985]
loss: 5.847909  [192128/344985]
loss: 5.849825  [198528/344985]
loss: 5.859756  [204928/344985]
loss: 5.857700  [211328/344985]
loss: 5.846259  [217728/344985]
loss: 5.848367  [224128/344985]
loss: 5.835546  [230528/344985]
loss: 5.844507  [236928/344985]
loss: 5.842557  [243328/344985]
loss: 5.856194  [249728/344985]
loss: 5.851430  [256128/344985]
loss: 5.858901  [262528/344985]
loss: 5.851306  [268928/344985]
loss: 5.848536  [275328/344985]
loss: 5.840609  [281728/344985]
loss: 5.851660  [288128/344985]
loss: 5.861656  [294528/344985]
loss: 5.840198  [300928/344985]
loss: 5.861507  [307328/344985]
loss: 5.837748  [313728/344985]
loss: 5.841158  [320128/344985]
loss: 5.855622  [326528/344985]
loss: 5.854016  [332928/344985]
loss: 5.857903  [339328/344985]

epoch avg train loss: 5.8480137   epoch avg train accuracy: 0.0029538

-------------------------------

Epoch 5

loss: 5.846693  [  128/344985]
loss: 5.848332  [ 6528/344985]
loss: 5.845844  [12928/344985]
loss: 5.852958  [19328/344985]
loss: 5.849004  [25728/344985]
loss: 5.852730  [32128/344985]
loss: 5.855211  [38528/344985]
loss: 5.847431  [44928/344985]
loss: 5.841959  [51328/344985]
loss: 5.847698  [57728/344985]
loss: 5.846197  [64128/344985]
loss: 5.863102  [70528/344985]
loss: 5.848812  [76928/344985]
loss: 5.852835  [83328/344985]
loss: 5.837881  [89728/344985]
loss: 5.838523  [96128/344985]
loss: 5.856508  [102528/344985]
loss: 5.848608  [108928/344985]
loss: 5.852369  [115328/344985]
loss: 5.851668  [121728/344985]
loss: 5.843398  [128128/344985]
loss: 5.858189  [134528/344985]
loss: 5.841000  [140928/344985]
loss: 5.837403  [147328/344985]
loss: 5.833173  [153728/344985]
loss: 5.849319  [160128/344985]
loss: 5.849047  [166528/344985]
loss: 5.850042  [172928/344985]
loss: 5.860815  [179328/344985]
loss: 5.847573  [185728/344985]
loss: 5.845346  [192128/344985]
loss: 5.845301  [198528/344985]
loss: 5.846086  [204928/344985]
loss: 5.850621  [211328/344985]
loss: 5.855390  [217728/344985]
loss: 5.838845  [224128/344985]
loss: 5.856150  [230528/344985]
loss: 5.848828  [236928/344985]
loss: 5.854039  [243328/344985]
loss: 5.839200  [249728/344985]
loss: 5.840026  [256128/344985]
loss: 5.847022  [262528/344985]
loss: 5.839768  [268928/344985]
loss: 5.849184  [275328/344985]
loss: 5.857523  [281728/344985]
loss: 5.843376  [288128/344985]
loss: 5.847493  [294528/344985]
loss: 5.862540  [300928/344985]
loss: 5.849880  [307328/344985]
loss: 5.851928  [313728/344985]
loss: 5.847433  [320128/344985]
loss: 5.851034  [326528/344985]
loss: 5.841726  [332928/344985]
loss: 5.851003  [339328/344985]

epoch avg train loss: 5.8481157   epoch avg train accuracy: 0.0028349

-------------------------------

Epoch 6

loss: 5.852731  [  128/344985]
loss: 5.865243  [ 6528/344985]
loss: 5.844559  [12928/344985]
loss: 5.834932  [19328/344985]
loss: 5.850791  [25728/344985]
loss: 5.837951  [32128/344985]
loss: 5.842719  [38528/344985]
loss: 5.857612  [44928/344985]
loss: 5.855695  [51328/344985]
loss: 5.843351  [57728/344985]
loss: 5.856038  [64128/344985]
loss: 5.850047  [70528/344985]
loss: 5.851291  [76928/344985]
loss: 5.857652  [83328/344985]
loss: 5.851629  [89728/344985]
loss: 5.853018  [96128/344985]
loss: 5.852760  [102528/344985]
loss: 5.842551  [108928/344985]
loss: 5.838811  [115328/344985]
loss: 5.838221  [121728/344985]
loss: 5.850691  [128128/344985]
loss: 5.860529  [134528/344985]
loss: 5.861495  [140928/344985]
loss: 5.850475  [147328/344985]
loss: 5.856490  [153728/344985]
loss: 5.844962  [160128/344985]
loss: 5.856646  [166528/344985]
loss: 5.844179  [172928/344985]
loss: 5.844714  [179328/344985]
loss: 5.848531  [185728/344985]
loss: 5.848730  [192128/344985]
loss: 5.852248  [198528/344985]
loss: 5.864382  [204928/344985]
loss: 5.840032  [211328/344985]
loss: 5.851587  [217728/344985]
loss: 5.850760  [224128/344985]
loss: 5.845279  [230528/344985]
loss: 5.853803  [236928/344985]
loss: 5.849893  [243328/344985]
loss: 5.850135  [249728/344985]
loss: 5.849633  [256128/344985]
loss: 5.854891  [262528/344985]
loss: 5.844391  [268928/344985]
loss: 5.844593  [275328/344985]
loss: 5.832988  [281728/344985]
loss: 5.850489  [288128/344985]
loss: 5.835491  [294528/344985]
loss: 5.842790  [300928/344985]
loss: 5.846814  [307328/344985]
loss: 5.855933  [313728/344985]
loss: 5.844976  [320128/344985]
loss: 5.853322  [326528/344985]
loss: 5.831746  [332928/344985]
loss: 5.851816  [339328/344985]

epoch avg train loss: 5.8480213   epoch avg train accuracy: 0.0028958

-------------------------------

Epoch 7

loss: 5.837816  [  128/344985]
loss: 5.832118  [ 6528/344985]
loss: 5.851784  [12928/344985]
loss: 5.838134  [19328/344985]
loss: 5.855525  [25728/344985]
loss: 5.844265  [32128/344985]
loss: 5.838538  [38528/344985]
loss: 5.844053  [44928/344985]
loss: 5.851840  [51328/344985]
loss: 5.843570  [57728/344985]
loss: 5.846699  [64128/344985]
loss: 5.861082  [70528/344985]
loss: 5.850560  [76928/344985]
loss: 5.843902  [83328/344985]
loss: 5.836735  [89728/344985]
loss: 5.845542  [96128/344985]
loss: 5.844416  [102528/344985]
loss: 5.840970  [108928/344985]
loss: 5.858230  [115328/344985]
loss: 5.844756  [121728/344985]
loss: 5.856738  [128128/344985]
loss: 5.849810  [134528/344985]
loss: 5.859428  [140928/344985]
loss: 5.844853  [147328/344985]
loss: 5.860306  [153728/344985]
loss: 5.847804  [160128/344985]
loss: 5.861173  [166528/344985]
loss: 5.853673  [172928/344985]
loss: 5.849901  [179328/344985]
loss: 5.846344  [185728/344985]
loss: 5.847427  [192128/344985]
loss: 5.845272  [198528/344985]
loss: 5.853783  [204928/344985]
loss: 5.846514  [211328/344985]
loss: 5.841015  [217728/344985]
loss: 5.848155  [224128/344985]
loss: 5.850794  [230528/344985]
loss: 5.840010  [236928/344985]
loss: 5.851550  [243328/344985]
loss: 5.852148  [249728/344985]
loss: 5.847214  [256128/344985]
loss: 5.838082  [262528/344985]
loss: 5.850408  [268928/344985]
loss: 5.839754  [275328/344985]
loss: 5.855618  [281728/344985]
loss: 5.857183  [288128/344985]
loss: 5.851827  [294528/344985]
loss: 5.832869  [300928/344985]
loss: 5.851658  [307328/344985]
loss: 5.853508  [313728/344985]
loss: 5.836325  [320128/344985]
loss: 5.851567  [326528/344985]
loss: 5.846930  [332928/344985]
loss: 5.852208  [339328/344985]

epoch avg train loss: 5.8481449   epoch avg train accuracy: 0.0029190

-------------------------------

Epoch 8

loss: 5.853933  [  128/344985]
loss: 5.849575  [ 6528/344985]
loss: 5.837257  [12928/344985]
loss: 5.859614  [19328/344985]
loss: 5.852533  [25728/344985]
loss: 5.828394  [32128/344985]
loss: 5.843383  [38528/344985]
loss: 5.838523  [44928/344985]
loss: 5.860207  [51328/344985]
loss: 5.843845  [57728/344985]
loss: 5.842190  [64128/344985]
loss: 5.843660  [70528/344985]
loss: 5.843582  [76928/344985]
loss: 5.848164  [83328/344985]
loss: 5.844234  [89728/344985]
loss: 5.840179  [96128/344985]
loss: 5.853199  [102528/344985]
loss: 5.839925  [108928/344985]
loss: 5.850917  [115328/344985]
loss: 5.844841  [121728/344985]
loss: 5.847961  [128128/344985]
loss: 5.836704  [134528/344985]
loss: 5.846710  [140928/344985]
loss: 5.845722  [147328/344985]
loss: 5.851408  [153728/344985]
loss: 5.848073  [160128/344985]
loss: 5.844258  [166528/344985]
loss: 5.852553  [172928/344985]
loss: 5.828724  [179328/344985]
loss: 5.852340  [185728/344985]
loss: 5.836161  [192128/344985]
loss: 5.850149  [198528/344985]
loss: 5.858285  [204928/344985]
loss: 5.839994  [211328/344985]
loss: 5.849386  [217728/344985]
loss: 5.852662  [224128/344985]
loss: 5.850071  [230528/344985]
loss: 5.853262  [236928/344985]
loss: 5.854228  [243328/344985]
loss: 5.866551  [249728/344985]
loss: 5.851861  [256128/344985]
loss: 5.845991  [262528/344985]
loss: 5.848669  [268928/344985]
loss: 5.837187  [275328/344985]
loss: 5.856505  [281728/344985]
loss: 5.850570  [288128/344985]
loss: 5.860942  [294528/344985]
loss: 5.848603  [300928/344985]
loss: 5.862148  [307328/344985]
loss: 5.843692  [313728/344985]
loss: 5.854744  [320128/344985]
loss: 5.849784  [326528/344985]
loss: 5.857809  [332928/344985]
loss: 5.853121  [339328/344985]

epoch avg train loss: 5.8480762   epoch avg train accuracy: 0.0027566

-------------------------------

Epoch 9

loss: 5.857919  [  128/344985]
loss: 5.847023  [ 6528/344985]
loss: 5.848797  [12928/344985]
loss: 5.856576  [19328/344985]
loss: 5.846192  [25728/344985]
loss: 5.848755  [32128/344985]
loss: 5.846703  [38528/344985]
loss: 5.846067  [44928/344985]
loss: 5.847766  [51328/344985]
loss: 5.830832  [57728/344985]
loss: 5.851068  [64128/344985]
loss: 5.852328  [70528/344985]
loss: 5.849311  [76928/344985]
loss: 5.849838  [83328/344985]
loss: 5.858575  [89728/344985]
loss: 5.857225  [96128/344985]
loss: 5.864583  [102528/344985]
loss: 5.858390  [108928/344985]
loss: 5.849504  [115328/344985]
loss: 5.860318  [121728/344985]
loss: 5.851015  [128128/344985]
loss: 5.857631  [134528/344985]
loss: 5.858588  [140928/344985]
loss: 5.846865  [147328/344985]
loss: 5.847938  [153728/344985]
loss: 5.851161  [160128/344985]
loss: 5.853824  [166528/344985]
loss: 5.843502  [172928/344985]
loss: 5.844054  [179328/344985]
loss: 5.843072  [185728/344985]
loss: 5.844812  [192128/344985]
loss: 5.839540  [198528/344985]
loss: 5.843152  [204928/344985]
loss: 5.854777  [211328/344985]
loss: 5.845257  [217728/344985]
loss: 5.848950  [224128/344985]
loss: 5.851213  [230528/344985]
loss: 5.836648  [236928/344985]
loss: 5.841420  [243328/344985]
loss: 5.827122  [249728/344985]
loss: 5.844790  [256128/344985]
loss: 5.850544  [262528/344985]
loss: 5.839462  [268928/344985]
loss: 5.836422  [275328/344985]
loss: 5.854813  [281728/344985]
loss: 5.844231  [288128/344985]
loss: 5.844278  [294528/344985]
loss: 5.846786  [300928/344985]
loss: 5.845327  [307328/344985]
loss: 5.843957  [313728/344985]
loss: 5.852757  [320128/344985]
loss: 5.846902  [326528/344985]
loss: 5.846400  [332928/344985]
loss: 5.855947  [339328/344985]

epoch avg train loss: 5.8481606   epoch avg train accuracy: 0.0027132

-------------------------------

Epoch 10

loss: 5.841835  [  128/344985]
loss: 5.854969  [ 6528/344985]
loss: 5.841751  [12928/344985]
loss: 5.846732  [19328/344985]
loss: 5.850141  [25728/344985]
loss: 5.846516  [32128/344985]
loss: 5.854371  [38528/344985]
loss: 5.860275  [44928/344985]
loss: 5.855589  [51328/344985]
loss: 5.846347  [57728/344985]
loss: 5.846677  [64128/344985]
loss: 5.844428  [70528/344985]
loss: 5.849943  [76928/344985]
loss: 5.864681  [83328/344985]
loss: 5.851458  [89728/344985]
loss: 5.852946  [96128/344985]
loss: 5.856255  [102528/344985]
loss: 5.848005  [108928/344985]
loss: 5.863633  [115328/344985]
loss: 5.849668  [121728/344985]
loss: 5.846014  [128128/344985]
loss: 5.850444  [134528/344985]
loss: 5.859773  [140928/344985]
loss: 5.847595  [147328/344985]
loss: 5.854756  [153728/344985]
loss: 5.853923  [160128/344985]
loss: 5.841298  [166528/344985]
loss: 5.848492  [172928/344985]
loss: 5.845674  [179328/344985]
loss: 5.842795  [185728/344985]
loss: 5.844036  [192128/344985]
loss: 5.844510  [198528/344985]
loss: 5.852755  [204928/344985]
loss: 5.851359  [211328/344985]
loss: 5.861267  [217728/344985]
loss: 5.841069  [224128/344985]
loss: 5.854749  [230528/344985]
loss: 5.843896  [236928/344985]
loss: 5.847433  [243328/344985]
loss: 5.847946  [249728/344985]
loss: 5.839964  [256128/344985]
loss: 5.843774  [262528/344985]
loss: 5.848062  [268928/344985]
loss: 5.855025  [275328/344985]
loss: 5.842939  [281728/344985]
loss: 5.838329  [288128/344985]
loss: 5.852638  [294528/344985]
loss: 5.847506  [300928/344985]
loss: 5.844806  [307328/344985]
loss: 5.856913  [313728/344985]
loss: 5.849069  [320128/344985]
loss: 5.852939  [326528/344985]
loss: 5.848473  [332928/344985]
loss: 5.846064  [339328/344985]

epoch avg train loss: 5.8480532   epoch avg train accuracy: 0.0029277

-------------------------------

Evaluating against random transformations...
Mean acc: 0.0029
Acc std: 0.0000000
