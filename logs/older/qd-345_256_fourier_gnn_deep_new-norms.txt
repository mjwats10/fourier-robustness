Epoch 1

loss: 5.843552  [  128/344985]
loss: 5.842698  [ 6528/344985]
loss: 5.782218  [12928/344985]
loss: 5.835860  [19328/344985]
loss: 5.657390  [25728/344985]
loss: 5.690861  [32128/344985]
loss: 5.746639  [38528/344985]
loss: 5.612446  [44928/344985]
loss: 5.591243  [51328/344985]
loss: 5.591531  [57728/344985]
loss: 5.543900  [64128/344985]
loss: 5.537907  [70528/344985]
loss: 5.564374  [76928/344985]
loss: 5.649124  [83328/344985]
loss: 5.468843  [89728/344985]
loss: 5.499204  [96128/344985]
loss: 5.567222  [102528/344985]
loss: 5.427632  [108928/344985]
loss: 5.572237  [115328/344985]
loss: 5.529361  [121728/344985]
loss: 5.516450  [128128/344985]
loss: 5.563085  [134528/344985]
loss: 5.417301  [140928/344985]
loss: 5.514650  [147328/344985]
loss: 5.391183  [153728/344985]
loss: 5.298218  [160128/344985]
loss: 5.302810  [166528/344985]
loss: 5.371161  [172928/344985]
loss: 5.351433  [179328/344985]
loss: 5.061514  [185728/344985]
loss: 5.341770  [192128/344985]
loss: 5.226181  [198528/344985]
loss: 5.166885  [204928/344985]
loss: 5.163021  [211328/344985]
loss: 5.228628  [217728/344985]
loss: 5.075826  [224128/344985]
loss: 5.221658  [230528/344985]
loss: 5.112192  [236928/344985]
loss: 5.260280  [243328/344985]
loss: 4.961125  [249728/344985]
loss: 4.945670  [256128/344985]
loss: 5.105836  [262528/344985]
loss: 5.224428  [268928/344985]
loss: 5.132152  [275328/344985]
loss: 5.106022  [281728/344985]
loss: 4.894728  [288128/344985]
loss: 5.098124  [294528/344985]
loss: 5.171726  [300928/344985]
loss: 5.258690  [307328/344985]
loss: 5.029026  [313728/344985]
loss: 5.072586  [320128/344985]
loss: 4.881774  [326528/344985]
loss: 4.991495  [332928/344985]
loss: 5.030592  [339328/344985]

epoch avg train loss: 5.3425457   epoch avg train accuracy: 0.0233691

-------------------------------

Epoch 2

loss: 4.910513  [  128/344985]
loss: 5.060706  [ 6528/344985]
loss: 4.961671  [12928/344985]
loss: 4.996352  [19328/344985]
loss: 4.862672  [25728/344985]
loss: 4.951604  [32128/344985]
loss: 5.038084  [38528/344985]
loss: 5.091523  [44928/344985]
loss: 4.867813  [51328/344985]
loss: 4.841390  [57728/344985]
loss: 4.982488  [64128/344985]
loss: 5.056701  [70528/344985]
loss: 4.867167  [76928/344985]
loss: 4.707172  [83328/344985]
loss: 4.811642  [89728/344985]
loss: 4.615127  [96128/344985]
loss: 4.918200  [102528/344985]
loss: 4.837914  [108928/344985]
loss: 4.739005  [115328/344985]
loss: 4.926755  [121728/344985]
loss: 4.878806  [128128/344985]
loss: 4.935254  [134528/344985]
loss: 4.938643  [140928/344985]
loss: 4.736638  [147328/344985]
loss: 4.858693  [153728/344985]
loss: 4.565034  [160128/344985]
loss: 4.793624  [166528/344985]
loss: 4.777570  [172928/344985]
loss: 4.646016  [179328/344985]
loss: 4.524818  [185728/344985]
loss: 4.810732  [192128/344985]
loss: 4.594727  [198528/344985]
loss: 4.578696  [204928/344985]
loss: 4.522465  [211328/344985]
loss: 4.400697  [217728/344985]
loss: 4.770335  [224128/344985]
loss: 4.610445  [230528/344985]
loss: 4.452279  [236928/344985]
loss: 4.950383  [243328/344985]
loss: 4.602009  [249728/344985]
loss: 4.502577  [256128/344985]
loss: 4.640569  [262528/344985]
loss: 4.668012  [268928/344985]
loss: 4.714616  [275328/344985]
loss: 4.668016  [281728/344985]
loss: 4.586358  [288128/344985]
loss: 4.611948  [294528/344985]
loss: 4.834725  [300928/344985]
loss: 4.771880  [307328/344985]
loss: 4.515695  [313728/344985]
loss: 4.495036  [320128/344985]
loss: 4.783289  [326528/344985]
loss: 4.384111  [332928/344985]
loss: 4.362160  [339328/344985]

epoch avg train loss: 4.7421118   epoch avg train accuracy: 0.0655478

-------------------------------

Epoch 3

loss: 4.795263  [  128/344985]
loss: 4.487071  [ 6528/344985]
loss: 4.724611  [12928/344985]
loss: 4.564624  [19328/344985]
loss: 4.405985  [25728/344985]
loss: 4.616421  [32128/344985]
loss: 4.335529  [38528/344985]
loss: 4.556875  [44928/344985]
loss: 4.802444  [51328/344985]
loss: 4.550381  [57728/344985]
loss: 4.797291  [64128/344985]
loss: 4.578145  [70528/344985]
loss: 4.359264  [76928/344985]
loss: 4.475614  [83328/344985]
loss: 4.632560  [89728/344985]
loss: 4.385824  [96128/344985]
loss: 4.642408  [102528/344985]
loss: 4.479261  [108928/344985]
loss: 4.353850  [115328/344985]
loss: 4.454841  [121728/344985]
loss: 4.264561  [128128/344985]
loss: 4.390620  [134528/344985]
loss: 4.399046  [140928/344985]
loss: 4.520988  [147328/344985]
loss: 4.295465  [153728/344985]
loss: 4.458619  [160128/344985]
loss: 4.579559  [166528/344985]
loss: 4.487475  [172928/344985]
loss: 4.287047  [179328/344985]
loss: 4.500006  [185728/344985]
loss: 4.347276  [192128/344985]
loss: 4.408294  [198528/344985]
loss: 4.647972  [204928/344985]
loss: 4.571996  [211328/344985]
loss: 4.472209  [217728/344985]
loss: 4.533860  [224128/344985]
loss: 4.526179  [230528/344985]
loss: 4.415096  [236928/344985]
loss: 4.389401  [243328/344985]
loss: 4.356204  [249728/344985]
loss: 4.613645  [256128/344985]
loss: 4.532795  [262528/344985]
loss: 4.609428  [268928/344985]
loss: 4.408291  [275328/344985]
loss: 4.299186  [281728/344985]
loss: 4.273732  [288128/344985]
loss: 4.115460  [294528/344985]
loss: 4.370517  [300928/344985]
loss: 4.472937  [307328/344985]
loss: 4.459752  [313728/344985]
loss: 4.467196  [320128/344985]
loss: 4.616792  [326528/344985]
loss: 4.474756  [332928/344985]
loss: 4.515503  [339328/344985]

epoch avg train loss: 4.4860997   epoch avg train accuracy: 0.0933548

-------------------------------

Epoch 4

loss: 4.534083  [  128/344985]
loss: 4.640868  [ 6528/344985]
loss: 4.551876  [12928/344985]
loss: 4.370142  [19328/344985]
loss: 4.239856  [25728/344985]
loss: 4.252477  [32128/344985]
loss: 4.451607  [38528/344985]
loss: 4.506969  [44928/344985]
loss: 4.352304  [51328/344985]
loss: 4.314192  [57728/344985]
loss: 4.625572  [64128/344985]
loss: 4.419253  [70528/344985]
loss: 4.162271  [76928/344985]
loss: 4.254373  [83328/344985]
loss: 4.288044  [89728/344985]
loss: 4.562042  [96128/344985]
loss: 4.554488  [102528/344985]
loss: 3.879945  [108928/344985]
loss: 4.236253  [115328/344985]
loss: 4.302537  [121728/344985]
loss: 4.464991  [128128/344985]
loss: 4.561432  [134528/344985]
loss: 4.397943  [140928/344985]
loss: 4.556933  [147328/344985]
loss: 4.339034  [153728/344985]
loss: 4.345443  [160128/344985]
loss: 4.328790  [166528/344985]
loss: 4.442114  [172928/344985]
loss: 4.292849  [179328/344985]
loss: 4.086007  [185728/344985]
loss: 4.375638  [192128/344985]
loss: 4.450477  [198528/344985]
loss: 4.497934  [204928/344985]
loss: 4.294141  [211328/344985]
loss: 4.301263  [217728/344985]
loss: 4.190759  [224128/344985]
loss: 4.080025  [230528/344985]
loss: 4.330763  [236928/344985]
loss: 4.277902  [243328/344985]
loss: 4.393017  [249728/344985]
loss: 4.141980  [256128/344985]
loss: 4.564601  [262528/344985]
loss: 4.174472  [268928/344985]
loss: 4.139471  [275328/344985]
loss: 4.270368  [281728/344985]
loss: 4.370665  [288128/344985]
loss: 4.176499  [294528/344985]
loss: 4.086543  [300928/344985]
loss: 4.328125  [307328/344985]
loss: 4.235420  [313728/344985]
loss: 4.481495  [320128/344985]
loss: 4.371608  [326528/344985]
loss: 4.400988  [332928/344985]
loss: 4.508335  [339328/344985]

epoch avg train loss: 4.3681096   epoch avg train accuracy: 0.1071583

-------------------------------

Epoch 5

loss: 4.271623  [  128/344985]
loss: 4.282045  [ 6528/344985]
loss: 4.295684  [12928/344985]
loss: 4.356112  [19328/344985]
loss: 4.182454  [25728/344985]
loss: 4.201830  [32128/344985]
loss: 4.114098  [38528/344985]
loss: 4.050271  [44928/344985]
loss: 4.102346  [51328/344985]
loss: 4.033782  [57728/344985]
loss: 4.127417  [64128/344985]
loss: 4.008452  [70528/344985]
loss: 4.204576  [76928/344985]
loss: 4.188078  [83328/344985]
loss: 4.125458  [89728/344985]
loss: 4.202680  [96128/344985]
loss: 4.281873  [102528/344985]
loss: 4.280918  [108928/344985]
loss: 4.281066  [115328/344985]
loss: 4.347989  [121728/344985]
loss: 3.954391  [128128/344985]
loss: 4.218486  [134528/344985]
loss: 4.186415  [140928/344985]
loss: 4.066572  [147328/344985]
loss: 4.146152  [153728/344985]
loss: 3.932623  [160128/344985]
loss: 3.983573  [166528/344985]
loss: 4.131755  [172928/344985]
loss: 4.212996  [179328/344985]
loss: 4.084285  [185728/344985]
loss: 3.965027  [192128/344985]
loss: 4.066964  [198528/344985]
loss: 4.364787  [204928/344985]
loss: 4.327407  [211328/344985]
loss: 4.070965  [217728/344985]
loss: 4.127662  [224128/344985]
loss: 4.024029  [230528/344985]
loss: 3.978921  [236928/344985]
loss: 4.272399  [243328/344985]
loss: 4.374331  [249728/344985]
loss: 4.059571  [256128/344985]
loss: 3.819285  [262528/344985]
loss: 4.044466  [268928/344985]
loss: 3.790911  [275328/344985]
loss: 3.988260  [281728/344985]
loss: 3.992826  [288128/344985]
loss: 4.305071  [294528/344985]
loss: 3.847410  [300928/344985]
loss: 3.981208  [307328/344985]
loss: 3.729453  [313728/344985]
loss: 4.215575  [320128/344985]
loss: 4.142200  [326528/344985]
loss: 3.981689  [332928/344985]
loss: 3.855249  [339328/344985]

epoch avg train loss: 4.1174683   epoch avg train accuracy: 0.1368262

-------------------------------

Epoch 6

loss: 3.914892  [  128/344985]
loss: 3.889669  [ 6528/344985]
loss: 3.995299  [12928/344985]
loss: 3.863381  [19328/344985]
loss: 4.033305  [25728/344985]
loss: 4.076174  [32128/344985]
loss: 3.755654  [38528/344985]
loss: 3.732498  [44928/344985]
loss: 3.777480  [51328/344985]
loss: 3.713146  [57728/344985]
loss: 3.883263  [64128/344985]
loss: 3.806958  [70528/344985]
loss: 3.912194  [76928/344985]
loss: 3.865757  [83328/344985]
loss: 3.825786  [89728/344985]
loss: 3.918350  [96128/344985]
loss: 4.174627  [102528/344985]
loss: 3.869875  [108928/344985]
loss: 4.138102  [115328/344985]
loss: 3.784982  [121728/344985]
loss: 3.951902  [128128/344985]
loss: 4.008553  [134528/344985]
loss: 4.032641  [140928/344985]
loss: 4.058046  [147328/344985]
loss: 4.074948  [153728/344985]
loss: 4.448219  [160128/344985]
loss: 3.825032  [166528/344985]
loss: 3.896173  [172928/344985]
loss: 3.854685  [179328/344985]
loss: 3.818693  [185728/344985]
loss: 3.816583  [192128/344985]
loss: 3.767104  [198528/344985]
loss: 4.069019  [204928/344985]
loss: 3.686605  [211328/344985]
loss: 4.276015  [217728/344985]
loss: 3.840807  [224128/344985]
loss: 3.917386  [230528/344985]
loss: 4.075835  [236928/344985]
loss: 4.023893  [243328/344985]
loss: 3.941040  [249728/344985]
loss: 3.832591  [256128/344985]
loss: 4.104724  [262528/344985]
loss: 3.673795  [268928/344985]
loss: 3.761983  [275328/344985]
loss: 3.834878  [281728/344985]
loss: 3.945386  [288128/344985]
loss: 3.880849  [294528/344985]
loss: 4.127845  [300928/344985]
loss: 4.097262  [307328/344985]
loss: 3.887467  [313728/344985]
loss: 3.778442  [320128/344985]
loss: 3.927644  [326528/344985]
loss: 3.967440  [332928/344985]
loss: 3.856020  [339328/344985]

epoch avg train loss: 3.9229692   epoch avg train accuracy: 0.1651550

-------------------------------

Epoch 7

loss: 3.786219  [  128/344985]
loss: 3.947083  [ 6528/344985]
loss: 3.943905  [12928/344985]
loss: 3.570529  [19328/344985]
loss: 3.812245  [25728/344985]
loss: 3.835638  [32128/344985]
loss: 4.036099  [38528/344985]
loss: 3.546001  [44928/344985]
loss: 3.762756  [51328/344985]
loss: 3.886374  [57728/344985]
loss: 3.719854  [64128/344985]
loss: 3.793195  [70528/344985]
loss: 3.947675  [76928/344985]
loss: 3.891911  [83328/344985]
loss: 3.624286  [89728/344985]
loss: 3.674463  [96128/344985]
loss: 3.877783  [102528/344985]
loss: 3.756927  [108928/344985]
loss: 4.028782  [115328/344985]
loss: 3.736996  [121728/344985]
loss: 3.638985  [128128/344985]
loss: 4.042224  [134528/344985]
loss: 3.910352  [140928/344985]
loss: 4.059909  [147328/344985]
loss: 4.098402  [153728/344985]
loss: 3.408263  [160128/344985]
loss: 3.723810  [166528/344985]
loss: 3.874517  [172928/344985]
loss: 3.800906  [179328/344985]
loss: 3.912798  [185728/344985]
loss: 3.911823  [192128/344985]
loss: 3.747464  [198528/344985]
loss: 3.996550  [204928/344985]
loss: 3.751529  [211328/344985]
loss: 3.832811  [217728/344985]
loss: 3.827218  [224128/344985]
loss: 4.018023  [230528/344985]
loss: 3.759727  [236928/344985]
loss: 3.456055  [243328/344985]
loss: 3.939578  [249728/344985]
loss: 3.941707  [256128/344985]
loss: 3.919077  [262528/344985]
loss: 3.879790  [268928/344985]
loss: 3.996353  [275328/344985]
loss: 3.841873  [281728/344985]
loss: 3.917030  [288128/344985]
loss: 3.738286  [294528/344985]
loss: 4.076020  [300928/344985]
loss: 3.710729  [307328/344985]
loss: 3.959768  [313728/344985]
loss: 3.779544  [320128/344985]
loss: 3.834561  [326528/344985]
loss: 3.794331  [332928/344985]
loss: 4.026309  [339328/344985]

epoch avg train loss: 3.7992524   epoch avg train accuracy: 0.1844776

-------------------------------

Epoch 8

loss: 3.907674  [  128/344985]
loss: 3.558617  [ 6528/344985]
loss: 3.920530  [12928/344985]
loss: 3.521725  [19328/344985]
loss: 3.690101  [25728/344985]
loss: 3.759727  [32128/344985]
loss: 3.743733  [38528/344985]
loss: 3.452107  [44928/344985]
loss: 3.620922  [51328/344985]
loss: 3.546568  [57728/344985]
loss: 3.794756  [64128/344985]
loss: 3.718931  [70528/344985]
loss: 4.093117  [76928/344985]
loss: 3.819758  [83328/344985]
loss: 3.831867  [89728/344985]
loss: 3.681909  [96128/344985]
loss: 3.672554  [102528/344985]
loss: 3.901631  [108928/344985]
loss: 3.322023  [115328/344985]
loss: 3.871624  [121728/344985]
loss: 3.622591  [128128/344985]
loss: 3.718497  [134528/344985]
loss: 4.039755  [140928/344985]
loss: 3.601201  [147328/344985]
loss: 3.763784  [153728/344985]
loss: 3.508905  [160128/344985]
loss: 3.793151  [166528/344985]
loss: 3.674406  [172928/344985]
loss: 3.794639  [179328/344985]
loss: 3.423055  [185728/344985]
loss: 3.610584  [192128/344985]
loss: 3.508110  [198528/344985]
loss: 3.987248  [204928/344985]
loss: 3.920825  [211328/344985]
loss: 3.586385  [217728/344985]
loss: 3.765992  [224128/344985]
loss: 3.859963  [230528/344985]
loss: 3.473990  [236928/344985]
loss: 3.822539  [243328/344985]
loss: 3.484151  [249728/344985]
loss: 3.422444  [256128/344985]
loss: 3.513124  [262528/344985]
loss: 3.489529  [268928/344985]
loss: 3.611515  [275328/344985]
loss: 3.517767  [281728/344985]
loss: 3.914891  [288128/344985]
loss: 3.554447  [294528/344985]
loss: 3.739489  [300928/344985]
loss: 3.888601  [307328/344985]
loss: 3.666082  [313728/344985]
loss: 3.478844  [320128/344985]
loss: 3.377873  [326528/344985]
loss: 3.772812  [332928/344985]
loss: 3.814996  [339328/344985]

epoch avg train loss: 3.7079943   epoch avg train accuracy: 0.2011276

-------------------------------

Epoch 9

loss: 3.496129  [  128/344985]
loss: 3.928912  [ 6528/344985]
loss: 3.648213  [12928/344985]
loss: 3.545644  [19328/344985]
loss: 3.721976  [25728/344985]
loss: 3.141539  [32128/344985]
loss: 3.674847  [38528/344985]
loss: 3.534488  [44928/344985]
loss: 3.991768  [51328/344985]
loss: 3.288075  [57728/344985]
loss: 3.450063  [64128/344985]
loss: 3.637662  [70528/344985]
loss: 3.746735  [76928/344985]
loss: 3.748513  [83328/344985]
loss: 3.826391  [89728/344985]
loss: 3.546698  [96128/344985]
loss: 3.568997  [102528/344985]
loss: 3.786984  [108928/344985]
loss: 3.937881  [115328/344985]
loss: 3.945701  [121728/344985]
loss: 3.712408  [128128/344985]
loss: 3.781656  [134528/344985]
loss: 3.401021  [140928/344985]
loss: 3.339554  [147328/344985]
loss: 3.925411  [153728/344985]
loss: 3.722713  [160128/344985]
loss: 3.519623  [166528/344985]
loss: 3.821574  [172928/344985]
loss: 3.725619  [179328/344985]
loss: 3.347767  [185728/344985]
loss: 3.666265  [192128/344985]
loss: 3.679586  [198528/344985]
loss: 3.441270  [204928/344985]
loss: 3.837895  [211328/344985]
loss: 3.603873  [217728/344985]
loss: 3.476911  [224128/344985]
loss: 3.688889  [230528/344985]
loss: 3.752910  [236928/344985]
loss: 3.463311  [243328/344985]
loss: 3.369244  [249728/344985]
loss: 3.596618  [256128/344985]
loss: 3.639987  [262528/344985]
loss: 3.601791  [268928/344985]
loss: 3.720361  [275328/344985]
loss: 3.514636  [281728/344985]
loss: 3.755469  [288128/344985]
loss: 3.501363  [294528/344985]
loss: 3.422912  [300928/344985]
loss: 3.569053  [307328/344985]
loss: 3.481643  [313728/344985]
loss: 3.981838  [320128/344985]
loss: 3.670451  [326528/344985]
loss: 3.865222  [332928/344985]
loss: 3.300560  [339328/344985]

epoch avg train loss: 3.6340700   epoch avg train accuracy: 0.2121484

-------------------------------

Epoch 10

loss: 3.827238  [  128/344985]
loss: 3.633364  [ 6528/344985]
loss: 3.448148  [12928/344985]
loss: 3.594574  [19328/344985]
loss: 3.666841  [25728/344985]
loss: 3.504264  [32128/344985]
loss: 3.702672  [38528/344985]
loss: 3.290866  [44928/344985]
loss: 3.402186  [51328/344985]
loss: 3.350767  [57728/344985]
loss: 3.778017  [64128/344985]
loss: 3.689397  [70528/344985]
loss: 3.866601  [76928/344985]
loss: 3.749161  [83328/344985]
loss: 3.737846  [89728/344985]
loss: 3.552760  [96128/344985]
loss: 3.702320  [102528/344985]
loss: 3.776366  [108928/344985]
loss: 3.641134  [115328/344985]
loss: 3.965965  [121728/344985]
loss: 3.773333  [128128/344985]
loss: 3.402030  [134528/344985]
loss: 3.538096  [140928/344985]
loss: 3.243973  [147328/344985]
loss: 3.583242  [153728/344985]
loss: 3.765075  [160128/344985]
loss: 3.647924  [166528/344985]
loss: 3.884558  [172928/344985]
loss: 3.541684  [179328/344985]
loss: 3.526135  [185728/344985]
loss: 3.813233  [192128/344985]
loss: 3.788646  [198528/344985]
loss: 3.623482  [204928/344985]
loss: 3.580485  [211328/344985]
loss: 3.692922  [217728/344985]
loss: 3.659983  [224128/344985]
loss: 3.648838  [230528/344985]
loss: 3.862410  [236928/344985]
loss: 3.415929  [243328/344985]
loss: 3.421632  [249728/344985]
loss: 3.717533  [256128/344985]
loss: 3.321914  [262528/344985]
loss: 3.415121  [268928/344985]
loss: 3.476139  [275328/344985]
loss: 3.553663  [281728/344985]
loss: 3.900247  [288128/344985]
loss: 3.641097  [294528/344985]
loss: 3.518610  [300928/344985]
loss: 3.815595  [307328/344985]
loss: 3.568478  [313728/344985]
loss: 3.823678  [320128/344985]
loss: 3.664071  [326528/344985]
loss: 3.423932  [332928/344985]
loss: 3.914738  [339328/344985]

epoch avg train loss: 3.5705236   epoch avg train accuracy: 0.2240097

-------------------------------

Evaluating against random transformations...
Mean acc: 0.2112
Acc std: 0.0007508
