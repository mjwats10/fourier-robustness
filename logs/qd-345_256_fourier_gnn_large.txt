Epoch 1

loss: 5.843556  [  128/344985]
loss: 5.776301  [ 6528/344985]
loss: 5.845443  [12928/344985]
loss: 5.824966  [19328/344985]
loss: 5.708550  [25728/344985]
loss: 5.646080  [32128/344985]
loss: 5.613952  [38528/344985]
loss: 5.565255  [44928/344985]
loss: 5.520233  [51328/344985]
loss: 5.594833  [57728/344985]
loss: 5.406544  [64128/344985]
loss: 5.556274  [70528/344985]
loss: 5.528346  [76928/344985]
loss: 5.564264  [83328/344985]
loss: 5.401105  [89728/344985]
loss: 5.470878  [96128/344985]
loss: 5.467683  [102528/344985]
loss: 5.436537  [108928/344985]
loss: 5.483358  [115328/344985]
loss: 5.517396  [121728/344985]
loss: 5.427094  [128128/344985]
loss: 5.537843  [134528/344985]
loss: 5.341399  [140928/344985]
loss: 5.477965  [147328/344985]
loss: 5.478713  [153728/344985]
loss: 5.498611  [160128/344985]
loss: 5.563339  [166528/344985]
loss: 5.415688  [172928/344985]
loss: 5.676713  [179328/344985]
loss: 5.562954  [185728/344985]
loss: 5.499964  [192128/344985]
loss: 5.434167  [198528/344985]
loss: 5.513667  [204928/344985]
loss: 5.459230  [211328/344985]
loss: 5.483509  [217728/344985]
loss: 5.432146  [224128/344985]
loss: 5.510685  [230528/344985]
loss: 5.397904  [236928/344985]
loss: 5.503186  [243328/344985]
loss: 5.297517  [249728/344985]
loss: 5.282944  [256128/344985]
loss: 5.374455  [262528/344985]
loss: 5.453342  [268928/344985]
loss: 5.328974  [275328/344985]
loss: 5.309938  [281728/344985]
loss: 5.206391  [288128/344985]
loss: 5.239627  [294528/344985]
loss: 5.287336  [300928/344985]
loss: 5.233343  [307328/344985]
loss: 5.111783  [313728/344985]
loss: 5.172861  [320128/344985]
loss: 5.094181  [326528/344985]
loss: 5.013978  [332928/344985]
loss: 5.155278  [339328/344985]

epoch avg train loss: 5.4567205   epoch avg train accuracy: 0.0157398

-------------------------------

Epoch 2

loss: 5.087622  [  128/344985]
loss: 5.135025  [ 6528/344985]
loss: 5.137141  [12928/344985]
loss: 5.129209  [19328/344985]
loss: 4.917063  [25728/344985]
loss: 5.012677  [32128/344985]
loss: 5.131995  [38528/344985]
loss: 5.164873  [44928/344985]
loss: 4.903727  [51328/344985]
loss: 4.903635  [57728/344985]
loss: 5.107738  [64128/344985]
loss: 4.946269  [70528/344985]
loss: 4.890715  [76928/344985]
loss: 4.805772  [83328/344985]
loss: 4.825027  [89728/344985]
loss: 4.678339  [96128/344985]
loss: 4.995063  [102528/344985]
loss: 4.873076  [108928/344985]
loss: 4.783037  [115328/344985]
loss: 4.869198  [121728/344985]
loss: 4.875978  [128128/344985]
loss: 4.890086  [134528/344985]
loss: 4.972843  [140928/344985]
loss: 4.641317  [147328/344985]
loss: 4.816814  [153728/344985]
loss: 4.638920  [160128/344985]
loss: 4.701149  [166528/344985]
loss: 4.602730  [172928/344985]
loss: 4.534393  [179328/344985]
loss: 4.606519  [185728/344985]
loss: 4.807687  [192128/344985]
loss: 4.552388  [198528/344985]
loss: 4.509161  [204928/344985]
loss: 4.563237  [211328/344985]
loss: 4.339150  [217728/344985]
loss: 4.624208  [224128/344985]
loss: 4.659178  [230528/344985]
loss: 4.415947  [236928/344985]
loss: 4.831257  [243328/344985]
loss: 4.582193  [249728/344985]
loss: 4.348822  [256128/344985]
loss: 4.595732  [262528/344985]
loss: 4.545859  [268928/344985]
loss: 4.709869  [275328/344985]
loss: 4.658501  [281728/344985]
loss: 4.561758  [288128/344985]
loss: 4.553687  [294528/344985]
loss: 4.744150  [300928/344985]
loss: 4.680059  [307328/344985]
loss: 4.535285  [313728/344985]
loss: 4.421762  [320128/344985]
loss: 4.624866  [326528/344985]
loss: 4.324998  [332928/344985]
loss: 4.434521  [339328/344985]

epoch avg train loss: 4.7462731   epoch avg train accuracy: 0.0631970

-------------------------------

Epoch 3

loss: 4.625886  [  128/344985]
loss: 4.374681  [ 6528/344985]
loss: 4.636580  [12928/344985]
loss: 4.606052  [19328/344985]
loss: 4.319552  [25728/344985]
loss: 4.573347  [32128/344985]
loss: 4.297607  [38528/344985]
loss: 4.480781  [44928/344985]
loss: 4.758492  [51328/344985]
loss: 4.485347  [57728/344985]
loss: 4.686345  [64128/344985]
loss: 4.455143  [70528/344985]
loss: 4.450662  [76928/344985]
loss: 4.311493  [83328/344985]
loss: 4.594409  [89728/344985]
loss: 4.335030  [96128/344985]
loss: 4.576237  [102528/344985]
loss: 4.367514  [108928/344985]
loss: 4.416559  [115328/344985]
loss: 4.486975  [121728/344985]
loss: 4.219219  [128128/344985]
loss: 4.362153  [134528/344985]
loss: 4.430411  [140928/344985]
loss: 4.390007  [147328/344985]
loss: 4.233665  [153728/344985]
loss: 4.314463  [160128/344985]
loss: 4.594330  [166528/344985]
loss: 4.523172  [172928/344985]
loss: 4.190548  [179328/344985]
loss: 4.462574  [185728/344985]
loss: 4.262180  [192128/344985]
loss: 4.434666  [198528/344985]
loss: 4.476116  [204928/344985]
loss: 4.585304  [211328/344985]
loss: 4.401416  [217728/344985]
loss: 4.519301  [224128/344985]
loss: 4.432457  [230528/344985]
loss: 4.405314  [236928/344985]
loss: 4.401900  [243328/344985]
loss: 4.375468  [249728/344985]
loss: 4.538321  [256128/344985]
loss: 4.445678  [262528/344985]
loss: 4.370728  [268928/344985]
loss: 4.358361  [275328/344985]
loss: 4.244250  [281728/344985]
loss: 4.284147  [288128/344985]
loss: 4.161311  [294528/344985]
loss: 4.238720  [300928/344985]
loss: 4.266816  [307328/344985]
loss: 4.365470  [313728/344985]
loss: 4.356339  [320128/344985]
loss: 4.637036  [326528/344985]
loss: 4.491274  [332928/344985]
loss: 4.407968  [339328/344985]

epoch avg train loss: 4.4235324   epoch avg train accuracy: 0.0980419

-------------------------------

Epoch 4

loss: 4.179101  [  128/344985]
loss: 4.494263  [ 6528/344985]
loss: 4.349524  [12928/344985]
loss: 4.321158  [19328/344985]
loss: 4.160338  [25728/344985]
loss: 4.207924  [32128/344985]
loss: 4.246572  [38528/344985]
loss: 4.267396  [44928/344985]
loss: 4.223335  [51328/344985]
loss: 4.209197  [57728/344985]
loss: 4.395857  [64128/344985]
loss: 4.143666  [70528/344985]
loss: 4.126964  [76928/344985]
loss: 4.148197  [83328/344985]
loss: 4.313461  [89728/344985]
loss: 4.424367  [96128/344985]
loss: 4.425907  [102528/344985]
loss: 3.861394  [108928/344985]
loss: 4.048681  [115328/344985]
loss: 4.032067  [121728/344985]
loss: 4.241096  [128128/344985]
loss: 4.343960  [134528/344985]
loss: 4.268180  [140928/344985]
loss: 4.148931  [147328/344985]
loss: 4.065969  [153728/344985]
loss: 4.086627  [160128/344985]
loss: 4.248399  [166528/344985]
loss: 4.268668  [172928/344985]
loss: 4.101470  [179328/344985]
loss: 3.947423  [185728/344985]
loss: 4.129290  [192128/344985]
loss: 4.402256  [198528/344985]
loss: 4.290130  [204928/344985]
loss: 4.014921  [211328/344985]
loss: 4.177809  [217728/344985]
loss: 4.048985  [224128/344985]
loss: 3.924634  [230528/344985]
loss: 4.171395  [236928/344985]
loss: 4.032419  [243328/344985]
loss: 4.270368  [249728/344985]
loss: 4.029817  [256128/344985]
loss: 4.384628  [262528/344985]
loss: 4.005830  [268928/344985]
loss: 3.898377  [275328/344985]
loss: 4.155340  [281728/344985]
loss: 4.186166  [288128/344985]
loss: 4.092415  [294528/344985]
loss: 3.884121  [300928/344985]
loss: 4.180581  [307328/344985]
loss: 4.016885  [313728/344985]
loss: 4.397014  [320128/344985]
loss: 4.300497  [326528/344985]
loss: 4.219801  [332928/344985]
loss: 4.373065  [339328/344985]

epoch avg train loss: 4.2111067   epoch avg train accuracy: 0.1231677

-------------------------------

Epoch 5

loss: 4.062298  [  128/344985]
loss: 4.247796  [ 6528/344985]
loss: 4.132323  [12928/344985]
loss: 4.219831  [19328/344985]
loss: 4.078923  [25728/344985]
loss: 4.141246  [32128/344985]
loss: 3.877222  [38528/344985]
loss: 3.971252  [44928/344985]
loss: 4.055152  [51328/344985]
loss: 3.859565  [57728/344985]
loss: 4.124889  [64128/344985]
loss: 3.806009  [70528/344985]
loss: 4.029280  [76928/344985]
loss: 3.960328  [83328/344985]
loss: 4.049660  [89728/344985]
loss: 4.148029  [96128/344985]
loss: 4.168322  [102528/344985]
loss: 4.070633  [108928/344985]
loss: 4.029605  [115328/344985]
loss: 4.001738  [121728/344985]
loss: 3.924917  [128128/344985]
loss: 4.075326  [134528/344985]
loss: 4.037585  [140928/344985]
loss: 4.013373  [147328/344985]
loss: 4.051879  [153728/344985]
loss: 3.882886  [160128/344985]
loss: 3.949038  [166528/344985]
loss: 3.964808  [172928/344985]
loss: 4.183830  [179328/344985]
loss: 3.978909  [185728/344985]
loss: 3.982645  [192128/344985]
loss: 3.923665  [198528/344985]
loss: 4.079441  [204928/344985]
loss: 4.117533  [211328/344985]
loss: 3.974455  [217728/344985]
loss: 4.144988  [224128/344985]
loss: 4.016904  [230528/344985]
loss: 3.984232  [236928/344985]
loss: 3.998316  [243328/344985]
loss: 4.256064  [249728/344985]
loss: 3.980929  [256128/344985]
loss: 3.624496  [262528/344985]
loss: 3.894177  [268928/344985]
loss: 3.771927  [275328/344985]
loss: 3.926105  [281728/344985]
loss: 3.829410  [288128/344985]
loss: 4.220316  [294528/344985]
loss: 3.610937  [300928/344985]
loss: 3.859477  [307328/344985]
loss: 3.689427  [313728/344985]
loss: 4.067192  [320128/344985]
loss: 3.955420  [326528/344985]
loss: 3.832056  [332928/344985]
loss: 3.687427  [339328/344985]

epoch avg train loss: 3.9945400   epoch avg train accuracy: 0.1538705

-------------------------------

Epoch 6

loss: 3.740398  [  128/344985]
loss: 3.757419  [ 6528/344985]
loss: 3.807391  [12928/344985]
loss: 3.857552  [19328/344985]
loss: 3.719002  [25728/344985]
loss: 4.070003  [32128/344985]
loss: 3.773538  [38528/344985]
loss: 3.467566  [44928/344985]
loss: 3.643530  [51328/344985]
loss: 3.645091  [57728/344985]
loss: 3.696232  [64128/344985]
loss: 3.749396  [70528/344985]
loss: 3.609233  [76928/344985]
loss: 3.860156  [83328/344985]
loss: 3.589244  [89728/344985]
loss: 3.925169  [96128/344985]
loss: 4.129231  [102528/344985]
loss: 3.830276  [108928/344985]
loss: 3.973464  [115328/344985]
loss: 3.630375  [121728/344985]
loss: 3.896543  [128128/344985]
loss: 3.919742  [134528/344985]
loss: 3.882628  [140928/344985]
loss: 3.968843  [147328/344985]
loss: 3.964702  [153728/344985]
loss: 4.195655  [160128/344985]
loss: 3.700213  [166528/344985]
loss: 3.727587  [172928/344985]
loss: 3.723268  [179328/344985]
loss: 3.914299  [185728/344985]
loss: 3.789371  [192128/344985]
loss: 3.842821  [198528/344985]
loss: 3.869641  [204928/344985]
loss: 3.297549  [211328/344985]
loss: 4.092052  [217728/344985]
loss: 3.904158  [224128/344985]
loss: 3.928391  [230528/344985]
loss: 3.915071  [236928/344985]
loss: 3.868361  [243328/344985]
loss: 3.930250  [249728/344985]
loss: 3.796054  [256128/344985]
loss: 4.035587  [262528/344985]
loss: 3.588081  [268928/344985]
loss: 3.798937  [275328/344985]
loss: 3.706581  [281728/344985]
loss: 3.766535  [288128/344985]
loss: 3.711562  [294528/344985]
loss: 3.972273  [300928/344985]
loss: 3.897361  [307328/344985]
loss: 3.734045  [313728/344985]
loss: 3.602210  [320128/344985]
loss: 3.750272  [326528/344985]
loss: 3.781557  [332928/344985]
loss: 3.670184  [339328/344985]

epoch avg train loss: 3.8113570   epoch avg train accuracy: 0.1828485

-------------------------------

Epoch 7

loss: 3.631328  [  128/344985]
loss: 3.606840  [ 6528/344985]
loss: 3.796165  [12928/344985]
loss: 3.444408  [19328/344985]
loss: 3.644362  [25728/344985]
loss: 3.787068  [32128/344985]
loss: 4.020347  [38528/344985]
loss: 3.592510  [44928/344985]
loss: 3.687255  [51328/344985]
loss: 3.783853  [57728/344985]
loss: 3.561354  [64128/344985]
loss: 3.591466  [70528/344985]
loss: 3.766309  [76928/344985]
loss: 3.730405  [83328/344985]
loss: 3.626880  [89728/344985]
loss: 3.517487  [96128/344985]
loss: 3.635444  [102528/344985]
loss: 3.579268  [108928/344985]
loss: 3.956068  [115328/344985]
loss: 3.608361  [121728/344985]
loss: 3.523334  [128128/344985]
loss: 3.951229  [134528/344985]
loss: 3.738989  [140928/344985]
loss: 3.920333  [147328/344985]
loss: 4.067327  [153728/344985]
loss: 3.366446  [160128/344985]
loss: 3.696397  [166528/344985]
loss: 3.731301  [172928/344985]
loss: 3.701226  [179328/344985]
loss: 3.849613  [185728/344985]
loss: 3.689716  [192128/344985]
loss: 3.476382  [198528/344985]
loss: 3.837544  [204928/344985]
loss: 3.630469  [211328/344985]
loss: 3.614926  [217728/344985]
loss: 3.790792  [224128/344985]
loss: 3.983009  [230528/344985]
loss: 3.638134  [236928/344985]
loss: 3.351603  [243328/344985]
loss: 3.844224  [249728/344985]
loss: 3.806430  [256128/344985]
loss: 3.739039  [262528/344985]
loss: 3.671486  [268928/344985]
loss: 3.847908  [275328/344985]
loss: 3.732135  [281728/344985]
loss: 3.881426  [288128/344985]
loss: 3.790748  [294528/344985]
loss: 4.045113  [300928/344985]
loss: 3.600463  [307328/344985]
loss: 3.808406  [313728/344985]
loss: 3.538671  [320128/344985]
loss: 3.775156  [326528/344985]
loss: 3.572018  [332928/344985]
loss: 3.732286  [339328/344985]

epoch avg train loss: 3.6670315   epoch avg train accuracy: 0.2062640

-------------------------------

Epoch 8

loss: 3.726745  [  128/344985]
loss: 3.486275  [ 6528/344985]
loss: 3.713512  [12928/344985]
loss: 3.557276  [19328/344985]
loss: 3.575713  [25728/344985]
loss: 3.513491  [32128/344985]
loss: 3.569785  [38528/344985]
loss: 3.267555  [44928/344985]
loss: 3.249997  [51328/344985]
loss: 3.288720  [57728/344985]
loss: 3.713813  [64128/344985]
loss: 3.652734  [70528/344985]
loss: 3.945401  [76928/344985]
loss: 3.703535  [83328/344985]
loss: 3.727207  [89728/344985]
loss: 3.783453  [96128/344985]
loss: 3.576663  [102528/344985]
loss: 3.816633  [108928/344985]
loss: 3.282256  [115328/344985]
loss: 3.642465  [121728/344985]
loss: 3.327279  [128128/344985]
loss: 3.648423  [134528/344985]
loss: 3.754846  [140928/344985]
loss: 3.445401  [147328/344985]
loss: 3.623043  [153728/344985]
loss: 3.194469  [160128/344985]
loss: 3.762069  [166528/344985]
loss: 3.695478  [172928/344985]
loss: 3.675533  [179328/344985]
loss: 3.279630  [185728/344985]
loss: 3.493976  [192128/344985]
loss: 3.509721  [198528/344985]
loss: 4.018589  [204928/344985]
loss: 3.894845  [211328/344985]
loss: 3.540949  [217728/344985]
loss: 3.464563  [224128/344985]
loss: 3.507532  [230528/344985]
loss: 3.214258  [236928/344985]
loss: 3.598640  [243328/344985]
loss: 3.238855  [249728/344985]
loss: 3.368075  [256128/344985]
loss: 3.287482  [262528/344985]
loss: 3.551146  [268928/344985]
loss: 3.710371  [275328/344985]
loss: 3.491557  [281728/344985]
loss: 3.707624  [288128/344985]
loss: 3.535047  [294528/344985]
loss: 3.527831  [300928/344985]
loss: 3.498426  [307328/344985]
loss: 3.365198  [313728/344985]
loss: 3.363403  [320128/344985]
loss: 3.034512  [326528/344985]
loss: 3.399481  [332928/344985]
loss: 3.600815  [339328/344985]

epoch avg train loss: 3.5548672   epoch avg train accuracy: 0.2247460

-------------------------------

Epoch 9

loss: 3.394814  [  128/344985]
loss: 3.731725  [ 6528/344985]
loss: 3.590215  [12928/344985]
loss: 3.465348  [19328/344985]
loss: 3.535039  [25728/344985]
loss: 2.853131  [32128/344985]
loss: 3.444113  [38528/344985]
loss: 3.271787  [44928/344985]
loss: 3.723430  [51328/344985]
loss: 3.083154  [57728/344985]
loss: 3.432753  [64128/344985]
loss: 3.394185  [70528/344985]
loss: 3.669168  [76928/344985]
loss: 3.518801  [83328/344985]
loss: 3.535228  [89728/344985]
loss: 3.627609  [96128/344985]
loss: 3.531695  [102528/344985]
loss: 3.549906  [108928/344985]
loss: 3.580296  [115328/344985]
loss: 3.793621  [121728/344985]
loss: 3.601497  [128128/344985]
loss: 3.622085  [134528/344985]
loss: 3.454337  [140928/344985]
loss: 3.117337  [147328/344985]
loss: 3.659106  [153728/344985]
loss: 3.566890  [160128/344985]
loss: 3.427188  [166528/344985]
loss: 3.369725  [172928/344985]
loss: 3.568537  [179328/344985]
loss: 3.294859  [185728/344985]
loss: 3.294221  [192128/344985]
loss: 3.550112  [198528/344985]
loss: 3.330325  [204928/344985]
loss: 3.570111  [211328/344985]
loss: 3.617269  [217728/344985]
loss: 3.338205  [224128/344985]
loss: 3.664282  [230528/344985]
loss: 3.518920  [236928/344985]
loss: 3.326426  [243328/344985]
loss: 3.137408  [249728/344985]
loss: 3.462923  [256128/344985]
loss: 3.388502  [262528/344985]
loss: 3.418789  [268928/344985]
loss: 3.349904  [275328/344985]
loss: 3.578568  [281728/344985]
loss: 3.479157  [288128/344985]
loss: 3.490070  [294528/344985]
loss: 3.254454  [300928/344985]
loss: 3.499014  [307328/344985]
loss: 3.247547  [313728/344985]
loss: 3.812060  [320128/344985]
loss: 3.476164  [326528/344985]
loss: 3.674830  [332928/344985]
loss: 3.312383  [339328/344985]

epoch avg train loss: 3.4631770   epoch avg train accuracy: 0.2397930

-------------------------------

Epoch 10

loss: 3.534233  [  128/344985]
loss: 3.538789  [ 6528/344985]
loss: 3.359596  [12928/344985]
loss: 3.370725  [19328/344985]
loss: 3.384054  [25728/344985]
loss: 3.202166  [32128/344985]
loss: 3.532606  [38528/344985]
loss: 3.125531  [44928/344985]
loss: 3.268188  [51328/344985]
loss: 3.120952  [57728/344985]
loss: 3.679399  [64128/344985]
loss: 3.538023  [70528/344985]
loss: 3.570278  [76928/344985]
loss: 3.540667  [83328/344985]
loss: 3.543185  [89728/344985]
loss: 3.424793  [96128/344985]
loss: 3.583194  [102528/344985]
loss: 3.618408  [108928/344985]
loss: 3.265714  [115328/344985]
loss: 3.673421  [121728/344985]
loss: 3.590989  [128128/344985]
loss: 3.160749  [134528/344985]
loss: 3.274578  [140928/344985]
loss: 3.153096  [147328/344985]
loss: 3.466499  [153728/344985]
loss: 3.405082  [160128/344985]
loss: 3.284936  [166528/344985]
loss: 3.552319  [172928/344985]
loss: 3.348236  [179328/344985]
loss: 3.403557  [185728/344985]
loss: 3.561049  [192128/344985]
loss: 3.507288  [198528/344985]
loss: 3.251050  [204928/344985]
loss: 3.418586  [211328/344985]
loss: 3.581733  [217728/344985]
loss: 3.606990  [224128/344985]
loss: 3.532099  [230528/344985]
loss: 3.669853  [236928/344985]
loss: 3.160284  [243328/344985]
loss: 3.275656  [249728/344985]
loss: 3.808982  [256128/344985]
loss: 3.220074  [262528/344985]
loss: 3.257526  [268928/344985]
loss: 3.231631  [275328/344985]
loss: 3.225919  [281728/344985]
loss: 3.685409  [288128/344985]
loss: 3.593686  [294528/344985]
loss: 3.269296  [300928/344985]
loss: 3.425752  [307328/344985]
loss: 3.442030  [313728/344985]
loss: 3.565796  [320128/344985]
loss: 3.336597  [326528/344985]
loss: 3.149885  [332928/344985]
loss: 3.636516  [339328/344985]

epoch avg train loss: 3.3781085   epoch avg train accuracy: 0.2542487

-------------------------------

Evaluating against random transformations...
Mean acc: 0.2372
Acc std: 0.0008940
