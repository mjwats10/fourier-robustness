Epoch 1

loss: 5.845045  [  128/344985]
loss: 5.844180  [ 6528/344985]
loss: 5.842371  [12928/344985]
loss: 5.841845  [19328/344985]
loss: 5.844338  [25728/344985]
loss: 5.559839  [32128/344985]
loss: 4.584591  [38528/344985]
loss: 3.988183  [44928/344985]
loss: 3.827193  [51328/344985]
loss: 3.797542  [57728/344985]
loss: 3.853039  [64128/344985]
loss: 3.626632  [70528/344985]
loss: 3.245347  [76928/344985]
loss: 3.621152  [83328/344985]
loss: 2.777823  [89728/344985]
loss: 3.330078  [96128/344985]
loss: 3.221838  [102528/344985]
loss: 2.980231  [108928/344985]
loss: 3.078223  [115328/344985]
loss: 3.327488  [121728/344985]
loss: 2.838088  [128128/344985]
loss: 2.697841  [134528/344985]
loss: 3.152853  [140928/344985]
loss: 2.458319  [147328/344985]
loss: 2.821120  [153728/344985]
loss: 2.673662  [160128/344985]
loss: 2.878486  [166528/344985]
loss: 2.728332  [172928/344985]
loss: 2.893478  [179328/344985]
loss: 2.413277  [185728/344985]
loss: 2.625180  [192128/344985]
loss: 2.607211  [198528/344985]
loss: 2.676565  [204928/344985]
loss: 2.495194  [211328/344985]
loss: 2.518409  [217728/344985]
loss: 2.326589  [224128/344985]
loss: 2.269936  [230528/344985]
loss: 2.539417  [236928/344985]
loss: 2.892216  [243328/344985]
loss: 2.377306  [249728/344985]
loss: 2.280430  [256128/344985]
loss: 2.243442  [262528/344985]
loss: 2.226541  [268928/344985]
loss: 2.751081  [275328/344985]
loss: 2.625157  [281728/344985]
loss: 2.204688  [288128/344985]
loss: 2.449034  [294528/344985]
loss: 2.195013  [300928/344985]
loss: 2.424630  [307328/344985]
loss: 2.304606  [313728/344985]
loss: 2.256980  [320128/344985]
loss: 2.141192  [326528/344985]
loss: 2.312877  [332928/344985]
loss: 2.391456  [339328/344985]

epoch avg train loss: 3.078470   epoch avg train accuracy: 0.344647

-------------------------------

Epoch 2

loss: 1.692664  [  128/344985]
loss: 1.880067  [ 6528/344985]
loss: 1.946624  [12928/344985]
loss: 1.875281  [19328/344985]
loss: 2.097690  [25728/344985]
loss: 1.576518  [32128/344985]
loss: 1.876531  [38528/344985]
loss: 2.066656  [44928/344985]
loss: 1.934800  [51328/344985]
loss: 2.306765  [57728/344985]
loss: 2.498818  [64128/344985]
loss: 1.970201  [70528/344985]
loss: 2.067245  [76928/344985]
loss: 1.854113  [83328/344985]
loss: 2.187388  [89728/344985]
loss: 1.716796  [96128/344985]
loss: 2.118277  [102528/344985]
loss: 1.855450  [108928/344985]
loss: 2.114202  [115328/344985]
loss: 2.297825  [121728/344985]
loss: 2.013049  [128128/344985]
loss: 2.126666  [134528/344985]
loss: 2.163211  [140928/344985]
loss: 2.203334  [147328/344985]
loss: 2.322167  [153728/344985]
loss: 2.086893  [160128/344985]
loss: 2.047332  [166528/344985]
loss: 1.743292  [172928/344985]
loss: 1.918903  [179328/344985]
loss: 1.876906  [185728/344985]
loss: 1.868142  [192128/344985]
loss: 1.868760  [198528/344985]
loss: 1.788707  [204928/344985]
loss: 2.063885  [211328/344985]
loss: 2.341329  [217728/344985]
loss: 1.867212  [224128/344985]
loss: 2.448597  [230528/344985]
loss: 1.699245  [236928/344985]
loss: 2.217472  [243328/344985]
loss: 1.948295  [249728/344985]
loss: 1.778322  [256128/344985]
loss: 1.720111  [262528/344985]
loss: 2.317960  [268928/344985]
loss: 1.929493  [275328/344985]
loss: 1.753508  [281728/344985]
loss: 2.133257  [288128/344985]
loss: 2.137924  [294528/344985]
loss: 1.785859  [300928/344985]
loss: 1.721027  [307328/344985]
loss: 1.969032  [313728/344985]
loss: 1.619559  [320128/344985]
loss: 1.984705  [326528/344985]
loss: 1.678785  [332928/344985]
loss: 1.831307  [339328/344985]

epoch avg train loss: 2.003847   epoch avg train accuracy: 0.528446

-------------------------------

Epoch 3

loss: 1.960035  [  128/344985]
loss: 1.561802  [ 6528/344985]
loss: 2.208988  [12928/344985]
loss: 1.723351  [19328/344985]
loss: 1.606687  [25728/344985]
loss: 1.783211  [32128/344985]
loss: 1.419147  [38528/344985]
loss: 1.526577  [44928/344985]
loss: 1.803594  [51328/344985]
loss: 1.750458  [57728/344985]
loss: 1.669085  [64128/344985]
loss: 2.018919  [70528/344985]
loss: 1.780190  [76928/344985]
loss: 1.621039  [83328/344985]
loss: 1.663477  [89728/344985]
loss: 1.645655  [96128/344985]
loss: 1.881288  [102528/344985]
loss: 1.707058  [108928/344985]
loss: 1.613596  [115328/344985]
loss: 1.682467  [121728/344985]
loss: 1.623821  [128128/344985]
loss: 1.628208  [134528/344985]
loss: 1.827523  [140928/344985]
loss: 1.972937  [147328/344985]
loss: 1.472602  [153728/344985]
loss: 1.599905  [160128/344985]
loss: 2.016719  [166528/344985]
loss: 1.609712  [172928/344985]
loss: 1.685823  [179328/344985]
loss: 1.985393  [185728/344985]
loss: 1.690710  [192128/344985]
loss: 1.933940  [198528/344985]
loss: 1.706092  [204928/344985]
loss: 2.013207  [211328/344985]
loss: 1.744506  [217728/344985]
loss: 1.937057  [224128/344985]
loss: 1.712290  [230528/344985]
loss: 1.997139  [236928/344985]
loss: 1.812934  [243328/344985]
loss: 1.380438  [249728/344985]
loss: 1.719496  [256128/344985]
loss: 1.579383  [262528/344985]
loss: 2.019769  [268928/344985]
loss: 1.453588  [275328/344985]
loss: 1.540745  [281728/344985]
loss: 1.600599  [288128/344985]
loss: 1.523499  [294528/344985]
loss: 1.984988  [300928/344985]
loss: 1.641697  [307328/344985]
loss: 1.706383  [313728/344985]
loss: 1.671131  [320128/344985]
loss: 1.623170  [326528/344985]
loss: 1.699419  [332928/344985]
loss: 1.717574  [339328/344985]

epoch avg train loss: 1.767731   epoch avg train accuracy: 0.576155

-------------------------------

Epoch 4

loss: 1.061986  [  128/344985]
loss: 1.834843  [ 6528/344985]
loss: 1.736024  [12928/344985]
loss: 1.374450  [19328/344985]
loss: 1.531795  [25728/344985]
loss: 1.423890  [32128/344985]
loss: 1.466226  [38528/344985]
loss: 1.546064  [44928/344985]
loss: 1.672366  [51328/344985]
loss: 1.444018  [57728/344985]
loss: 1.575685  [64128/344985]
loss: 1.664845  [70528/344985]
loss: 1.635173  [76928/344985]
loss: 1.372489  [83328/344985]
loss: 1.913546  [89728/344985]
loss: 2.014967  [96128/344985]
loss: 1.865866  [102528/344985]
loss: 1.507173  [108928/344985]
loss: 1.373263  [115328/344985]
loss: 1.443647  [121728/344985]
loss: 1.860986  [128128/344985]
loss: 1.661719  [134528/344985]
loss: 1.521685  [140928/344985]
loss: 1.648968  [147328/344985]
loss: 1.438110  [153728/344985]
loss: 1.650381  [160128/344985]
loss: 1.421708  [166528/344985]
loss: 1.698928  [172928/344985]
loss: 1.720432  [179328/344985]
loss: 1.541947  [185728/344985]
loss: 1.468384  [192128/344985]
loss: 1.534973  [198528/344985]
loss: 1.574437  [204928/344985]
loss: 1.578728  [211328/344985]
loss: 1.666544  [217728/344985]
loss: 1.736403  [224128/344985]
loss: 1.395839  [230528/344985]
loss: 1.662455  [236928/344985]
loss: 1.516828  [243328/344985]
loss: 1.550634  [249728/344985]
loss: 1.739174  [256128/344985]
loss: 1.861544  [262528/344985]
loss: 1.539104  [268928/344985]
loss: 1.279310  [275328/344985]
loss: 1.415994  [281728/344985]
loss: 1.990405  [288128/344985]
loss: 1.696427  [294528/344985]
loss: 1.693167  [300928/344985]
loss: 1.785231  [307328/344985]
loss: 1.494063  [313728/344985]
loss: 1.922104  [320128/344985]
loss: 1.533564  [326528/344985]
loss: 1.918379  [332928/344985]
loss: 1.583087  [339328/344985]

epoch avg train loss: 1.632491   epoch avg train accuracy: 0.603638

-------------------------------

Epoch 5

loss: 1.483753  [  128/344985]
loss: 1.567303  [ 6528/344985]
loss: 1.503715  [12928/344985]
loss: 1.933591  [19328/344985]
loss: 1.586560  [25728/344985]
loss: 1.570527  [32128/344985]
loss: 1.316295  [38528/344985]
loss: 1.557867  [44928/344985]
loss: 1.418455  [51328/344985]
loss: 1.327547  [57728/344985]
loss: 1.627840  [64128/344985]
loss: 1.745100  [70528/344985]
loss: 1.729447  [76928/344985]
loss: 1.306327  [83328/344985]
loss: 1.273122  [89728/344985]
loss: 1.578241  [96128/344985]
loss: 1.370787  [102528/344985]
loss: 1.299550  [108928/344985]
loss: 1.424602  [115328/344985]
loss: 1.855176  [121728/344985]
loss: 1.399678  [128128/344985]
loss: 1.534851  [134528/344985]
loss: 1.570167  [140928/344985]
loss: 1.527153  [147328/344985]
loss: 1.450810  [153728/344985]
loss: 1.485760  [160128/344985]
loss: 1.629457  [166528/344985]
loss: 1.255166  [172928/344985]
loss: 1.407796  [179328/344985]
loss: 1.499751  [185728/344985]
loss: 1.661468  [192128/344985]
loss: 1.773352  [198528/344985]
loss: 1.855021  [204928/344985]
loss: 1.640611  [211328/344985]
loss: 1.746765  [217728/344985]
loss: 1.438563  [224128/344985]
loss: 1.535455  [230528/344985]
loss: 1.529858  [236928/344985]
loss: 1.755775  [243328/344985]
loss: 1.867243  [249728/344985]
loss: 1.275434  [256128/344985]
loss: 1.338383  [262528/344985]
loss: 1.748005  [268928/344985]
loss: 1.260729  [275328/344985]
loss: 1.564672  [281728/344985]
loss: 1.362947  [288128/344985]
loss: 1.889422  [294528/344985]
loss: 1.385339  [300928/344985]
loss: 1.713545  [307328/344985]
loss: 1.196722  [313728/344985]
loss: 1.292609  [320128/344985]
loss: 1.468902  [326528/344985]
loss: 1.488924  [332928/344985]
loss: 1.350804  [339328/344985]

epoch avg train loss: 1.534080   epoch avg train accuracy: 0.624323

-------------------------------

Epoch 6

loss: 1.200668  [  128/344985]
loss: 1.337913  [ 6528/344985]
loss: 1.443968  [12928/344985]
loss: 1.308586  [19328/344985]
loss: 1.355469  [25728/344985]
loss: 1.498006  [32128/344985]
loss: 1.266357  [38528/344985]
loss: 1.389582  [44928/344985]
loss: 1.339521  [51328/344985]
loss: 1.367961  [57728/344985]
loss: 1.301850  [64128/344985]
loss: 1.566128  [70528/344985]
loss: 1.608929  [76928/344985]
loss: 1.592093  [83328/344985]
loss: 1.256083  [89728/344985]
loss: 1.270666  [96128/344985]
loss: 1.488664  [102528/344985]
loss: 1.469939  [108928/344985]
loss: 1.595118  [115328/344985]
loss: 1.645438  [121728/344985]
loss: 1.551551  [128128/344985]
loss: 1.682103  [134528/344985]
loss: 1.235415  [140928/344985]
loss: 1.598755  [147328/344985]
loss: 1.780707  [153728/344985]
loss: 1.986323  [160128/344985]
loss: 1.549963  [166528/344985]
loss: 1.329055  [172928/344985]
loss: 1.493947  [179328/344985]
loss: 1.578919  [185728/344985]
loss: 1.504367  [192128/344985]
loss: 1.332244  [198528/344985]
loss: 1.525587  [204928/344985]
loss: 1.504145  [211328/344985]
loss: 1.636334  [217728/344985]
loss: 1.315061  [224128/344985]
loss: 1.502661  [230528/344985]
loss: 1.860086  [236928/344985]
loss: 1.699237  [243328/344985]
loss: 1.315767  [249728/344985]
loss: 1.156682  [256128/344985]
loss: 1.425401  [262528/344985]
loss: 1.354548  [268928/344985]
loss: 1.556516  [275328/344985]
loss: 1.652851  [281728/344985]
loss: 1.523450  [288128/344985]
loss: 1.410856  [294528/344985]
loss: 1.607668  [300928/344985]
loss: 1.426636  [307328/344985]
loss: 1.303040  [313728/344985]
loss: 1.589790  [320128/344985]
loss: 1.592993  [326528/344985]
loss: 1.507216  [332928/344985]
loss: 1.637483  [339328/344985]

epoch avg train loss: 1.457739   epoch avg train accuracy: 0.638718

-------------------------------

Epoch 7

loss: 1.286382  [  128/344985]
loss: 1.291256  [ 6528/344985]
loss: 1.363501  [12928/344985]
loss: 1.133951  [19328/344985]
loss: 1.493332  [25728/344985]
loss: 1.451017  [32128/344985]
loss: 1.489094  [38528/344985]
loss: 1.348224  [44928/344985]
loss: 1.291092  [51328/344985]
loss: 1.294936  [57728/344985]
loss: 1.393100  [64128/344985]
loss: 1.099005  [70528/344985]
loss: 1.187630  [76928/344985]
loss: 1.592160  [83328/344985]
loss: 1.376994  [89728/344985]
loss: 1.363348  [96128/344985]
loss: 1.417223  [102528/344985]
loss: 1.621686  [108928/344985]
loss: 1.705732  [115328/344985]
loss: 1.539253  [121728/344985]
loss: 1.348434  [128128/344985]
loss: 1.593783  [134528/344985]
loss: 1.046873  [140928/344985]
loss: 1.549987  [147328/344985]
loss: 1.877559  [153728/344985]
loss: 1.080411  [160128/344985]
loss: 1.574678  [166528/344985]
loss: 1.373916  [172928/344985]
loss: 1.386304  [179328/344985]
loss: 1.789141  [185728/344985]
loss: 1.589056  [192128/344985]
loss: 1.393753  [198528/344985]
loss: 1.578449  [204928/344985]
loss: 1.596932  [211328/344985]
loss: 1.372650  [217728/344985]
loss: 1.446718  [224128/344985]
loss: 1.585120  [230528/344985]
loss: 1.218943  [236928/344985]
loss: 1.282034  [243328/344985]
loss: 1.340995  [249728/344985]
loss: 1.254299  [256128/344985]
loss: 1.478830  [262528/344985]
loss: 1.330093  [268928/344985]
loss: 1.759157  [275328/344985]
loss: 1.459930  [281728/344985]
loss: 1.507255  [288128/344985]
loss: 1.652186  [294528/344985]
loss: 1.439613  [300928/344985]
loss: 1.227735  [307328/344985]
loss: 1.552207  [313728/344985]
loss: 1.641270  [320128/344985]
loss: 1.315682  [326528/344985]
loss: 1.340866  [332928/344985]
loss: 1.206142  [339328/344985]

epoch avg train loss: 1.392050   epoch avg train accuracy: 0.652536

-------------------------------

Epoch 8

loss: 1.265920  [  128/344985]
loss: 1.166399  [ 6528/344985]
loss: 1.126341  [12928/344985]
loss: 1.056464  [19328/344985]
loss: 1.619013  [25728/344985]
loss: 1.041496  [32128/344985]
loss: 1.173544  [38528/344985]
loss: 1.164455  [44928/344985]
loss: 1.440636  [51328/344985]
loss: 1.369095  [57728/344985]
loss: 1.417006  [64128/344985]
loss: 1.213974  [70528/344985]
loss: 1.475637  [76928/344985]
loss: 1.282483  [83328/344985]
loss: 1.220428  [89728/344985]
loss: 1.490515  [96128/344985]
loss: 1.135146  [102528/344985]
loss: 1.753423  [108928/344985]
loss: 1.134162  [115328/344985]
loss: 1.427363  [121728/344985]
loss: 1.412022  [128128/344985]
loss: 1.302055  [134528/344985]
loss: 1.269032  [140928/344985]
loss: 1.272691  [147328/344985]
loss: 1.380260  [153728/344985]
loss: 1.324570  [160128/344985]
loss: 1.527049  [166528/344985]
loss: 1.193861  [172928/344985]
loss: 1.062394  [179328/344985]
loss: 0.934848  [185728/344985]
loss: 1.550237  [192128/344985]
loss: 1.160297  [198528/344985]
loss: 1.561622  [204928/344985]
loss: 1.329076  [211328/344985]
loss: 1.242149  [217728/344985]
loss: 1.219347  [224128/344985]
loss: 1.455739  [230528/344985]
loss: 1.474184  [236928/344985]
loss: 1.454546  [243328/344985]
loss: 1.349572  [249728/344985]
loss: 1.328964  [256128/344985]
loss: 1.382729  [262528/344985]
loss: 1.040116  [268928/344985]
loss: 1.361380  [275328/344985]
loss: 1.346075  [281728/344985]
loss: 1.565844  [288128/344985]
loss: 1.390297  [294528/344985]
loss: 1.697564  [300928/344985]
loss: 1.203557  [307328/344985]
loss: 1.077488  [313728/344985]
loss: 1.159711  [320128/344985]
loss: 1.389803  [326528/344985]
loss: 1.145282  [332928/344985]
loss: 1.460998  [339328/344985]

epoch avg train loss: 1.334639   epoch avg train accuracy: 0.664255

-------------------------------

Epoch 9

loss: 1.353176  [  128/344985]
loss: 1.406417  [ 6528/344985]
loss: 1.088515  [12928/344985]
loss: 1.238442  [19328/344985]
loss: 1.082120  [25728/344985]
loss: 1.136347  [32128/344985]
loss: 1.350765  [38528/344985]
loss: 1.215808  [44928/344985]
loss: 1.759926  [51328/344985]
loss: 1.366293  [57728/344985]
loss: 1.042322  [64128/344985]
loss: 1.305929  [70528/344985]
loss: 1.252851  [76928/344985]
loss: 1.112790  [83328/344985]
loss: 1.450172  [89728/344985]
loss: 1.555658  [96128/344985]
loss: 1.334149  [102528/344985]
loss: 1.367950  [108928/344985]
loss: 1.401338  [115328/344985]
loss: 1.373764  [121728/344985]
loss: 1.358561  [128128/344985]
loss: 1.194182  [134528/344985]
loss: 1.116074  [140928/344985]
loss: 1.214166  [147328/344985]
loss: 1.551637  [153728/344985]
loss: 1.101857  [160128/344985]
loss: 1.342690  [166528/344985]
loss: 1.395950  [172928/344985]
loss: 1.326261  [179328/344985]
loss: 1.191991  [185728/344985]
loss: 1.067490  [192128/344985]
loss: 1.391690  [198528/344985]
loss: 1.176179  [204928/344985]
loss: 1.742506  [211328/344985]
loss: 1.766489  [217728/344985]
loss: 1.154289  [224128/344985]
loss: 1.219151  [230528/344985]
loss: 1.043112  [236928/344985]
loss: 1.215065  [243328/344985]
loss: 1.056346  [249728/344985]
loss: 1.401354  [256128/344985]
loss: 1.370779  [262528/344985]
loss: 1.232219  [268928/344985]
loss: 1.254318  [275328/344985]
loss: 1.209400  [281728/344985]
loss: 1.313859  [288128/344985]
loss: 1.358972  [294528/344985]
loss: 1.356414  [300928/344985]
loss: 1.453171  [307328/344985]
loss: 0.984999  [313728/344985]
loss: 1.645415  [320128/344985]
loss: 1.471541  [326528/344985]
loss: 1.431945  [332928/344985]
loss: 1.262740  [339328/344985]

epoch avg train loss: 1.283378   epoch avg train accuracy: 0.674731

-------------------------------

Epoch 10

loss: 1.092106  [  128/344985]
loss: 1.089194  [ 6528/344985]
loss: 0.853046  [12928/344985]
loss: 0.973858  [19328/344985]
loss: 1.060667  [25728/344985]
loss: 1.065666  [32128/344985]
loss: 1.027419  [38528/344985]
loss: 1.003469  [44928/344985]
loss: 1.036825  [51328/344985]
loss: 1.025705  [57728/344985]
loss: 1.432131  [64128/344985]
loss: 1.091021  [70528/344985]
loss: 1.235243  [76928/344985]
loss: 1.559005  [83328/344985]
loss: 1.129260  [89728/344985]
loss: 1.337712  [96128/344985]
loss: 1.446777  [102528/344985]
loss: 1.430248  [108928/344985]
loss: 1.137963  [115328/344985]
loss: 1.476676  [121728/344985]
loss: 1.519881  [128128/344985]
loss: 1.340032  [134528/344985]
loss: 1.319042  [140928/344985]
loss: 0.926427  [147328/344985]
loss: 0.982813  [153728/344985]
loss: 1.449736  [160128/344985]
loss: 1.167743  [166528/344985]
loss: 1.388556  [172928/344985]
loss: 1.193613  [179328/344985]
loss: 1.228391  [185728/344985]
loss: 1.322464  [192128/344985]
loss: 1.330828  [198528/344985]
loss: 1.075535  [204928/344985]
loss: 1.317369  [211328/344985]
loss: 1.253679  [217728/344985]
loss: 1.573256  [224128/344985]
loss: 1.194056  [230528/344985]
loss: 1.244067  [236928/344985]
loss: 1.029346  [243328/344985]
loss: 1.332675  [249728/344985]
loss: 1.384740  [256128/344985]
loss: 1.217076  [262528/344985]
loss: 1.194603  [268928/344985]
loss: 1.412817  [275328/344985]
loss: 1.166854  [281728/344985]
loss: 1.575948  [288128/344985]
loss: 1.321123  [294528/344985]
loss: 1.159202  [300928/344985]
loss: 1.264073  [307328/344985]
loss: 1.078397  [313728/344985]
loss: 1.250393  [320128/344985]
loss: 1.225283  [326528/344985]
loss: 1.172111  [332928/344985]
loss: 1.493505  [339328/344985]

epoch avg train loss: 1.237363   epoch avg train accuracy: 0.684772

-------------------------------

0.2796743489394598
0.0018431692833032589
