Epoch 1

loss: 5.845556  [  128/344985]
loss: 5.746114  [ 6528/344985]
loss: 5.695706  [12928/344985]
loss: 5.676731  [19328/344985]
loss: 5.528554  [25728/344985]
loss: 5.425800  [32128/344985]
loss: 5.412562  [38528/344985]
loss: 5.149771  [44928/344985]
loss: 5.293389  [51328/344985]
loss: 5.321162  [57728/344985]
loss: 5.079805  [64128/344985]
loss: 5.148740  [70528/344985]
loss: 5.091594  [76928/344985]
loss: 5.189624  [83328/344985]
loss: 4.922962  [89728/344985]
loss: 4.805101  [96128/344985]
loss: 4.844593  [102528/344985]
loss: 4.793287  [108928/344985]
loss: 4.773506  [115328/344985]
loss: 4.773725  [121728/344985]
loss: 4.724715  [128128/344985]
loss: 4.755059  [134528/344985]
loss: 4.610359  [140928/344985]
loss: 4.460082  [147328/344985]
loss: 4.500349  [153728/344985]
loss: 4.421620  [160128/344985]
loss: 4.396042  [166528/344985]
loss: 4.317056  [172928/344985]
loss: 4.569077  [179328/344985]
loss: 4.230606  [185728/344985]
loss: 4.345031  [192128/344985]
loss: 4.345812  [198528/344985]
loss: 4.358620  [204928/344985]
loss: 4.454419  [211328/344985]
loss: 4.195994  [217728/344985]
loss: 4.155738  [224128/344985]
loss: 4.287414  [230528/344985]
loss: 3.896460  [236928/344985]
loss: 4.481005  [243328/344985]
loss: 3.858408  [249728/344985]
loss: 3.853434  [256128/344985]
loss: 4.053427  [262528/344985]
loss: 4.133205  [268928/344985]
loss: 4.415663  [275328/344985]
loss: 4.029441  [281728/344985]
loss: 3.881682  [288128/344985]
loss: 3.963104  [294528/344985]
loss: 3.862653  [300928/344985]
loss: 4.390284  [307328/344985]
loss: 3.865266  [313728/344985]
loss: 3.958017  [320128/344985]
loss: 3.891229  [326528/344985]
loss: 3.791047  [332928/344985]
loss: 3.664666  [339328/344985]

epoch avg train loss: 4.5359258   epoch avg train accuracy: 0.1008073

-------------------------------

Epoch 2

loss: 3.583903  [  128/344985]
loss: 4.027459  [ 6528/344985]
loss: 3.523170  [12928/344985]
loss: 3.778129  [19328/344985]
loss: 3.447447  [25728/344985]
loss: 3.583705  [32128/344985]
loss: 3.917307  [38528/344985]
loss: 3.944457  [44928/344985]
loss: 3.598741  [51328/344985]
loss: 3.755287  [57728/344985]
loss: 3.778678  [64128/344985]
loss: 3.891095  [70528/344985]
loss: 3.554854  [76928/344985]
loss: 3.410142  [83328/344985]
loss: 3.889677  [89728/344985]
loss: 3.473350  [96128/344985]
loss: 3.825624  [102528/344985]
loss: 3.816591  [108928/344985]
loss: 3.635845  [115328/344985]
loss: 3.902323  [121728/344985]
loss: 3.561497  [128128/344985]
loss: 3.830034  [134528/344985]
loss: 3.836432  [140928/344985]
loss: 3.780198  [147328/344985]
loss: 3.653117  [153728/344985]
loss: 3.801368  [160128/344985]
loss: 3.555508  [166528/344985]
loss: 3.543824  [172928/344985]
loss: 3.559564  [179328/344985]
loss: 3.395675  [185728/344985]
loss: 3.906120  [192128/344985]
loss: 3.412694  [198528/344985]
loss: 3.242085  [204928/344985]
loss: 3.306889  [211328/344985]
loss: 3.267179  [217728/344985]
loss: 3.599694  [224128/344985]
loss: 3.767837  [230528/344985]
loss: 3.440922  [236928/344985]
loss: 3.801404  [243328/344985]
loss: 3.746761  [249728/344985]
loss: 2.841906  [256128/344985]
loss: 3.470461  [262528/344985]
loss: 3.600740  [268928/344985]
loss: 3.636113  [275328/344985]
loss: 3.515811  [281728/344985]
loss: 3.569406  [288128/344985]
loss: 3.528768  [294528/344985]
loss: 3.564974  [300928/344985]
loss: 3.312554  [307328/344985]
loss: 3.527091  [313728/344985]
loss: 3.442947  [320128/344985]
loss: 3.517347  [326528/344985]
loss: 3.089650  [332928/344985]
loss: 3.139377  [339328/344985]

epoch avg train loss: 3.6029518   epoch avg train accuracy: 0.2203023

-------------------------------

Epoch 3

loss: 3.469244  [  128/344985]
loss: 3.279633  [ 6528/344985]
loss: 3.496608  [12928/344985]
loss: 3.288756  [19328/344985]
loss: 3.114727  [25728/344985]
loss: 3.382454  [32128/344985]
loss: 2.965359  [38528/344985]
loss: 3.307980  [44928/344985]
loss: 3.611770  [51328/344985]
loss: 3.312776  [57728/344985]
loss: 3.540294  [64128/344985]
loss: 3.422701  [70528/344985]
loss: 3.467117  [76928/344985]
loss: 2.988562  [83328/344985]
loss: 3.287366  [89728/344985]
loss: 3.254835  [96128/344985]
loss: 3.330074  [102528/344985]
loss: 3.274030  [108928/344985]
loss: 3.174449  [115328/344985]
loss: 3.282010  [121728/344985]
loss: 3.076245  [128128/344985]
loss: 3.176975  [134528/344985]
loss: 3.252507  [140928/344985]
loss: 3.543952  [147328/344985]
loss: 3.075304  [153728/344985]
loss: 3.133238  [160128/344985]
loss: 3.566972  [166528/344985]
loss: 3.132763  [172928/344985]
loss: 2.719836  [179328/344985]
loss: 3.472946  [185728/344985]
loss: 3.066705  [192128/344985]
loss: 3.339849  [198528/344985]
loss: 3.527115  [204928/344985]
loss: 3.668284  [211328/344985]
loss: 3.420612  [217728/344985]
loss: 3.440800  [224128/344985]
loss: 3.288691  [230528/344985]
loss: 3.284891  [236928/344985]
loss: 3.377331  [243328/344985]
loss: 3.268691  [249728/344985]
loss: 3.462255  [256128/344985]
loss: 3.191565  [262528/344985]
loss: 3.261938  [268928/344985]
loss: 3.259254  [275328/344985]
loss: 3.022419  [281728/344985]
loss: 3.009832  [288128/344985]
loss: 3.005621  [294528/344985]
loss: 3.258395  [300928/344985]
loss: 3.385342  [307328/344985]
loss: 3.351834  [313728/344985]
loss: 3.350622  [320128/344985]
loss: 3.338416  [326528/344985]
loss: 3.074517  [332928/344985]
loss: 3.183419  [339328/344985]

epoch avg train loss: 3.2742632   epoch avg train accuracy: 0.2729394

-------------------------------

Epoch 4

loss: 3.054806  [  128/344985]
loss: 3.182634  [ 6528/344985]
loss: 3.203772  [12928/344985]
loss: 3.046529  [19328/344985]
loss: 2.854922  [25728/344985]
loss: 2.951796  [32128/344985]
loss: 3.040177  [38528/344985]
loss: 3.158056  [44928/344985]
loss: 3.060276  [51328/344985]
loss: 2.841121  [57728/344985]
loss: 3.149576  [64128/344985]
loss: 2.873718  [70528/344985]
loss: 2.822946  [76928/344985]
loss: 2.939879  [83328/344985]
loss: 3.214369  [89728/344985]
loss: 3.281351  [96128/344985]
loss: 3.354907  [102528/344985]
loss: 2.859114  [108928/344985]
loss: 2.719924  [115328/344985]
loss: 2.807343  [121728/344985]
loss: 3.359004  [128128/344985]
loss: 3.053583  [134528/344985]
loss: 3.142279  [140928/344985]
loss: 2.997353  [147328/344985]
loss: 2.963597  [153728/344985]
loss: 2.898967  [160128/344985]
loss: 2.966436  [166528/344985]
loss: 3.002809  [172928/344985]
loss: 2.992177  [179328/344985]
loss: 2.825811  [185728/344985]
loss: 2.925574  [192128/344985]
loss: 3.038480  [198528/344985]
loss: 3.332170  [204928/344985]
loss: 2.989084  [211328/344985]
loss: 2.967093  [217728/344985]
loss: 2.770583  [224128/344985]
loss: 2.893733  [230528/344985]
loss: 3.128736  [236928/344985]
loss: 2.855954  [243328/344985]
loss: 3.115677  [249728/344985]
loss: 3.285423  [256128/344985]
loss: 3.414572  [262528/344985]
loss: 3.063530  [268928/344985]
loss: 2.800884  [275328/344985]
loss: 2.871478  [281728/344985]
loss: 3.215559  [288128/344985]
loss: 3.128591  [294528/344985]
loss: 2.928177  [300928/344985]
loss: 3.120082  [307328/344985]
loss: 3.012277  [313728/344985]
loss: 3.121109  [320128/344985]
loss: 3.183445  [326528/344985]
loss: 3.219091  [332928/344985]
loss: 3.389217  [339328/344985]

epoch avg train loss: 3.0642005   epoch avg train accuracy: 0.3078974

-------------------------------

Epoch 5

loss: 3.138296  [  128/344985]
loss: 2.766786  [ 6528/344985]
loss: 2.812689  [12928/344985]
loss: 3.016893  [19328/344985]
loss: 2.820442  [25728/344985]
loss: 2.896775  [32128/344985]
loss: 2.773311  [38528/344985]
loss: 2.740974  [44928/344985]
loss: 2.927876  [51328/344985]
loss: 2.706035  [57728/344985]
loss: 2.851194  [64128/344985]
loss: 3.072969  [70528/344985]
loss: 2.799427  [76928/344985]
loss: 2.717994  [83328/344985]
loss: 2.847288  [89728/344985]
loss: 3.076674  [96128/344985]
loss: 2.980706  [102528/344985]
loss: 2.571006  [108928/344985]
loss: 2.914372  [115328/344985]
loss: 3.090762  [121728/344985]
loss: 2.657930  [128128/344985]
loss: 2.853375  [134528/344985]
loss: 3.091236  [140928/344985]
loss: 2.782365  [147328/344985]
loss: 3.006383  [153728/344985]
loss: 2.747231  [160128/344985]
loss: 3.038846  [166528/344985]
loss: 2.561771  [172928/344985]
loss: 2.935209  [179328/344985]
loss: 2.732254  [185728/344985]
loss: 3.042189  [192128/344985]
loss: 2.756970  [198528/344985]
loss: 2.998266  [204928/344985]
loss: 2.881741  [211328/344985]
loss: 3.066868  [217728/344985]
loss: 3.198545  [224128/344985]
loss: 2.819456  [230528/344985]
loss: 2.734499  [236928/344985]
loss: 3.203080  [243328/344985]
loss: 2.994710  [249728/344985]
loss: 2.959255  [256128/344985]
loss: 2.716887  [262528/344985]
loss: 3.035071  [268928/344985]
loss: 2.740402  [275328/344985]
loss: 3.025781  [281728/344985]
loss: 2.727401  [288128/344985]
loss: 3.027789  [294528/344985]
loss: 2.601882  [300928/344985]
loss: 2.838923  [307328/344985]
loss: 2.587920  [313728/344985]
loss: 2.774608  [320128/344985]
loss: 2.851387  [326528/344985]
loss: 2.860160  [332928/344985]
loss: 2.687470  [339328/344985]

epoch avg train loss: 2.9030148   epoch avg train accuracy: 0.3359537

-------------------------------

Epoch 6

loss: 2.774533  [  128/344985]
loss: 2.717329  [ 6528/344985]
loss: 2.714095  [12928/344985]
loss: 2.923847  [19328/344985]
loss: 2.720342  [25728/344985]
loss: 2.767760  [32128/344985]
loss: 2.635011  [38528/344985]
loss: 2.404529  [44928/344985]
loss: 2.677588  [51328/344985]
loss: 2.462869  [57728/344985]
loss: 2.619508  [64128/344985]
loss: 2.622810  [70528/344985]
loss: 2.564314  [76928/344985]
loss: 2.587916  [83328/344985]
loss: 2.545801  [89728/344985]
loss: 2.860942  [96128/344985]
loss: 3.023572  [102528/344985]
loss: 2.925567  [108928/344985]
loss: 2.882439  [115328/344985]
loss: 2.498250  [121728/344985]
loss: 3.060429  [128128/344985]
loss: 2.705936  [134528/344985]
loss: 2.872465  [140928/344985]
loss: 2.969951  [147328/344985]
loss: 3.195582  [153728/344985]
loss: 3.116635  [160128/344985]
loss: 2.583993  [166528/344985]
loss: 2.679609  [172928/344985]
loss: 2.705174  [179328/344985]
loss: 2.837933  [185728/344985]
loss: 2.682546  [192128/344985]
loss: 2.767223  [198528/344985]
loss: 2.785864  [204928/344985]
loss: 2.477193  [211328/344985]
loss: 3.004214  [217728/344985]
loss: 2.919939  [224128/344985]
loss: 2.947507  [230528/344985]
loss: 3.051063  [236928/344985]
loss: 2.957141  [243328/344985]
loss: 3.035981  [249728/344985]
loss: 2.600460  [256128/344985]
loss: 3.206464  [262528/344985]
loss: 2.614124  [268928/344985]
loss: 2.928464  [275328/344985]
loss: 2.546309  [281728/344985]
loss: 2.992067  [288128/344985]
loss: 2.681548  [294528/344985]
loss: 2.727969  [300928/344985]
loss: 3.209497  [307328/344985]
loss: 2.728619  [313728/344985]
loss: 2.760800  [320128/344985]
loss: 2.922451  [326528/344985]
loss: 2.904357  [332928/344985]
loss: 2.731502  [339328/344985]

epoch avg train loss: 2.7723005   epoch avg train accuracy: 0.3576213

-------------------------------

Epoch 7

loss: 2.589873  [  128/344985]
loss: 2.338884  [ 6528/344985]
loss: 2.757734  [12928/344985]
loss: 2.186319  [19328/344985]
loss: 2.594619  [25728/344985]
loss: 2.751680  [32128/344985]
loss: 2.829593  [38528/344985]
loss: 2.537894  [44928/344985]
loss: 2.581068  [51328/344985]
loss: 2.752107  [57728/344985]
loss: 2.711443  [64128/344985]
loss: 2.600707  [70528/344985]
loss: 2.620655  [76928/344985]
loss: 2.832743  [83328/344985]
loss: 2.548259  [89728/344985]
loss: 2.317040  [96128/344985]
loss: 2.665102  [102528/344985]
loss: 2.638561  [108928/344985]
loss: 2.905854  [115328/344985]
loss: 2.820771  [121728/344985]
loss: 2.631528  [128128/344985]
loss: 2.882522  [134528/344985]
loss: 2.717604  [140928/344985]
loss: 2.731830  [147328/344985]
loss: 2.941255  [153728/344985]
loss: 2.396883  [160128/344985]
loss: 2.640502  [166528/344985]
loss: 2.576415  [172928/344985]
loss: 2.504453  [179328/344985]
loss: 2.791368  [185728/344985]
loss: 2.654164  [192128/344985]
loss: 2.634856  [198528/344985]
loss: 2.740932  [204928/344985]
loss: 2.831586  [211328/344985]
loss: 2.591418  [217728/344985]
loss: 2.686381  [224128/344985]
loss: 3.000229  [230528/344985]
loss: 2.804210  [236928/344985]
loss: 2.594949  [243328/344985]
loss: 2.702071  [249728/344985]
loss: 2.725385  [256128/344985]
loss: 2.653342  [262528/344985]
loss: 2.555721  [268928/344985]
loss: 2.946958  [275328/344985]
loss: 2.675718  [281728/344985]
loss: 2.897295  [288128/344985]
loss: 2.793087  [294528/344985]
loss: 3.034953  [300928/344985]
loss: 2.634708  [307328/344985]
loss: 3.098231  [313728/344985]
loss: 2.590356  [320128/344985]
loss: 2.656431  [326528/344985]
loss: 2.390896  [332928/344985]
loss: 2.706101  [339328/344985]

epoch avg train loss: 2.6586715   epoch avg train accuracy: 0.3774019

-------------------------------

Epoch 8

loss: 2.809770  [  128/344985]
loss: 2.331159  [ 6528/344985]
loss: 2.595226  [12928/344985]
loss: 2.313821  [19328/344985]
loss: 2.489899  [25728/344985]
loss: 2.335653  [32128/344985]
loss: 2.595538  [38528/344985]
loss: 2.047229  [44928/344985]
loss: 2.469293  [51328/344985]
loss: 2.353912  [57728/344985]
loss: 2.623206  [64128/344985]
loss: 2.386608  [70528/344985]
loss: 2.876185  [76928/344985]
loss: 2.743593  [83328/344985]
loss: 2.659299  [89728/344985]
loss: 2.613805  [96128/344985]
loss: 2.473710  [102528/344985]
loss: 2.716647  [108928/344985]
loss: 2.225855  [115328/344985]
loss: 2.630089  [121728/344985]
loss: 2.520333  [128128/344985]
loss: 2.713226  [134528/344985]
loss: 2.589831  [140928/344985]
loss: 2.451883  [147328/344985]
loss: 2.437395  [153728/344985]
loss: 2.441912  [160128/344985]
loss: 2.760664  [166528/344985]
loss: 2.340672  [172928/344985]
loss: 2.463955  [179328/344985]
loss: 2.500631  [185728/344985]
loss: 2.520601  [192128/344985]
loss: 2.614992  [198528/344985]
loss: 2.959611  [204928/344985]
loss: 2.913361  [211328/344985]
loss: 2.432384  [217728/344985]
loss: 2.282889  [224128/344985]
loss: 2.480628  [230528/344985]
loss: 2.450229  [236928/344985]
loss: 2.536017  [243328/344985]
loss: 2.323769  [249728/344985]
loss: 2.659482  [256128/344985]
loss: 2.375605  [262528/344985]
loss: 2.665334  [268928/344985]
loss: 2.388789  [275328/344985]
loss: 2.445975  [281728/344985]
loss: 2.665254  [288128/344985]
loss: 2.587217  [294528/344985]
loss: 2.413656  [300928/344985]
loss: 2.570536  [307328/344985]
loss: 2.583514  [313728/344985]
loss: 2.499471  [320128/344985]
loss: 2.141402  [326528/344985]
loss: 2.719065  [332928/344985]
loss: 2.637922  [339328/344985]

epoch avg train loss: 2.5589857   epoch avg train accuracy: 0.3942983

-------------------------------

Epoch 9

loss: 2.534521  [  128/344985]
loss: 2.641288  [ 6528/344985]
loss: 2.545080  [12928/344985]
loss: 2.432683  [19328/344985]
loss: 2.523861  [25728/344985]
loss: 2.069293  [32128/344985]
loss: 2.467846  [38528/344985]
loss: 2.455561  [44928/344985]
loss: 2.952566  [51328/344985]
loss: 2.084739  [57728/344985]
loss: 2.093784  [64128/344985]
loss: 2.359160  [70528/344985]
loss: 2.618198  [76928/344985]
loss: 2.437197  [83328/344985]
loss: 2.698176  [89728/344985]
loss: 2.580198  [96128/344985]
loss: 2.446393  [102528/344985]
loss: 2.443923  [108928/344985]
loss: 2.451153  [115328/344985]
loss: 2.741804  [121728/344985]
loss: 2.344449  [128128/344985]
loss: 2.685442  [134528/344985]
loss: 2.274845  [140928/344985]
loss: 2.049495  [147328/344985]
loss: 2.675070  [153728/344985]
loss: 2.491894  [160128/344985]
loss: 2.359846  [166528/344985]
loss: 2.501813  [172928/344985]
loss: 2.750612  [179328/344985]
loss: 2.264585  [185728/344985]
loss: 2.202754  [192128/344985]
loss: 2.618844  [198528/344985]
loss: 2.157903  [204928/344985]
loss: 2.477150  [211328/344985]
loss: 2.816834  [217728/344985]
loss: 2.202625  [224128/344985]
loss: 2.584810  [230528/344985]
loss: 2.462507  [236928/344985]
loss: 2.286535  [243328/344985]
loss: 2.119344  [249728/344985]
loss: 2.250752  [256128/344985]
loss: 2.431956  [262528/344985]
loss: 2.333904  [268928/344985]
loss: 2.746212  [275328/344985]
loss: 2.563176  [281728/344985]
loss: 2.578892  [288128/344985]
loss: 2.658770  [294528/344985]
loss: 2.229371  [300928/344985]
loss: 2.558512  [307328/344985]
loss: 2.482717  [313728/344985]
loss: 2.846806  [320128/344985]
loss: 2.365395  [326528/344985]
loss: 2.817907  [332928/344985]
loss: 2.322490  [339328/344985]

epoch avg train loss: 2.4704918   epoch avg train accuracy: 0.4111367

-------------------------------

Epoch 10

loss: 2.521600  [  128/344985]
loss: 2.235312  [ 6528/344985]
loss: 2.261328  [12928/344985]
loss: 2.195822  [19328/344985]
loss: 2.308185  [25728/344985]
loss: 2.118807  [32128/344985]
loss: 2.366540  [38528/344985]
loss: 2.187559  [44928/344985]
loss: 2.144726  [51328/344985]
loss: 1.951593  [57728/344985]
loss: 2.763263  [64128/344985]
loss: 2.356744  [70528/344985]
loss: 2.456133  [76928/344985]
loss: 2.491668  [83328/344985]
loss: 2.656978  [89728/344985]
loss: 2.373883  [96128/344985]
loss: 2.431102  [102528/344985]
loss: 2.513642  [108928/344985]
loss: 2.498052  [115328/344985]
loss: 2.754966  [121728/344985]
loss: 2.666851  [128128/344985]
loss: 2.321836  [134528/344985]
loss: 2.506261  [140928/344985]
loss: 2.145686  [147328/344985]
loss: 2.343772  [153728/344985]
loss: 2.372676  [160128/344985]
loss: 2.265895  [166528/344985]
loss: 2.457778  [172928/344985]
loss: 2.407535  [179328/344985]
loss: 2.290965  [185728/344985]
loss: 2.719187  [192128/344985]
loss: 2.681250  [198528/344985]
loss: 2.299643  [204928/344985]
loss: 2.523566  [211328/344985]
loss: 2.636359  [217728/344985]
loss: 2.645443  [224128/344985]
loss: 2.382961  [230528/344985]
loss: 2.475640  [236928/344985]
loss: 2.083873  [243328/344985]
loss: 2.598878  [249728/344985]
loss: 2.414703  [256128/344985]
loss: 2.386279  [262528/344985]
loss: 2.311892  [268928/344985]
loss: 2.118225  [275328/344985]
loss: 2.251285  [281728/344985]
loss: 2.727241  [288128/344985]
loss: 2.417474  [294528/344985]
loss: 2.301056  [300928/344985]
loss: 2.622064  [307328/344985]
loss: 2.598683  [313728/344985]
loss: 2.623867  [320128/344985]
loss: 2.395569  [326528/344985]
loss: 2.235210  [332928/344985]
loss: 2.709105  [339328/344985]

epoch avg train loss: 2.3871720   epoch avg train accuracy: 0.4260475

-------------------------------

Epoch 11

loss: 2.416297  [  128/344985]
loss: 2.114565  [ 6528/344985]
loss: 2.095536  [12928/344985]
loss: 2.117713  [19328/344985]
loss: 2.087495  [25728/344985]
loss: 2.371225  [32128/344985]
loss: 2.200283  [38528/344985]
loss: 1.992233  [44928/344985]
loss: 2.183340  [51328/344985]
loss: 2.482443  [57728/344985]
loss: 2.302538  [64128/344985]
loss: 2.162700  [70528/344985]
loss: 2.395921  [76928/344985]
loss: 2.658343  [83328/344985]
loss: 2.404472  [89728/344985]
loss: 2.292721  [96128/344985]
loss: 2.632473  [102528/344985]
loss: 1.915005  [108928/344985]
loss: 2.289459  [115328/344985]
loss: 2.173604  [121728/344985]
loss: 2.032772  [128128/344985]
loss: 2.054032  [134528/344985]
loss: 2.580117  [140928/344985]
loss: 2.514208  [147328/344985]
loss: 2.168347  [153728/344985]
loss: 2.426857  [160128/344985]
loss: 2.683834  [166528/344985]
loss: 2.198391  [172928/344985]
loss: 2.160964  [179328/344985]
loss: 2.086254  [185728/344985]
loss: 2.524179  [192128/344985]
loss: 2.113171  [198528/344985]
loss: 2.651348  [204928/344985]
loss: 1.941549  [211328/344985]
loss: 2.318245  [217728/344985]
loss: 1.984607  [224128/344985]
loss: 2.625175  [230528/344985]
loss: 2.846547  [236928/344985]
loss: 2.042985  [243328/344985]
loss: 2.590174  [249728/344985]
loss: 2.479629  [256128/344985]
loss: 2.304550  [262528/344985]
loss: 2.244306  [268928/344985]
loss: 2.607218  [275328/344985]
loss: 2.156318  [281728/344985]
loss: 2.587106  [288128/344985]
loss: 2.664086  [294528/344985]
loss: 2.178872  [300928/344985]
loss: 2.645446  [307328/344985]
loss: 2.240523  [313728/344985]
loss: 2.618231  [320128/344985]
loss: 2.662726  [326528/344985]
loss: 2.073348  [332928/344985]
loss: 2.824281  [339328/344985]

epoch avg train loss: 2.3148374   epoch avg train accuracy: 0.4384017

-------------------------------

Epoch 12

loss: 2.319103  [  128/344985]
loss: 2.036248  [ 6528/344985]
loss: 2.079479  [12928/344985]
loss: 2.109020  [19328/344985]
loss: 2.263140  [25728/344985]
loss: 2.083207  [32128/344985]
loss: 1.936732  [38528/344985]
loss: 2.247805  [44928/344985]
loss: 2.253400  [51328/344985]
loss: 2.424172  [57728/344985]
loss: 2.183439  [64128/344985]
loss: 2.663005  [70528/344985]
loss: 2.149002  [76928/344985]
loss: 2.286288  [83328/344985]
loss: 2.297777  [89728/344985]
loss: 2.403460  [96128/344985]
loss: 2.167710  [102528/344985]
loss: 2.155780  [108928/344985]
loss: 2.105848  [115328/344985]
loss: 2.084884  [121728/344985]
loss: 2.156494  [128128/344985]
loss: 2.286685  [134528/344985]
loss: 2.038095  [140928/344985]
loss: 2.345495  [147328/344985]
loss: 2.116376  [153728/344985]
loss: 2.343110  [160128/344985]
loss: 2.189884  [166528/344985]
loss: 2.188354  [172928/344985]
loss: 2.639253  [179328/344985]
loss: 2.414022  [185728/344985]
loss: 2.325183  [192128/344985]
loss: 2.217186  [198528/344985]
loss: 2.300194  [204928/344985]
loss: 2.549955  [211328/344985]
loss: 2.251032  [217728/344985]
loss: 2.427945  [224128/344985]
loss: 2.526824  [230528/344985]
loss: 1.814018  [236928/344985]
loss: 1.965402  [243328/344985]
loss: 2.130498  [249728/344985]
loss: 2.140095  [256128/344985]
loss: 1.979219  [262528/344985]
loss: 2.210017  [268928/344985]
loss: 2.530511  [275328/344985]
loss: 2.302848  [281728/344985]
loss: 2.484478  [288128/344985]
loss: 2.482950  [294528/344985]
loss: 2.706070  [300928/344985]
loss: 2.449633  [307328/344985]
loss: 2.491962  [313728/344985]
loss: 2.829760  [320128/344985]
loss: 2.344922  [326528/344985]
loss: 2.067783  [332928/344985]
loss: 2.128763  [339328/344985]

epoch avg train loss: 2.2449700   epoch avg train accuracy: 0.4514341

-------------------------------

Epoch 13

loss: 2.370571  [  128/344985]
loss: 2.005053  [ 6528/344985]
loss: 2.127971  [12928/344985]
loss: 1.950342  [19328/344985]
loss: 1.636856  [25728/344985]
loss: 2.311369  [32128/344985]
loss: 2.354359  [38528/344985]
loss: 2.175443  [44928/344985]
loss: 2.258042  [51328/344985]
loss: 2.360529  [57728/344985]
loss: 2.163784  [64128/344985]
loss: 1.867405  [70528/344985]
loss: 2.389154  [76928/344985]
loss: 2.194005  [83328/344985]
loss: 2.513838  [89728/344985]
loss: 2.238812  [96128/344985]
loss: 2.083637  [102528/344985]
loss: 1.926632  [108928/344985]
loss: 2.209399  [115328/344985]
loss: 2.036530  [121728/344985]
loss: 2.146869  [128128/344985]
loss: 2.079926  [134528/344985]
loss: 2.111566  [140928/344985]
loss: 2.045271  [147328/344985]
loss: 2.038678  [153728/344985]
loss: 2.064522  [160128/344985]
loss: 2.663808  [166528/344985]
loss: 2.390725  [172928/344985]
loss: 2.137874  [179328/344985]
loss: 2.454637  [185728/344985]
loss: 1.874566  [192128/344985]
loss: 1.991037  [198528/344985]
loss: 2.327125  [204928/344985]
loss: 2.419251  [211328/344985]
loss: 2.220655  [217728/344985]
loss: 2.056504  [224128/344985]
loss: 1.975141  [230528/344985]
loss: 2.151240  [236928/344985]
loss: 2.657357  [243328/344985]
loss: 2.423945  [249728/344985]
loss: 1.979994  [256128/344985]
loss: 2.342906  [262528/344985]
loss: 2.465025  [268928/344985]
loss: 2.255274  [275328/344985]
loss: 2.467412  [281728/344985]
loss: 2.194989  [288128/344985]
loss: 2.115309  [294528/344985]
loss: 2.259384  [300928/344985]
loss: 2.411212  [307328/344985]
loss: 2.500634  [313728/344985]
loss: 2.285867  [320128/344985]
loss: 2.256226  [326528/344985]
loss: 2.283082  [332928/344985]
loss: 2.313962  [339328/344985]

epoch avg train loss: 2.1818121   epoch avg train accuracy: 0.4631303

-------------------------------

Epoch 14

loss: 2.107555  [  128/344985]
loss: 1.963619  [ 6528/344985]
loss: 2.239322  [12928/344985]
loss: 1.981432  [19328/344985]
loss: 2.083498  [25728/344985]
loss: 2.038680  [32128/344985]
loss: 2.231317  [38528/344985]
loss: 2.061932  [44928/344985]
loss: 1.949854  [51328/344985]
loss: 2.459893  [57728/344985]
loss: 2.040377  [64128/344985]
loss: 2.073320  [70528/344985]
loss: 1.971119  [76928/344985]
loss: 2.155120  [83328/344985]
loss: 2.199526  [89728/344985]
loss: 2.299753  [96128/344985]
loss: 2.234574  [102528/344985]
loss: 2.032664  [108928/344985]
loss: 1.867767  [115328/344985]
loss: 1.827603  [121728/344985]
loss: 2.161593  [128128/344985]
loss: 2.026933  [134528/344985]
loss: 2.353189  [140928/344985]
loss: 2.140236  [147328/344985]
loss: 1.850841  [153728/344985]
loss: 2.152319  [160128/344985]
loss: 2.285848  [166528/344985]
loss: 2.102211  [172928/344985]
loss: 2.214727  [179328/344985]
loss: 2.252975  [185728/344985]
loss: 2.296820  [192128/344985]
loss: 2.451220  [198528/344985]
loss: 2.479232  [204928/344985]
loss: 2.277907  [211328/344985]
loss: 2.183435  [217728/344985]
loss: 2.350034  [224128/344985]
loss: 1.832464  [230528/344985]
loss: 1.951328  [236928/344985]
loss: 2.520672  [243328/344985]
loss: 2.056197  [249728/344985]
loss: 2.310937  [256128/344985]
loss: 2.221439  [262528/344985]
loss: 2.064076  [268928/344985]
loss: 1.992408  [275328/344985]
loss: 2.352279  [281728/344985]
loss: 2.546728  [288128/344985]
loss: 1.976777  [294528/344985]
loss: 2.030866  [300928/344985]
loss: 2.376967  [307328/344985]
loss: 2.424061  [313728/344985]
loss: 2.095670  [320128/344985]
loss: 2.257690  [326528/344985]
loss: 2.079486  [332928/344985]
loss: 2.622564  [339328/344985]

epoch avg train loss: 2.1205704   epoch avg train accuracy: 0.4740670

-------------------------------

Epoch 15

loss: 1.920488  [  128/344985]
loss: 1.825245  [ 6528/344985]
loss: 2.047001  [12928/344985]
loss: 2.167646  [19328/344985]
loss: 1.765309  [25728/344985]
loss: 1.810015  [32128/344985]
loss: 1.814318  [38528/344985]
loss: 1.974549  [44928/344985]
loss: 1.759859  [51328/344985]
loss: 1.918341  [57728/344985]
loss: 2.174431  [64128/344985]
loss: 2.182228  [70528/344985]
loss: 1.538457  [76928/344985]
loss: 1.910728  [83328/344985]
loss: 1.888212  [89728/344985]
loss: 2.158631  [96128/344985]
loss: 1.943274  [102528/344985]
loss: 2.258517  [108928/344985]
loss: 2.104546  [115328/344985]
loss: 1.976136  [121728/344985]
loss: 2.003787  [128128/344985]
loss: 2.018245  [134528/344985]
loss: 1.851388  [140928/344985]
loss: 2.235210  [147328/344985]
loss: 2.050948  [153728/344985]
loss: 2.236382  [160128/344985]
loss: 2.117864  [166528/344985]
loss: 2.253696  [172928/344985]
loss: 2.083813  [179328/344985]
loss: 2.276396  [185728/344985]
loss: 1.939741  [192128/344985]
loss: 1.966655  [198528/344985]
loss: 2.168308  [204928/344985]
loss: 2.064721  [211328/344985]
loss: 2.369116  [217728/344985]
loss: 1.959369  [224128/344985]
loss: 2.055524  [230528/344985]
loss: 1.993579  [236928/344985]
loss: 2.239254  [243328/344985]
loss: 2.462979  [249728/344985]
loss: 1.825781  [256128/344985]
loss: 2.060557  [262528/344985]
loss: 1.850512  [268928/344985]
loss: 2.130272  [275328/344985]
loss: 2.284246  [281728/344985]
loss: 1.969011  [288128/344985]
loss: 1.874728  [294528/344985]
loss: 2.258080  [300928/344985]
loss: 2.290563  [307328/344985]
loss: 2.074502  [313728/344985]
loss: 2.070645  [320128/344985]
loss: 2.187471  [326528/344985]
loss: 2.009587  [332928/344985]
loss: 2.227950  [339328/344985]

epoch avg train loss: 2.0619014   epoch avg train accuracy: 0.4854008

-------------------------------

Epoch 16

loss: 1.958055  [  128/344985]
loss: 1.757344  [ 6528/344985]
loss: 1.946520  [12928/344985]
loss: 1.809897  [19328/344985]
loss: 1.880386  [25728/344985]
loss: 1.650022  [32128/344985]
loss: 1.891317  [38528/344985]
loss: 2.222301  [44928/344985]
loss: 1.804553  [51328/344985]
loss: 2.143922  [57728/344985]
loss: 1.744099  [64128/344985]
loss: 1.873904  [70528/344985]
loss: 2.285937  [76928/344985]
loss: 2.106336  [83328/344985]
loss: 1.729770  [89728/344985]
loss: 1.857068  [96128/344985]
loss: 1.843093  [102528/344985]
loss: 1.940747  [108928/344985]
loss: 1.717187  [115328/344985]
loss: 1.838560  [121728/344985]
loss: 1.715915  [128128/344985]
loss: 2.134670  [134528/344985]
loss: 1.975928  [140928/344985]
loss: 2.162298  [147328/344985]
loss: 1.889427  [153728/344985]
loss: 2.041612  [160128/344985]
loss: 1.812602  [166528/344985]
loss: 2.014538  [172928/344985]
loss: 2.169871  [179328/344985]
loss: 1.865072  [185728/344985]
loss: 1.754091  [192128/344985]
loss: 2.329899  [198528/344985]
loss: 1.932873  [204928/344985]
loss: 1.972238  [211328/344985]
loss: 1.835588  [217728/344985]
loss: 2.102772  [224128/344985]
loss: 1.882726  [230528/344985]
loss: 2.165508  [236928/344985]
loss: 2.018622  [243328/344985]
loss: 1.956836  [249728/344985]
loss: 1.858866  [256128/344985]
loss: 2.454330  [262528/344985]
loss: 2.111542  [268928/344985]
loss: 2.157829  [275328/344985]
loss: 2.070470  [281728/344985]
loss: 2.111486  [288128/344985]
loss: 2.045035  [294528/344985]
loss: 2.317770  [300928/344985]
loss: 2.149158  [307328/344985]
loss: 1.833325  [313728/344985]
loss: 2.107658  [320128/344985]
loss: 2.269829  [326528/344985]
loss: 2.259571  [332928/344985]
loss: 1.831355  [339328/344985]

epoch avg train loss: 2.0074652   epoch avg train accuracy: 0.4957114

-------------------------------

Epoch 17

loss: 1.959631  [  128/344985]
loss: 1.810154  [ 6528/344985]
loss: 2.015012  [12928/344985]
loss: 2.065185  [19328/344985]
loss: 1.980291  [25728/344985]
loss: 1.874365  [32128/344985]
loss: 1.759971  [38528/344985]
loss: 2.074460  [44928/344985]
loss: 1.804516  [51328/344985]
loss: 1.577562  [57728/344985]
loss: 1.883343  [64128/344985]
loss: 2.050219  [70528/344985]
loss: 1.837329  [76928/344985]
loss: 1.655401  [83328/344985]
loss: 1.908753  [89728/344985]
loss: 1.789430  [96128/344985]
loss: 1.730603  [102528/344985]
loss: 1.871919  [108928/344985]
loss: 1.930834  [115328/344985]
loss: 1.987688  [121728/344985]
loss: 1.683912  [128128/344985]
loss: 2.088508  [134528/344985]
loss: 1.736029  [140928/344985]
loss: 1.587788  [147328/344985]
loss: 1.902558  [153728/344985]
loss: 2.105269  [160128/344985]
loss: 2.002182  [166528/344985]
loss: 1.906659  [172928/344985]
loss: 2.018162  [179328/344985]
loss: 2.204435  [185728/344985]
loss: 1.860099  [192128/344985]
loss: 2.080644  [198528/344985]
loss: 1.916731  [204928/344985]
loss: 2.011370  [211328/344985]
loss: 1.992614  [217728/344985]
loss: 1.859413  [224128/344985]
loss: 2.010439  [230528/344985]
loss: 1.784104  [236928/344985]
loss: 1.992086  [243328/344985]
loss: 1.949719  [249728/344985]
loss: 1.964256  [256128/344985]
loss: 2.061912  [262528/344985]
loss: 1.743885  [268928/344985]
loss: 2.135924  [275328/344985]
loss: 2.062038  [281728/344985]
loss: 2.331319  [288128/344985]
loss: 2.116004  [294528/344985]
loss: 1.624082  [300928/344985]
loss: 1.757175  [307328/344985]
loss: 1.770123  [313728/344985]
loss: 2.116480  [320128/344985]
loss: 2.278487  [326528/344985]
loss: 1.806061  [332928/344985]
loss: 2.050340  [339328/344985]

epoch avg train loss: 1.9589322   epoch avg train accuracy: 0.5038190

-------------------------------

Epoch 18

loss: 1.934632  [  128/344985]
loss: 1.940833  [ 6528/344985]
loss: 1.567770  [12928/344985]
loss: 1.844510  [19328/344985]
loss: 1.737186  [25728/344985]
loss: 1.893474  [32128/344985]
loss: 1.591442  [38528/344985]
loss: 1.667900  [44928/344985]
loss: 1.495191  [51328/344985]
loss: 1.878039  [57728/344985]
loss: 1.890386  [64128/344985]
loss: 2.164794  [70528/344985]
loss: 1.782551  [76928/344985]
loss: 1.769951  [83328/344985]
loss: 2.160787  [89728/344985]
loss: 1.683307  [96128/344985]
loss: 2.042183  [102528/344985]
loss: 1.876535  [108928/344985]
loss: 1.522761  [115328/344985]
loss: 1.927246  [121728/344985]
loss: 1.888415  [128128/344985]
loss: 1.878088  [134528/344985]
loss: 1.918401  [140928/344985]
loss: 1.923073  [147328/344985]
loss: 1.726739  [153728/344985]
loss: 1.943806  [160128/344985]
loss: 1.929894  [166528/344985]
loss: 2.590780  [172928/344985]
loss: 1.882329  [179328/344985]
loss: 1.895439  [185728/344985]
loss: 2.027316  [192128/344985]
loss: 1.995331  [198528/344985]
loss: 2.086162  [204928/344985]
loss: 2.000205  [211328/344985]
loss: 1.780282  [217728/344985]
loss: 1.959299  [224128/344985]
loss: 2.077473  [230528/344985]
loss: 2.003169  [236928/344985]
loss: 1.922843  [243328/344985]
loss: 1.931143  [249728/344985]
loss: 2.000108  [256128/344985]
loss: 2.153980  [262528/344985]
loss: 1.772089  [268928/344985]
loss: 1.913544  [275328/344985]
loss: 1.846886  [281728/344985]
loss: 2.210443  [288128/344985]
loss: 2.033783  [294528/344985]
loss: 2.225305  [300928/344985]
loss: 2.232064  [307328/344985]
loss: 2.006380  [313728/344985]
loss: 1.795296  [320128/344985]
loss: 2.124045  [326528/344985]
loss: 1.991829  [332928/344985]
loss: 1.997315  [339328/344985]

epoch avg train loss: 1.9113448   epoch avg train accuracy: 0.5136136

-------------------------------

Epoch 19

loss: 1.605071  [  128/344985]
loss: 1.713238  [ 6528/344985]
loss: 1.863527  [12928/344985]
loss: 1.419219  [19328/344985]
loss: 1.880528  [25728/344985]
loss: 1.498206  [32128/344985]
loss: 1.794056  [38528/344985]
loss: 1.629920  [44928/344985]
loss: 1.836968  [51328/344985]
loss: 1.577655  [57728/344985]
loss: 1.619340  [64128/344985]
loss: 2.091501  [70528/344985]
loss: 2.127365  [76928/344985]
loss: 1.938049  [83328/344985]
loss: 1.736801  [89728/344985]
loss: 1.924778  [96128/344985]
loss: 1.765064  [102528/344985]
loss: 1.703216  [108928/344985]
loss: 1.806878  [115328/344985]
loss: 1.932765  [121728/344985]
loss: 1.855749  [128128/344985]
loss: 1.811272  [134528/344985]
loss: 1.656554  [140928/344985]
loss: 1.777313  [147328/344985]
loss: 1.844247  [153728/344985]
loss: 1.798900  [160128/344985]
loss: 1.979354  [166528/344985]
loss: 2.132826  [172928/344985]
loss: 2.033042  [179328/344985]
loss: 1.954038  [185728/344985]
loss: 2.121893  [192128/344985]
loss: 2.019938  [198528/344985]
loss: 1.923573  [204928/344985]
loss: 2.333188  [211328/344985]
loss: 1.700966  [217728/344985]
loss: 2.285054  [224128/344985]
loss: 1.768202  [230528/344985]
loss: 1.715169  [236928/344985]
loss: 1.833007  [243328/344985]
loss: 1.622382  [249728/344985]
loss: 1.722320  [256128/344985]
loss: 1.849030  [262528/344985]
loss: 1.716133  [268928/344985]
loss: 1.711604  [275328/344985]
loss: 1.801827  [281728/344985]
loss: 1.910118  [288128/344985]
loss: 1.805164  [294528/344985]
loss: 2.422376  [300928/344985]
loss: 1.974809  [307328/344985]
loss: 1.926325  [313728/344985]
loss: 1.649244  [320128/344985]
loss: 1.939212  [326528/344985]
loss: 1.943672  [332928/344985]
loss: 2.250826  [339328/344985]

epoch avg train loss: 1.8661103   epoch avg train accuracy: 0.5225850

-------------------------------

Epoch 20

loss: 1.589292  [  128/344985]
loss: 1.555044  [ 6528/344985]
loss: 1.713713  [12928/344985]
loss: 1.457781  [19328/344985]
loss: 1.729387  [25728/344985]
loss: 1.680431  [32128/344985]
loss: 1.452296  [38528/344985]
loss: 1.602998  [44928/344985]
loss: 1.454539  [51328/344985]
loss: 1.451452  [57728/344985]
loss: 1.687758  [64128/344985]
loss: 1.599486  [70528/344985]
loss: 1.659021  [76928/344985]
loss: 1.735947  [83328/344985]
loss: 1.777029  [89728/344985]
loss: 1.792214  [96128/344985]
loss: 1.534089  [102528/344985]
loss: 2.161749  [108928/344985]
loss: 1.744450  [115328/344985]
loss: 1.999987  [121728/344985]
loss: 1.991127  [128128/344985]
loss: 2.028009  [134528/344985]
loss: 1.960853  [140928/344985]
loss: 2.004472  [147328/344985]
loss: 1.702780  [153728/344985]
loss: 2.064912  [160128/344985]
loss: 1.633937  [166528/344985]
loss: 1.667892  [172928/344985]
loss: 1.832988  [179328/344985]
loss: 1.984072  [185728/344985]
loss: 2.238866  [192128/344985]
loss: 1.977018  [198528/344985]
loss: 1.905989  [204928/344985]
loss: 1.960150  [211328/344985]
loss: 1.550359  [217728/344985]
loss: 1.865854  [224128/344985]
loss: 1.625889  [230528/344985]
loss: 1.865659  [236928/344985]
loss: 1.951949  [243328/344985]
loss: 1.713459  [249728/344985]
loss: 2.079221  [256128/344985]
loss: 1.833487  [262528/344985]
loss: 1.674523  [268928/344985]
loss: 2.075844  [275328/344985]
loss: 1.870092  [281728/344985]
loss: 1.976917  [288128/344985]
loss: 2.281187  [294528/344985]
loss: 1.777962  [300928/344985]
loss: 1.953881  [307328/344985]
loss: 1.793993  [313728/344985]
loss: 1.921603  [320128/344985]
loss: 1.888142  [326528/344985]
loss: 1.998562  [332928/344985]
loss: 1.650814  [339328/344985]

epoch avg train loss: 1.8195523   epoch avg train accuracy: 0.5326521

-------------------------------

Evaluating against random transformations...
Mean acc: 0.3260
Acc std: 0.0012721
